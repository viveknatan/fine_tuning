{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckbbj5diaHkg"
   },
   "source": [
    "# Fine-tuning Embeddings for RAG on Specific Data\n",
    "\n",
    "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
    "\n",
    "Fine-tuning embeddings!\n",
    "\n",
    "- 🤝 Breakout Room #1:\n",
    "  - Task 1: Dependencies and Boilerplate\n",
    "  - Task 2: Loading Data\n",
    "  - Task 3: Constructing a Fine-tuning Dataset\n",
    "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
    "  - Task 5: Evaluating our Retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xwor_3X6ODX"
   },
   "source": [
    "#### Basic Overview of Fine-tuning Embeddings\n",
    "\n",
    "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
    "\n",
    "```\n",
    "Move the embeddings for questions relating to a document\n",
    "closer together with that document\n",
    "```\n",
    "\n",
    "We can think of fine-tuning our embedding models as follows:\n",
    "\n",
    "1) We have some pair of text items that *should* be closer together\n",
    "  - `Question`, `Document` pairs\n",
    "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
    "\n",
    "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
    "\n",
    "The process of training helps the model more accurately associate our questions with the correct documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX5R3HVz6FOQ"
   },
   "source": [
    "##### ❓ Question #1:\n",
    "\n",
    "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
    "\n",
    "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
    "\n",
    "<b>Q&D Pairs Training Approach:</b> In this approach, the embedding model is trained using specific query-document pairs that are directly related to each other. It optimizes the embedding space for semantic retrieval and focuses on matching queries to their most relevant documents. However it could lead to overfitting and could perform well only with questions it has been exposed to.\n",
    "\n",
    "<b>Inter-document pairs Training Approach:</b> This method focuses on creating embeddings that capture broader semantic relationships between different pieces of text. It builds a more generalized semantic understanding of the data. However it may not be precisely tuned for some q&a retrieval and could produce a sub-optimal results in those cases.\n",
    "\n",
    "<b> What kind of Q's to use: </b> While using Q&D pairs training, it is essential to use a wide range of query types like keyword-based queries, domain-specific questions, abstract questions. The range of complexity and linguistic styles need to be accounted for to mimic real-life use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NkSaurzbpyS"
   },
   "source": [
    "## Task 1: Dependencies and Boilerplate\n",
    "\n",
    "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
    "\n",
    "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c_EUibmcDU3"
   },
   "source": [
    "### Nest Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746469887938,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "zq-6s7LbPnKH"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8uFz8RVcFFu"
   },
   "source": [
    "### Install Dependencies\n",
    "\n",
    "> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78268,
     "status": "ok",
     "timestamp": 1746469968078,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "ulZIBA1ZoSsV",
    "outputId": "6aadb487-7fab-48d3-bac0-56f3f3ab70b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU \"langchain_openai>=0.3.4\" \"langchain_huggingface\" \"langchain_core>=0.3.34\" \"langchain>=0.3.18\" \"langchain_community>=0.3.17\" \"langchain-text-splitters>=0.3.6\" \"datasets>=3.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7182,
     "status": "ok",
     "timestamp": 1746469975262,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "3GFD7B-tOCrx",
    "outputId": "f1142ff7-69db-45f8-9db1-ebb1690a78f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FM-eUlrcI8a"
   },
   "source": [
    "### Provide OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3758,
     "status": "ok",
     "timestamp": 1746469979022,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "wA_mlurVqtrp",
    "outputId": "557c56db-e29f-427d-daa7-57b1c3c8a048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your OpenAI API Key: ··········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFZ217gCDVTr"
   },
   "source": [
    "## Task 2: Loading Data\n",
    "\n",
    "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
    "\n",
    "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
    "\n",
    "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
    "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
    "\n",
    "Let's start by collecting our data into a useful pile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1746469982664,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "jjcwqjwJjq1l"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1746469986892,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "fa58FJr1jq1l",
    "outputId": "72d210fa-e2b1-4d8e-d4a4-459e5bffa901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 31554    0 31554    0     0   123k      0 --:--:-- --:--:-- --:--:--  123k\n"
     ]
    }
   ],
   "source": [
    "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1746469988901,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "cXKo46m8jq1l",
    "outputId": "9e4313b4-c2e9-44ab-abf2-a5e9efadfa85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 70721    0 70721    0     0  97403      0 --:--:-- --:--:-- --:--:-- 97546\n"
     ]
    }
   ],
   "source": [
    "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1746469991523,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "DHJhTzsvN75t"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "path = \"data/\"\n",
    "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UbKa6-V0nvp"
   },
   "source": [
    "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1746470003607,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "NsPrOOqXOsNX"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf_PoX7l09Rg"
   },
   "source": [
    "Next we can load/split these documents as follows.\n",
    "\n",
    "> NOTE: You may need to run this cell twice to get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1746470007576,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "OMYPX6N6Os8M"
   },
   "outputs": [],
   "source": [
    "training_documents = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746470008570,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "PAozuMoNOvnp",
    "outputId": "ba6d131b-7f49-4640-e738-12025c3c71c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yE2TFIq1BuJ"
   },
   "source": [
    "Next, we're going to associate each of our chunks with a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746470011380,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "AwyIForybIpo"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJnL4oNg341U"
   },
   "source": [
    "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746470017512,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "MTS4GTSEcnG4"
   },
   "outputs": [],
   "source": [
    "# training_split_documents = training_documents[:len(training_documents) - 24]\n",
    "# val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
    "# test_split_documents = training_documents[102-12:]\n",
    "training_split_documents = training_documents[:len(training_documents) - 24]\n",
    "val_split_documents = training_documents[len(training_documents) - 24:len(training_documents)-12]\n",
    "test_split_documents = training_documents[len(training_documents)-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746470068858,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "up8IJP1C7YIY",
    "outputId": "3788b225-696d-49f1-f296-3042efcff503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 12, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_split_documents), len(val_split_documents), len(test_split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzlvKbONDWvQ"
   },
   "source": [
    "## Task 3: Constructing a Fine-tuning Dataset\n",
    "\n",
    "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4.1-mini`\n",
    "\n",
    "The basic idea here is straightforward enough:\n",
    "\n",
    "1. We look at a document\n",
    "2. We generate questions that could be answered by that node\n",
    "\n",
    "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1746470081756,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "_EWfmIscMrvg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-hLnsSB6Y-S"
   },
   "source": [
    "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1746470091508,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "diEWcw00NMSj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u87Izpgm6_fk"
   },
   "source": [
    "We'll create a simple chain to query the LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1746470103152,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "ggl9SSjiNbpG"
   },
   "outputs": [],
   "source": [
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4duvHirh7DQv"
   },
   "source": [
    "There's a lot going on in this function - let's take a deeper look:\n",
    "\n",
    "1. First, we provide a list of documents and a number of questions\n",
    "2. We, for each document in our list, generate `n_questions` of questions.\n",
    "3. We then associate those questions and contexts via a `UUID`.\n",
    "\n",
    "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lm2JvgC9X37"
   },
   "source": [
    "##### 🏗️ Activity #1:\n",
    "\n",
    "We have:\n",
    "\n",
    "- Lists of `Documents` with the `metadata` field `id`.\n",
    "\n",
    "We need:\n",
    "\n",
    "- An object with key `id`, which have values `str` questions.\n",
    "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
    "\n",
    "An Example:\n",
    "\n",
    "question_object:\n",
    "```python\n",
    "{\n",
    "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
    "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
    "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
    "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
    "}\n",
    " ```\n",
    "\n",
    " context_object:\n",
    " ```python\n",
    "{\n",
    "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
    "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
    "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
    "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
    "}\n",
    " ```\n",
    "\n",
    " As you can see, a piece of context can be associated with more than 1 question.\n",
    "\n",
    " The task is to write the Python function(s) to accomplish this task.\n",
    "\n",
    " Your function signature is provided below, along with the desired return values.\n",
    "\n",
    " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746470108584,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "U4yi4NfTCnLc"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import asyncio\n",
    "\n",
    "\"\"\"\n",
    "Sample Usage of TQDM:\n",
    "\n",
    "for i in tqdm.tqdm(range(10)):\n",
    "  time.sleep(1)\n",
    "\"\"\"\n",
    "\n",
    "async def process_document(document, n_questions):\n",
    "    questions_generated = await question_generation_chain.ainvoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
    "\n",
    "    doc_questions = {}\n",
    "    doc_relevant_docs = {}\n",
    "\n",
    "    for question in questions_generated.content.split(\"\\n\"):\n",
    "        question_id = str(uuid.uuid4())\n",
    "        doc_questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
    "        doc_relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
    "\n",
    "    return doc_questions, doc_relevant_docs\n",
    "\n",
    "async def create_questions(documents, n_questions):\n",
    "    tasks = [process_document(doc, n_questions) for doc in documents]\n",
    "\n",
    "    questions = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for task in tqdm.tqdm(asyncio.as_completed(tasks), total=len(documents), desc=\"Processing documents\"):\n",
    "        doc_questions, doc_relevant_docs = await task\n",
    "        questions.update(doc_questions)\n",
    "        relevant_docs.update(doc_relevant_docs)\n",
    "\n",
    "    return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W0eWOUo4QGL"
   },
   "source": [
    "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10621,
     "status": "ok",
     "timestamp": 1746470124105,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "85Dq6KRqEs0F",
    "outputId": "0497636b-b9cf-475a-9e26-f9bec1457fe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 78/78 [00:10<00:00,  7.35it/s]\n"
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1746470127325,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "y76yiGuP9xR0",
    "outputId": "0c86e96d-9d27-4c41-c352-44c0ae565911"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c3a775d0-dc17-4310-8d40-a84bebcd41ae': 'What is the main discovery about Large Language Models (LLMs) in the past 24-36 months?',\n",
       " '38d648cf-b150-4b01-a656-b88381fe34c4': 'What resources are used to create Large Language Models?',\n",
       " 'd711f69a-fc1e-4344-b02b-d63714687692': 'What are some of the capabilities of LLMs mentioned in the context?',\n",
       " 'b9a59e64-9661-4d08-957e-a164ae7a70c6': 'How can LLMs assist with language translation?',\n",
       " '8d2ce54d-a85a-4e89-95ae-9d1698d62f34': 'In what ways can LLMs be used to generate content?',\n",
       " 'bcb5721d-5c4a-4b77-93c5-42266ba34428': 'What is one potential negative use of LLMs related to education?',\n",
       " 'af892954-1cb7-4d72-aabb-35682525e109': 'How do LLMs help in writing code?',\n",
       " 'eeeeab4e-142c-4c05-94ef-aeb318dde0fa': 'What ethical concerns arise from the use of LLMs?',\n",
       " '4d97f288-08e5-46a7-9c49-fef18669eca3': 'How do LLMs extract information from text?',\n",
       " '1998c613-cd11-477d-a172-fa68875e888a': 'What does the context imply about the impact of LLMs on software development?',\n",
       " '637bac2d-8bc8-4fad-8338-a4ddb537ca8f': 'Which multi-modal model series did Anthropic release in March 2024?',\n",
       " '28672591-8652-4ab0-bfad-ca84e4359f4c': 'What capabilities did Gemini 15 Pro introduce in April 2024?',\n",
       " '15bed389-19ea-4fed-8b7f-7c1495c373ad': 'Which multi-modal models were released in September 2024?',\n",
       " '6645831a-f1ac-4a53-86fe-47c687e0e7ba': 'What new features did OpenAI introduce in October 2024?',\n",
       " 'aa8f8239-f2ae-4a6d-bc4b-4104d25679dd': 'Which company released SmolVLM in November 2024?',\n",
       " '77f392af-7fbc-4ca1-a47a-6c593682510c': 'What types of models did Amazon Nova release in December 2024?',\n",
       " '7f8f474f-b1ee-4059-a787-28e80ac8a41a': 'When was the LLM CLI tool upgraded to support multi-modal models via attachments?',\n",
       " 'b268f238-9158-49dd-a8ec-41665af5d749': 'What new functionality does the upgraded LLM CLI tool include?',\n",
       " '28ce90ba-b1f4-40d9-9248-d1b04d71267c': 'How many vision models does the LLM CLI tool now support through plugins?',\n",
       " '3261e293-a008-4502-a5b2-4b60db6c34f1': 'Which vendors released multi-modal models in 2024 according to the context?',\n",
       " '994c5c19-6d5b-446a-a132-d62d12bc388b': 'What model do you run on your iPhone?',\n",
       " '9c6e7975-4068-49d8-b0c5-f0967e1a76c8': 'What type of tool does your LLM project provide for running different models?',\n",
       " 'a2f8a061-fc1e-4d42-b4a5-c3753d1c3d88': 'How can you run LLMs entirely in your browser?',\n",
       " '6d53db5f-1787-46aa-99d2-5d43e5e14f8b': 'Is it possible to have a completely private local LLM on your device?',\n",
       " '676f8922-ead0-49d0-8088-bd5810f71456': 'What is surprising about the Mistral 7B model according to the context?',\n",
       " '4ec01f27-c153-492a-99b8-fbb9dec61e1d': 'Can hobbyists build their own fine-tuned models?',\n",
       " '8524aa5e-54cf-40c6-bd2a-dac1bd1d1a00': 'What is the difference between training an LLM from scratch and fine-tuning one?',\n",
       " 'dda24d1a-6b2a-4d31-a11b-cef9cba0c9d3': 'What platforms or devices are mentioned for running LLMs?',\n",
       " '6c3799b4-2d00-4a15-8420-5e4f7ca15293': 'What technology enables running LLMs in the latest Chrome browser?',\n",
       " '24f9adde-b0b8-4241-9441-226690d14bd2': 'What does the CLI tool in your LLM project use to support different models?',\n",
       " 'dc94c59a-d1fa-4a75-9510-db3cf313fd1e': 'What significant milestone was achieved with GPT-4 according to the context?',\n",
       " '6bd1bbb7-9873-464d-a24c-2cabc7e9605f': 'Where are some of the GPT-4 models capable of running as mentioned?',\n",
       " '4db8bd19-2dfb-4906-bf1f-8df839352a2b': 'What factors contributed to the crash in LLM prices?',\n",
       " '7a31fc24-5db4-4ae2-adcb-6e7ac6712970': 'Which new modalities are becoming common or emerging in AI models?',\n",
       " '8ccd6d90-530c-4f8e-bacb-daba4a7f8666': 'How are voice and live camera modes described in the context?',\n",
       " 'e675fc8c-6089-4b1f-92b2-bd5fccdbd538': 'What is said about prompt-driven app generation?',\n",
       " 'af137f1e-0d5c-4086-9272-70b057cdf403': 'How long did universal access to the best models last?',\n",
       " '28fd8afa-e252-46cb-8125-ea79bfd6dfbb': 'What is the current status of “agents” in AI development?',\n",
       " 'c4dba797-25e1-410b-a1fa-a1595f94a8e4': 'How is Apple Intelligence compared to Apple’s MLX library?',\n",
       " 'e03f17b1-6a5e-4ead-b0d9-6ad30088e60a': 'What contrasting statements are made about the environmental impact of AI?',\n",
       " '151f264c-a3aa-4d03-9748-dba4348c10c2': 'What is the current ranking of GPT-4-0314 on the Chatbot Arena leaderboard?',\n",
       " 'dc18fa0e-fd55-4f8b-9480-d790b899842c': 'How many organizations have models that score higher than GPT-4-0314?',\n",
       " '2c7f530f-b6b4-431f-a5fe-f75469379d52': 'Can you name some of the organizations with higher scoring models than GPT-4-0314?',\n",
       " 'fd48ceec-9569-4cb4-9d9c-7d1a90da739f': 'What significance did training a GPT-4 beating model have in 2023?',\n",
       " '26abd3c6-ca96-4b38-85a7-980b23a4397c': 'How is the achievement of training a GPT-4 beating model viewed in 2024?',\n",
       " 'ae5e3f82-e886-41ad-8fe5-6d399b7fa822': 'What is the Chatbot Arena leaderboard used for?',\n",
       " '17548d96-9a55-4c51-aeae-11fa340369d8': 'Which organization names appear on the list of higher scoring models?',\n",
       " '536a4570-ca0d-4b23-87cd-7d52fdd0c944': 'Does the author personally celebrate new organizations joining the list of higher scoring models?',\n",
       " 'e5a222f9-a7bc-410e-ac0d-e5ca4d2c7191': 'Are any of the GPT-4 models mentioned capable of running on a laptop?',\n",
       " '1d0f06c5-cea5-416c-be62-f06054386136': 'How many organizations are listed as having higher scoring models than GPT-4-0314?',\n",
       " 'bf12a40d-d79a-4cda-ad1c-a2fa803d9bec': 'What are Large Language Models (LLMs)?',\n",
       " '35557c9e-6522-4ac4-86c2-e8df0eaa8a7b': 'How easy is it to build Large Language Models?',\n",
       " '35ac1a6b-133f-47a7-a012-60a368e90a71': 'Can you run LLMs on your own devices?',\n",
       " '87c54fe9-8698-45db-8115-41391a9a0e5c': 'Is it possible for hobbyists to build their own fine-tuned LLMs?',\n",
       " '2246ebfb-de7c-4d93-b6e8-46f18386e85d': 'Do we currently know how to build GPT-4?',\n",
       " 'b9202a67-25fc-441e-8d2d-ba5a74fc6914': 'What does \"Vibes Based Development\" refer to in the context of LLMs?',\n",
       " '658db603-7fac-4cec-9c12-1ba61bd52f09': 'Why are LLMs described as both really smart and really dumb?',\n",
       " 'b5fa6e90-8c53-4806-bdf3-788750da5e74': 'What is considered the biggest unsolved problem with LLMs?',\n",
       " '832a7edb-460a-422c-a592-5a502782a6ac': 'Why might code be the best application for LLMs?',\n",
       " 'd34c539d-46ff-491f-a2bc-8297fc7a3530': 'How are the ethics of the LLM space described?',\n",
       " 'ac6cc4db-4ac9-4dfa-9cf3-f0dd52dd8811': 'What is the main challenge the author faces when working with an LLM?',\n",
       " '9699c84f-8cfe-49a9-a8a1-35c58226149c': 'Why does the author find it difficult to evaluate many LLMs?',\n",
       " '2a24e149-eb69-496e-9dd1-73d7fcbea9b0': 'What specific aspect of prompting does the author find most frustrating?',\n",
       " '8a8ea102-06e8-416f-b6fe-84ebb5d7c922': 'How does the author try to emphasize certain parts of a prompt?',\n",
       " '56639650-0fe4-40d8-b982-3602f8a2a213': 'What uncertainty does the author have about capitalizing words in prompts?',\n",
       " '4ccc7192-1a1b-4cd6-8d67-24ca53331468': 'What does the author mean by \"Vibes Based Development\"?',\n",
       " 'd0e8d923-51e2-41b0-85e7-2f64a3c09a0b': 'Why does the author want to move beyond \"vibes\" in 2024?',\n",
       " '5c2ae038-0356-4934-afc3-e1ef72d2315d': 'How does the author describe the intelligence of LLMs?',\n",
       " 'a4c5fc75-cb53-469b-b0e9-98ad011a77ba': 'What does the author imply about the current state of methodologies for prompt evaluation?',\n",
       " 'c57a5a98-e05b-4041-9875-48ee0155956f': 'How long does the author typically work with an LLM to gain intuition about it?',\n",
       " '9885aa2b-ef24-493e-ae01-c86eaafa6849': 'What is the mlx-vlm project developed by Prince Canuma known for?',\n",
       " '301901eb-c333-4cb8-8358-450acaf29311': 'How does the mlx-vlm project impact vision LLMs on Apple Silicon?',\n",
       " 'b3ad17c0-5958-407c-9406-3264180b0437': 'Which model did the user run recently using the mlx-vlm project?',\n",
       " '04f25098-0a1b-4883-b4fd-5fe12b3703d9': 'How does the user describe the performance of the MLX project?',\n",
       " 'd1985048-b6aa-4c9f-bac9-efb88a6e38df': 'What has been the general reception of Apple’s “Apple Intelligence” features according to the user?',\n",
       " '20018215-e76b-49a2-aaff-1a4ac39d1216': 'When did the user write about Apple’s initial announcement of their intelligence features?',\n",
       " '8d5a44ec-d81d-4f6c-851e-ae18da663444': 'What was the user’s initial reaction to Apple’s focus on LLM applications?',\n",
       " '729b6244-4e9a-48a7-8e0e-1fd55d5b6a16': 'What two key aspects did Apple supposedly focus on in their LLM applications?',\n",
       " '1e8c2d21-c90f-4536-93d0-d15f0f11d698': 'Why was the user optimistic about Apple’s approach to LLM applications?',\n",
       " '5be0240c-dabf-460a-bacc-fa7e00c01acb': 'What concerns does the user imply about confusing features in LLM applications?',\n",
       " 'f1f7b548-7fcd-4dd2-b852-48330307a8bf': 'What was the status of GPT-4 in December 2023 according to the review?',\n",
       " '0cdcf1ec-8c7b-4cc2-af53-5720614232fd': 'How long had OpenAI’s best model been available by December 2023?',\n",
       " 'ccc4df77-574b-43c5-b6a2-a9547e97e018': 'What question did the author raise about OpenAI’s knowledge in December 2023?',\n",
       " 'e31d4304-0f51-41ea-8ff9-34e5b0b0c855': 'How has the situation changed in the twelve months following December 2023?',\n",
       " '1faf01b0-6012-4bf9-bd2c-8c155e60fba3': 'How many organizations now have models that rank higher than the original GPT-4?',\n",
       " '5a284a8a-ad58-4e18-8b87-7900eca8e4ea': 'What is the name of the original GPT-4 model referenced on the Chatbot Arena Leaderboard?',\n",
       " 'e015366b-61db-45c0-a358-cb66bd7f01d3': 'How many models in total are listed on the Chatbot Arena Leaderboard?',\n",
       " 'ea67e22c-a92c-4a2e-a804-681ec06d367c': 'What does the author mean by saying the \"GPT-4 barrier was comprehensively broken\"?',\n",
       " 'f92589a0-b28f-4e49-ac3f-74ee0900ffa3': 'What significance does the Chatbot Arena Leaderboard have in the context provided?',\n",
       " 'e28c8ef3-e00f-4543-8750-89a2db67168f': 'Why was the author relieved about the changes in AI model development over the past year?',\n",
       " '17bd7e23-b5f4-412a-a59d-5696063b4fc7': 'What unexpected capabilities do LLMs sometimes demonstrate?',\n",
       " '5d8b8001-4b34-42f2-a616-f40d0ccce9eb': 'Why are some of the methods used to get LLMs to behave considered \"incredibly dumb\"?',\n",
       " '66e03fdd-5f79-47fc-918e-c9a04ec1312e': 'How might the hidden system prompt in ChatGPT affect its behavior in December?',\n",
       " '09d7434d-e27f-4c80-bdbc-f45d0d3bd070': 'What is the speculation about ChatGPT’s performance during the holiday season?',\n",
       " 'd06d053d-d989-4a6c-b537-2c267aeaa69f': 'Why might ChatGPT provide less useful answers as the holidays approach?',\n",
       " 'a8183bdb-b6c2-480c-a923-bfc4e3664e2d': 'How does changing the date in ChatGPT’s system prompt potentially affect its responses?',\n",
       " '582adf2c-c62a-4553-8fff-5102d9989044': 'What is the level of certainty about ChatGPT’s behavior changes related to the date?',\n",
       " 'f166db62-89ca-40f6-9819-4db1fe65a3a8': 'What role does the training data play in ChatGPT’s response quality near the holidays?',\n",
       " '06b57cac-f546-46b7-8769-ac1b422e1a79': 'How do the expectations of the people who trained LLMs compare to the models’ actual capabilities?',\n",
       " 'f3988b7d-4496-4fce-958a-156e7b3c4dea': 'Why is it described as \"fun\" to discover new things that LLMs can do unexpectedly?',\n",
       " '57ee2ebc-4adb-4c74-bf11-a31a3e33ada0': 'What companies are mentioned as potentially having their own inference-scaling models in development?',\n",
       " '9b159c35-fc89-4bd8-a457-0d5998ac4e8d': 'When did Meta publish the paper titled \"Training Large Language Models to Reason in a Continuous Latent Space\"?',\n",
       " '944b4a3c-62e7-40ff-a138-b9a67f1f22e7': 'What is the title of the paper Meta published related to large language models?',\n",
       " '106dda3a-9d6f-4e88-be61-0a6a5398ca12': 'Was the best currently available large language model trained in China for less than $6 million?',\n",
       " 'ac73c1a9-0a79-4d01-a05e-e5a778935ad3': 'What was the big news announced at the end of the year?',\n",
       " 'de1b1bbd-785b-48f9-8080-a2def19727f1': 'On which platform was DeepSeek v3 released?',\n",
       " '93605d8a-a59e-4344-8202-15a8878443bb': 'On what date was DeepSeek v3 released on Hugging Face?',\n",
       " 'a3549269-bf0d-4b12-8cb2-3acea80f5d6a': 'What was initially missing when DeepSeek v3 was released on Christmas Day?',\n",
       " 'a6f82133-b897-4d3e-915d-f58c522ddce0': 'When were the documentation and paper for DeepSeek v3 released relative to the model itself?',\n",
       " '6d2dc1de-3ca4-4126-87d3-0a374e55050b': 'Why might the claim about the best LLM being trained in China for less than $6 million be considered an attention-grabbing headline?',\n",
       " '24fc54e2-3816-473c-bc16-5d1b884741a8': 'What is the chain-of-thought prompting trick as described in the context?',\n",
       " 'c7274886-778e-4f3b-a9c8-0b4eac8889eb': 'Which paper first explored the chain-of-thought prompting trick?',\n",
       " '1b82d4d0-8c05-4e9e-a76c-72c9a72debfc': 'How does talking out loud about a problem help a model according to the context?',\n",
       " '368e54a2-e9be-4b00-b1d2-976ab44a39b4': 'What does the o1 model do differently compared to the basic chain-of-thought prompting trick?',\n",
       " 'ab42df40-3b29-4464-9946-3f001fab4e85': 'What are “reasoning tokens” in the context of the o1 model?',\n",
       " '68ef1e47-52ac-4554-8789-1633cd1bef3c': 'Are the “reasoning tokens” directly visible to the user in the o1 model?',\n",
       " 'cb42f83e-5d2e-4358-a9b4-c3642cb7dd42': 'How does the ChatGPT UI handle the “reasoning tokens” used by the o1 model?',\n",
       " '63b60abf-13f4-4561-895e-e161d302b9c0': 'What is the main benefit of the chain-of-thought prompting trick for language models?',\n",
       " '6045c8ae-5381-4973-8ed4-39a79d7eb1fc': 'In what way is the process used by o1 models described as “baked into the model itself”?',\n",
       " '0d1afb2d-4800-49c6-bc95-1537c7ead145': 'Why might the details of the o1 model’s reasoning process be considered obfuscated?',\n",
       " '3fb803fc-5994-4e8c-b228-a2854766ea84': 'How have US export regulations on GPUs to China influenced training optimizations?',\n",
       " '421a4c10-cf34-4f28-b27d-817ace11667a': 'What effect has the increased efficiency of models had on environmental impact?',\n",
       " '28462d93-b97d-4c11-887f-a6688f1768c5': 'How has the energy usage of running a prompt changed over the past couple of years?',\n",
       " '006090ee-32eb-447d-ac30-cc3152359444': 'In what way has OpenAI adjusted its pricing for prompts compared to the GPT-3 era?',\n",
       " '9a0c9ae1-68a2-43be-8038-8b8fd2a09b59': 'Which companies are mentioned as some of the least expensive model providers?',\n",
       " 'f04bf7ea-073f-4f92-8715-fcecae492e82': 'Are Google Gemini and Amazon Nova running prompts at a loss according to the context?',\n",
       " 'a50abf2a-1295-4352-be82-c247d786a4c5': 'What types of models have seen increased efficiency according to the context?',\n",
       " '227b49db-4c08-40bb-bd84-e99872061562': 'How does the environmental impact relate to the efficiency of hosted and local models?',\n",
       " 'cde59836-2363-4e36-b5ed-7b1b25026081': 'What is the significance of the reduction in energy usage for running prompts?',\n",
       " '9029fe78-935a-4b3f-b03b-2077f3c1744b': 'How might export regulations indirectly affect the cost of running AI model prompts?',\n",
       " 'c7f6fce8-e58c-439f-ab6d-631b9db15540': 'When was Anthropic’s Claude 3 series launched?',\n",
       " '4bf7537e-111f-477b-863f-b5ded29b730c': 'Which model from the Claude 3 series quickly became the author’s new favourite daily-driver?',\n",
       " 'c121bdba-cdbe-40ec-9057-d7cd687950b4': 'What is the name of the model Anthropic launched in June?',\n",
       " '15529d87-496b-4025-bc6b-5010dccd6b48': 'How long has Claude 35 Sonnet remained the author’s favourite model?',\n",
       " '2edbea55-795d-4841-bd6c-2c420c416102': 'What significant event happened to Claude 35 Sonnet on October 22?',\n",
       " '0f85a75b-44ab-4aa2-a686-097579e13be7': 'Did the version number of Claude 35 Sonnet change after the October 22 upgrade?',\n",
       " '48166f87-8762-47a2-9514-fc4d00dbe4e0': 'What nickname have Anthropic fans given to the upgraded Claude 35 Sonnet?',\n",
       " '7f863c8a-b34b-4002-9a8a-f025fd6198a5': 'Which company developed the Claude 3 series?',\n",
       " 'fc5e51e1-da13-4fe7-8a57-4775719e0e29': 'How does the author describe the upgrade of Claude 35 Sonnet in June?',\n",
       " '58e3782d-7c9b-43cc-bd5c-d513d0c37fc0': 'What is the relationship between Claude 35 Sonnet and Claude 36?',\n",
       " '13de8172-764b-4b59-9d81-ef173b830a97': 'What happens when you tell the system you can’t type because you don’t have any fingers?',\n",
       " '43558550-be4e-4083-8dd7-f19308c3754f': 'How can offering cash tips affect the quality of the answers you receive?',\n",
       " '7537351f-13c9-40cb-85d2-03c78f9a2208': 'What are some methods mentioned to improve the responses from the system?',\n",
       " 'ce4a2a29-6108-414c-b437-d0836570d6ec': 'What is described as the biggest unsolved problem in the context?',\n",
       " 'e0f6c310-a137-4e05-9981-bec34e780ad2': 'When was the term \"prompt injection\" coined according to the text?',\n",
       " 'f43a0c9d-1c80-4a18-b2b4-95185548de86': 'How long has it been since the term \"prompt injection\" was coined?',\n",
       " '9a858cfa-8fd8-4908-af4a-cbfef56201d4': 'What is the current status of finding a solution to the problem of prompt injection?',\n",
       " 'bbd6f02b-a338-4001-9a16-78f4e1509dce': 'Besides prompt injection, what broader issue does the author identify?',\n",
       " '39d0f633-59c3-47c5-b6e1-79fec3835996': 'How does the author feel about the effectiveness of giving positive reinforcement to the system?',\n",
       " '2f28de1f-04d4-4a57-b8df-7bb82dc227bf': 'What has the author done in relation to the topic of prompt injection?',\n",
       " '24f31a28-560e-4cb4-b331-a5b4d93fd842': 'How many input tokens are required for each photo?',\n",
       " 'f6b0df25-0dc5-4f72-b2eb-f91d04b6931c': 'How many output tokens are estimated per photo?',\n",
       " 'fe9addf5-9954-4f32-bff4-180b9c4376eb': 'What is the total number of input tokens for 68,000 images?',\n",
       " '9202cf96-c7e5-4885-88bb-93a5d2fc78b8': 'How is the cost for input tokens calculated in the given context?',\n",
       " '7af5e3f1-6fa2-492c-a7db-64e1872da552': 'What is the total cost for processing the input tokens of 68,000 images?',\n",
       " '3ac5a075-a279-4063-9b53-340cf109ea4c': 'How many output tokens are there in total for 68,000 images?',\n",
       " '0e379ffe-497c-4f71-ae9e-fe6af2c624a5': 'What is the cost per million output tokens according to the context?',\n",
       " '47de01b0-bae9-4356-9c46-7ebb63c59b66': 'What is the total cost for processing the output tokens of 68,000 images?',\n",
       " '59471b84-677c-4eac-b3f4-54c4df9209fc': 'What is the combined total cost to process 68,000 images?',\n",
       " '9b28037e-3408-447f-a862-8c80db1e313d': 'What command was used to generate descriptions for the images?',\n",
       " '37ba2fbb-d9ba-489a-844a-4ed3bdd1249c': 'What challenges are mentioned regarding the implementation of evals?',\n",
       " '7a274c62-0702-4f1f-bc45-25ab4073a077': 'How does the author describe their SVG pelican riding a bicycle benchmark?',\n",
       " '2483444b-1a1e-4975-8110-df52335e165d': 'What is the author’s opinion on Apple Intelligence?',\n",
       " '4db325be-6e81-4a02-837b-41237f425a6a': 'How does the author feel about Apple’s MLX library?',\n",
       " 'feb3e1c8-c3a0-460a-b202-41b522c07c90': 'How has the author’s perception of using a Mac changed this year?',\n",
       " '192ce351-991c-43ff-affe-32ced96dcbb7': 'What disadvantage did the author experience last year related to hardware?',\n",
       " 'fd644136-6420-4287-9a96-bd713acfbc6a': 'Why does the author track their progress under the evals tag?',\n",
       " 'e88c6f06-42ad-498a-ac79-e771e145b254': 'What platforms does the author mention in relation to trying out new models?',\n",
       " '54263708-779c-43f8-95a7-7646d0b2959b': 'What is the significance of having an NVIDIA GPU according to the author?',\n",
       " '4b18053d-0924-4541-8588-a532290001c5': 'What is the overall tone of the author towards eval suites?',\n",
       " 'dc28af6a-69fc-40ad-b5b1-831e73ac4fdf': 'What are some key themes identified in the field of Large Language Models in 2024?',\n",
       " '58f4a191-6b8d-4aa9-9e5a-79e82ec83cbe': 'What pivotal moments in the development of LLMs occurred during 2024?',\n",
       " '52a7e438-58ba-4c1a-9b3b-8c6e5bc53f05': 'How does the 2024 review of LLMs compare to the review from 2023?',\n",
       " '83695bff-8603-415e-ae51-47c63c2a4518': 'What major advancements in LLM technology were made in 2024?',\n",
       " '39fa8c46-1f2e-46fc-a207-ea093772b268': 'How has the understanding of LLM capabilities evolved over the course of 2024?',\n",
       " '8460731b-0530-496f-80fd-0994dd7e68ef': 'What challenges related to LLMs were highlighted in 2024?',\n",
       " '6cabbb3c-b9b2-456b-b6ac-1654da86013a': 'How did the applications of LLMs change or expand in 2024?',\n",
       " '17c830ab-6445-4214-8961-c0d8724469ab': 'What role did Simon Willison’s weblog play in documenting LLM developments in 2024?',\n",
       " '115d3303-4b95-48bf-8960-bfa8d802aa99': 'What insights were gained about the ethical considerations of LLMs in 2024?',\n",
       " 'dc76a6eb-4e1f-45c6-b841-6778666a66fe': 'How did the LLM landscape shift in terms of research and industry focus during 2024?',\n",
       " '46e51ee8-1f5a-43e4-b49b-cdd85b679c6b': 'What is the surprising fact about the amount of code needed to train a basic version of a powerful system?',\n",
       " '7c7dc537-2c0b-4e4b-9659-9bb4853c4a39': 'How many lines of Python code are generally enough to train a basic version of such a system?',\n",
       " '6112a016-113e-4170-a848-79bec31295dd': 'According to the context, what matters most in building a successful model?',\n",
       " '64026c57-9b9f-4037-a438-c2d47073db64': 'Why is the quantity of training data important?',\n",
       " 'b9bf5ce2-10a4-4256-96ae-514cdd9f7d28': 'How does the quality of training data affect the resulting model?',\n",
       " '0bc01f2f-e469-40ee-b555-2aeca8f76bed': 'What two main resources are needed to build a large language model (LLM)?',\n",
       " 'b7e938cc-efb5-4552-ba2e-9a99b5860121': 'What role do GPUs play in training an LLM?',\n",
       " 'db2fa980-2297-4251-abe0-3923faabbad9': 'Can you build an LLM without the right data? Why or why not?',\n",
       " 'd51e667d-5fea-4377-98a2-11d264c99c61': 'What is the relationship between the amount of data and the performance of the model?',\n",
       " '8393900d-31cc-4c93-a20b-112399c312b4': 'Why might someone expect that training such systems requires millions of lines of code?',\n",
       " '0d4552d2-4694-46aa-ac87-3a864a07a891': 'What type of feeder is shown in the photo at the California Academy of Sciences?',\n",
       " '772cd307-88cb-4a27-9e03-7cd249d1437c': 'What color is the shallow dish in the photo?',\n",
       " 'f8088308-f9ab-4659-a6fe-70370a73e5aa': 'What kind of fruit is visible inside the feeder?',\n",
       " 'c41604f7-327b-4c46-853a-f65deb1469e8': 'How many butterflies are positioned in the feeder?',\n",
       " '5731734e-14b5-4967-ae2e-3f30477216a2': 'What are the colors and markings of the dark butterfly in the feeder?',\n",
       " '421623f1-8142-48f3-8456-04e09d55d888': 'Describe the appearance of the larger brown butterfly in the feeder',\n",
       " '8ffef983-a0c6-4d71-abde-fad423d00d16': 'Which butterfly appears to be feeding on the fruit?',\n",
       " '4a04fa7a-3c5f-4c67-9a4b-640d245c31a7': 'What distinctive features are visible on the larger brown butterfly?',\n",
       " '7e443bfa-09f0-448a-9d36-3542660fbbf3': 'Is the feeder likely intended for hummingbirds, butterflies, or another type of animal?',\n",
       " 'd87395fb-2cc7-403c-9e0f-e89f85ad3c40': 'Where was this photo of the butterflies taken?',\n",
       " '7ad720dc-bd3c-4a75-a845-25564531719a': 'What is described as the \"much bigger problem\" related to the environmental impact in the context?',\n",
       " '9d94119c-c856-461d-9f7e-957e8105c5cb': 'Which companies are mentioned as investing billions in new datacenter infrastructure?',\n",
       " 'ca839e2d-9024-4a4f-9551-2c2223764452': 'How does the buildout of new datacenters affect the electricity grid and the environment?',\n",
       " '1c1e0a3d-1830-4d47-8f1c-d22e6dc1f29d': 'What potential solution is mentioned that could take decades to implement?',\n",
       " '2ef6bdd8-db8d-4287-a294-08ea44b284e4': 'What does DeepSeek v3’s $6 million training cost suggest about the necessity of the new infrastructure?',\n",
       " '3f3e1b59-0e2b-4842-8605-23cac37715e7': 'How have the prices of large language models (LLMs) changed according to the context?',\n",
       " '8aa76cd2-6a88-499c-8351-46d61d38fc8d': 'Why might big tech executives be hesitant to argue against building out new infrastructure?',\n",
       " '08efec0f-d29b-4293-83fc-52857da0474b': 'What is the relationship between the competitive buildout of infrastructure and environmental impact?',\n",
       " 'e07317d4-cfa1-4b52-995a-46cc23a6139e': 'What is implied about the future demand for infrastructure to support AI models?',\n",
       " 'dc3091d3-13b6-4914-b6f8-aa3a3d6276b7': 'How does the context frame the uncertainty around the necessity of new datacenter infrastructure?',\n",
       " 'f010fd0f-ced5-4847-b096-21c292f03b1c': 'What is described as a natural consequence of gullibility in the context provided?',\n",
       " '36dcd5bc-09e2-4b1e-ac58-47d49caef585': 'Since when has the problem of prompt injection been discussed according to the context?',\n",
       " '1e4f4910-798e-4689-8183-560d36d95956': 'How much progress has been made on tackling prompt injection in 2024?',\n",
       " '56f2cc5f-ef49-4e0b-a1d7-241c9f3ccf0a': 'What is the most popular idea mentioned that seems dependent on AGI itself?',\n",
       " '0593c003-6901-40d9-ba37-206147ba9cbe': 'Why is creating a model that is robust against gullibility considered challenging?',\n",
       " 'bfb3aab2-5f96-4716-a6f4-b89a1b8f0bd9': 'What role do evals play according to the context?',\n",
       " 'f08d5c52-6f7a-4fbf-9181-f65dd4a7f7a6': 'Who is Amanda Askell and what is her contribution mentioned in the context?',\n",
       " 'ea5c52e5-e2c4-48d9-b9d6-50a9cf546ae8': 'Which AI model is associated with Amanda Askell’s work on character?',\n",
       " '8aefb0a8-3171-4558-bf4e-50ad4c38752c': 'What is the relationship between agents and AGI as suggested in the context?',\n",
       " '9c5edac1-bcb9-47a0-803b-ed154407725e': 'Why might prompt injection remain a persistent problem despite ongoing discussions?',\n",
       " '36b52789-024e-4d71-89da-0d9f324e71b2': 'What is the general opinion expressed about Apple’s LLM features in the context?',\n",
       " '219fd523-f1b4-4be4-93f4-b6bd93e94f59': 'How do the notification summaries provided by Apple’s LLM features perform according to the user?',\n",
       " '7e4d4def-00fc-4959-89b9-c67c0c35c90c': 'What is the user’s experience with Apple’s writing assistant tools?',\n",
       " 'a2a1a5b6-386b-4ec7-b87c-0421b7c30f44': 'Which feature mentioned in the context is described as “kind of fun”?',\n",
       " '5de20780-a238-4b5f-8493-98497bbe1d78': 'What significant development in LLM technology occurred in the final quarter of 2024?',\n",
       " 'ff3bcb15-4522-452b-9c06-e614305b6881': 'Which company’s models exemplify the new shape of LLM introduced in late 2024?',\n",
       " '94ff5612-bbfe-48ba-bb7e-cef5098dc159': 'When were OpenAI’s o1-preview and o1-mini models initially released?',\n",
       " '70a765a8-310b-4777-be84-f87168fce510': 'How does the user compare Apple’s LLM features to frontier LLM capabilities?',\n",
       " '36c65a5d-09a9-40d6-ad31-415ba9796076': 'What term is used to describe the new type of LLM models introduced in 2024?',\n",
       " 'f73ea54f-7c2e-47b9-9fdf-528c859ac081': 'What specific models are mentioned as part of the inference-scaling “reasoning” models?',\n",
       " '596241d1-dcdd-40b4-b8c3-9b8cd11bb68a': 'What model and specifications does the personal laptop mentioned have?',\n",
       " 'b24d9988-4cc1-4268-914d-92c7b0552fe2': 'When was the personal laptop first used to run a large language model (LLM)?',\n",
       " 'fe968f28-d3f1-41f7-ac39-8d60d436651d': 'How old is the laptop as of the context provided?',\n",
       " 'cca8ecff-010c-4c8c-bec8-efa66f3c50a8': 'What significant milestone did the laptop achieve in March 2023?',\n",
       " '0a8f860e-745e-4594-ab3f-27adb5390a19': \"How has the laptop's capability changed from March 2023 to the present?\",\n",
       " '4f50b237-b9ae-46e2-8f3c-09010dac73f7': 'What class of model could the laptop just about run in March 2023?',\n",
       " '9ef37e7a-8de8-44cd-a136-f5c3c2a5224f': 'What class of models has the laptop run more recently?',\n",
       " '13fca96f-66c4-4bca-be32-d4c3a974884f': 'What is the significance of the laptop running multiple GPT-4 class models?',\n",
       " '3653d55f-1c64-4dcb-9c52-8636f416b380': 'How does the author describe the power of the laptop?',\n",
       " 'be0c323f-b5ec-4d01-96cc-3b51249bdf6a': 'What is the reference made to \"Large language models are having their Stable Diffusion moment\"?',\n",
       " 'c20d1fd3-879a-44d7-b5a0-c318ea1d01bf': 'What is Qwen25-Coder-32B known for?',\n",
       " 'cce432dc-75a7-4774-a8e3-40ada22b155a': 'On which device can Qwen25-Coder-32B run according to the context?',\n",
       " 'b0d94628-74e9-49f5-8b3d-50e3a7c585b2': 'Under what license is Qwen25-Coder-32B released?',\n",
       " 'dc0e5cb3-07df-4a47-8202-62952880071a': 'In which month is Qwen25-Coder-32B mentioned?',\n",
       " '87667f9a-45c2-48f1-b121-d43b141e5353': 'What type of model is Qwen25-Coder-32B described as?',\n",
       " '824c0a40-aae0-42ca-a5ae-c9596bb510bd': 'Which GPT-4 class model can now be run on the laptop mentioned?',\n",
       " '186e4aa3-d757-44fa-832c-b50420a1bd6f': 'What is the name of the model released in December?',\n",
       " '5032be55-f25e-4d62-8cfd-639ba74e8f9a': 'How many parameters does Meta’s Llama 33 model have?',\n",
       " '1e27df04-35b4-4d9b-8407-27113fe4a42e': 'When was Meta’s Llama 33 70B model released?',\n",
       " '44d2c015-0576-4fc5-9416-4ddb8b293511': 'What is the relationship between Qwen25-Coder-32B and Meta’s Llama 33 70B in the context?',\n",
       " 'fabd35a3-329f-4eb0-a2e9-3496a625239b': 'When did OpenAI make GPT-4o free for all users?',\n",
       " '1f8e10bf-23f7-4aef-8b8c-a6c055b8b1ac': 'Which model was freely available from its launch in June?',\n",
       " 'f6c89ce9-52aa-45ca-9c7d-df24b12ea327': 'What was the limitation for free users before GPT-4o and Claude 35 Sonnet became available?',\n",
       " 'ebc87086-39a7-401b-9935-d2a709eb5155': 'Why was the previous restriction to GPT-35 level models significant for new users?',\n",
       " '188b51b5-e870-4945-8e10-40823d81823b': 'What service did OpenAI launch that changed access to their most capable model?',\n",
       " 'f1949f1d-319c-44cd-a6e8-1622133c31bd': 'How much does the ChatGPT Pro subscription cost per month?',\n",
       " '7465d5b0-b229-4827-8eba-bf689ba7a47e': 'Which model is exclusively accessible through the ChatGPT Pro subscription?',\n",
       " '93e2b24d-b4f6-4a29-8983-f704f9d4c1b9': 'What is the main reason the o1 series models require a subscription for access?',\n",
       " 'f5b6f3a1-cfd8-40e0-acfd-f3fc6f7ac3ba': 'According to the context, are free users likely to regain access to the best available models?',\n",
       " '4f19808c-45e2-45cf-bbdc-22244e0d01bf': 'What impact did the launch of ChatGPT Pro have on the availability of advanced language models?',\n",
       " '4392b1b5-7690-4ca5-a8a3-c075afc9d59f': 'What is the current age of the abilities mentioned in the context?',\n",
       " '04b48bf6-2f7f-41c0-9cac-3b1e1e468719': 'Which two companies offer API access to the new features?',\n",
       " '7ff3c518-2dcf-46a6-92db-f2e922cc2fab': 'What type of API did OpenAI initially provide for these features?',\n",
       " '6f77c96a-c07c-4c0d-97ef-3c014533740e': 'What improvement did OpenAI announce in December regarding their API?',\n",
       " '2c251efd-56b8-469c-aba9-6930ccd84667': 'Why is building a web app with voice interaction easier now?',\n",
       " 'eb701b6a-b961-42f1-93db-f1f032084fed': 'When did prompt-driven app generation become a commodity?',\n",
       " '0e5f7054-af11-4386-8009-39a10c82e7e7': 'Which version of GPT made prompt-driven app generation possible in 2023?',\n",
       " 'ad53ddf0-af14-4d49-a5aa-37f329c409e5': 'In what year did the value of prompt-driven app generation become evident?',\n",
       " 'e27350e6-05de-421c-b87e-c039eb8629ea': 'What is the suggested action for someone who hasn’t tried the new abilities yet?',\n",
       " 'bb746234-5295-44c9-a712-ea63be962899': 'How does the context describe the initial WebSocket API from OpenAI?',\n",
       " 'f9adec07-ed3c-44ce-9a08-f94ea5b02be5': 'What significant development in AI occurred in 2023 according to the weblog?',\n",
       " '6b91a5c6-bca3-4bff-ba2b-a53b17c502c4': 'How does the author describe Large Language Models (LLMs) in relation to AI?',\n",
       " '386608f5-11bf-47b6-85c8-c64c0dd3e36b': 'Why does the author consider 2023 a breakthrough year for LLMs?',\n",
       " '7a442d55-f131-4445-ba70-ed9155bb5dd4': 'What is the historical context of Artificial Intelligence mentioned in the weblog?',\n",
       " '5c4c9770-50c5-45bf-9c26-1e628ff8c4e1': 'Who is the author of the weblog discussing AI developments in 2023?',\n",
       " '5a0dfd82-5442-42c3-aa12-1a0ddf7baa4b': 'What is the main focus of the weblog post titled \"Stuff we figured out about AI in 2023\"?',\n",
       " '3fbc00e2-42d6-4e5e-aa0f-a2671e2fd326': 'How does the author justify calling LLMs \"AI\"?',\n",
       " 'd89178b2-4b82-4cc0-9556-d7729141067f': 'What does the author aim to provide in the weblog post about AI in 2023?',\n",
       " 'c8351c2f-35cb-4bec-b0b7-40fb2de0dbd3': 'According to the weblog, how long has the academic field of Artificial Intelligence been around?',\n",
       " '2261099b-1f30-4516-825d-f62176dc8a88': 'What is the significance of Large Language Models in the field of AI as per the weblog?',\n",
       " 'dc75139a-24cf-4581-a284-2fdb85bf6ea6': 'What is the main topic discussed in the article titled \"Industry’s Tardy Response to the AI Prompt Injection Vulnerability\" on RedMonk Conversations?',\n",
       " '445fab8a-9ee3-48cb-b65e-ecc79b43da3f': 'When was the article \"Industry’s Tardy Response to the AI Prompt Injection Vulnerability\" posted?',\n",
       " '59f9deca-2f97-4fdf-8c28-8a7d34c8ad7b': 'Who is the author of the article \"This is Stuff we figured out about AI in 2023\"?',\n",
       " '90ac8d8a-0cab-4ffb-9db2-367177a93851': 'What social media platforms can readers follow the author on according to the context?',\n",
       " '209ddf80-06fa-49e7-8d0f-cc6f6cf4ff51': 'What is the date of the most recent article mentioned in the context?',\n",
       " 'aa776da4-4336-4356-a424-6b3f07405da5': 'What is the subject of the article posted on 5th May 2025?',\n",
       " 'fcb2ea75-1dad-4c26-a557-36e310b5c3da': 'How many authors and publishers are mentioned in the article dated 1st May 2025?',\n",
       " '4da73f85-4cec-428e-83d0-8116a4783e2d': 'What is the focus of the article posted on 30th April 2025?',\n",
       " '89afc1c8-e83c-4d69-a9b0-de3f87eadc1d': 'What series does the article \"This is Stuff we figured out about AI in 2023\" belong to?',\n",
       " 'f549880d-75ae-4aa3-a946-876b78cf87f4': 'What type of content does the author offer besides articles, as indicated in the context?',\n",
       " '3ffd9fed-0098-4f32-b165-023c0cefdf3b': 'How many parameters does DeepSeek v3 have?',\n",
       " 'f47b20f5-297a-4940-88fc-4977f7883259': 'How does the size of DeepSeek v3 compare to Meta’s Llama 31 405B model?',\n",
       " '6630c224-c0d8-4865-ad74-5f70363904b2': 'Which models does DeepSeek v3 benchmark alongside according to the provided context?',\n",
       " '28fcd045-4716-4107-bb84-357a8082e95b': 'What is the current ranking of DeepSeek v3 in the Vibe benchmarks (Chatbot Arena)?',\n",
       " '886d74f5-05f0-44bf-98e3-4072ed0d7779': 'Which models are ranked just ahead of DeepSeek v3 in the Vibe benchmarks?',\n",
       " 'e5423d4c-2c15-430d-a0b2-8eda20fe895f': 'What distinguishes DeepSeek v3 in terms of licensing compared to other high-ranking models?',\n",
       " 'b541a896-1f29-42eb-9486-09a046b057fc': 'How many GPU hours were used to train DeepSeek v3?',\n",
       " '952e1e94-673a-47c4-805b-60e98a3826aa': 'What was the estimated training cost for DeepSeek v3?',\n",
       " '5cbf8866-ba45-4d74-8ae4-67fd74fc351e': 'How many GPU hours did it take to train Llama 31 405B, and how does this compare to DeepSeek v3?',\n",
       " 'd3644ea0-8e55-47d5-a727-2dcd40568324': 'Despite using fewer GPU hours, how does DeepSeek v3’s benchmark performance compare to Llama 31 405B?',\n",
       " 'dd7ad8d4-b6ff-4dca-84d7-efcacd65d294': 'What is the main issue the author has with the term \"agents\"?',\n",
       " '51d48501-9a39-423b-abed-54e3388442e4': 'Why does the author find the term \"agents\" frustrating?',\n",
       " '241f4f6e-9851-4ade-b59a-732797f6e283': 'According to the context, what is lacking in the use of the term \"agents\"?',\n",
       " 'e27de8e1-8489-4887-ace2-5cea6a4d9ba9': 'How does the author describe the clarity of the term \"agents\"?',\n",
       " '7f098cd7-eed8-4291-a299-0369249a8c45': 'What does the author imply about people who use the term \"agents\"?',\n",
       " 'b6c2994e-5975-48bb-bf07-77aeb0e1c9d3': 'What challenge does the author face when someone says they are building \"agents\"?',\n",
       " '3bd6b03e-9e2c-458c-81cd-e5e7c2c5b433': 'How many possible definitions of \"agents\" does the author suggest exist?',\n",
       " 'd0650c6a-f6af-408a-a77e-4f9fd21f2e5b': 'What does the author mean by saying \"agents\" still haven’t really happened yet?',\n",
       " '9f932bb4-47c5-4b2f-9303-7655cae5abb0': 'Why does the author say that saying \"I am building agents\" conveys almost no information?',\n",
       " '1ca68490-1ed9-4524-ac20-b8c8a4a574f0': 'What would the author need to do to understand what someone means by \"agents\"?',\n",
       " '01289986-d2c0-4aad-a74c-f71332e09fd1': 'What makes the prompt-driven custom interface feature powerful and easy to build?',\n",
       " '8e735e7d-47b4-4291-a639-e32f90835ab0': 'What challenge must be figured out to build the prompt-driven custom interface feature?',\n",
       " 'bdd20292-ab28-4d0f-bc10-116ca0baa1f3': 'When is the prompt-driven custom interface feature expected to appear in a wide range of products?',\n",
       " '48a60c59-3b8d-4c83-8871-4f7ae3ce8acb': 'How long did universal access to the best models last?',\n",
       " '3fc5678e-226f-4c86-955e-94733176a210': 'Which three best available models were freely accessible for a few months?',\n",
       " '2e1fd32f-b516-41a1-952a-8f0a4303e77f': 'During what time period were GPT-4o, Claude 35 Sonnet, and Gemini 15 Pro freely available?',\n",
       " '457a5d1e-9f85-4b60-ba36-ebc44b38d8da': 'What is the significance of browser sandboxing in building the custom interface feature?',\n",
       " 'da2e9d74-d807-4ec9-92a2-5683f81acb1a': 'What does the context suggest about the future availability of the best models?',\n",
       " '8e0bdae4-c08a-4999-9c06-2ff0263d6e39': 'How does the prompt-driven custom interface feature relate to the best available models?',\n",
       " 'c1a5353e-e1c5-451a-a14f-aa2bba21bb91': 'Why might the prompt-driven custom interface feature become widespread in 2025?',\n",
       " 'bef55bfd-9e98-4b25-93ba-5705afebd6d6': 'What is the significance of adding a walrus in prompt engineering for DALL-E 3?',\n",
       " '688897b0-9a7c-49d2-a1fb-56866b2c0db1': 'How does the vicuna-7b Large Language Model run entirely in a web browser?',\n",
       " 'd16242a2-354a-401d-b1ff-453dfeebb4f4': 'Why can’t ChatGPT access the internet despite appearing to do so?',\n",
       " '7acadd8d-6bcd-4a93-95ac-6429e757813a': 'What advancements has Stanford Alpaca contributed to on-device large language model development?',\n",
       " '1d9baf9e-7f89-419b-bce6-a6a463d15657': 'How can you run Llama 2 on a Mac using LLM and Homebrew?',\n",
       " '1fb4c3e1-75c1-4ccc-8a06-94d02dd64b6c': 'What are the new features or improvements in Midjourney 51?',\n",
       " 'ab21af1c-4bf8-4ae6-b247-d72b26c75caa': 'Why are language models like ChatGPT compared to a “calculator for words”?',\n",
       " '0de52d66-3f25-4239-951a-0950144b8df5': 'What are multi-modal prompt injection image attacks against GPT-4V?',\n",
       " '40f689d6-9d94-4537-86d2-c1c0336f7160': 'How do web-based large language models like vicuna-7b impact user accessibility?',\n",
       " 'd8e619ab-838b-4d8a-a074-f400e21408ac': 'What challenges exist in prompt engineering for multi-modal models such as GPT-4V?',\n",
       " '25a94cdf-a0e7-452c-b6dc-94d53de5aaf9': 'Why is the author surprised that no one has beaten GPT-4 yet?',\n",
       " '1e74fbe1-d040-4e68-8116-56968e40222c': 'What does the author imply about OpenAI’s approach to GPT-4?',\n",
       " '47f17bcb-8856-4787-82d0-b8b22fa70dd9': 'How does the author describe their feelings towards large language models (LLMs)?',\n",
       " 'a676a056-b39d-4267-ac0d-b0a09ccf3328': 'What is meant by the term \"Vibes Based Development\" in the context of LLMs?',\n",
       " '98ab1e13-a4a7-4710-baae-e54e40f06dd5': 'Why does the author find LLMs to be \"convoluted black boxes\"?',\n",
       " 'd425ffec-1aad-4ad8-8b0d-9c12fe2775e8': 'How does programming with LLMs differ from traditional programming, according to the author?',\n",
       " '1dcd90ee-c3d0-4401-8ce9-26a505b8262a': 'What challenges does the author mention about controlling LLMs?',\n",
       " '6b453f7d-acf5-4560-b47b-b43f54c5182b': 'Why is evaluating LLMs described as the \"worst part\"?',\n",
       " '5722224e-ff97-45ae-9f29-cf25732f59f0': 'What limitations do benchmarks have when assessing LLM performance?',\n",
       " '0bd6d0ee-de56-4440-ab72-eda26bfa0a08': 'What does the author mean by an LLM “feeling” right for a given task?',\n",
       " 'bd354bd8-f236-4406-93d0-321eeff342e2': 'Which organization was the only one to have released a generally useful LLM a year ago?',\n",
       " 'ab5f1d2b-374f-49ed-97f7-7d77e5a4ee35': 'Name some organizations that have produced better-than-GPT-3 class models recently',\n",
       " '5b7b03cd-4ed3-4e6b-9678-5b9b8594aa3c': 'What is the approximate initial training cost for large language models in terms of hardware and electricity?',\n",
       " 'a84ec43d-7eb5-4a58-922e-7b003828d8ab': 'How has the training cost for LLMs changed over time according to the context?',\n",
       " 'c32ed26a-ecf3-4e38-b96c-1c3e32c757c5': 'How many GPUs did Microsoft’s Phi-2 use during its training?',\n",
       " '31f9e1fb-4db7-40a7-affb-b2ea60676409': 'For how many days did Microsoft’s Phi-2 train on 96 A100 GPUs?',\n",
       " '05784cca-887a-4739-8228-6832ac6534db': 'What is the estimated cost of training Microsoft’s Phi-2 based on current Lambda pricing?',\n",
       " 'f6401c38-77f5-46da-9c64-d7fcc2c546ef': 'Which GPU model was used by Microsoft’s Phi-2 for training?',\n",
       " '66514c37-9341-4832-8047-a1db99215d0b': 'Besides OpenAI, which organizations from the Middle East are mentioned as producing LLMs?',\n",
       " '91a7cd04-1422-4afb-901f-a02b3830d0d7': 'What factors contribute to the significant training cost of large language models?',\n",
       " '8afe887a-9320-4b8b-bd77-c32a2b510b14': 'What programming languages can LLMs use to build a full interactive application?',\n",
       " '00a65dba-5cf4-4f8c-a8c5-edc9789a0a74': 'How can tools like React be integrated when using LLMs to build applications?',\n",
       " '5c13d2b8-7926-42b1-ae8f-256e9841bb1f': 'What is the name of the new feature released by Anthropic that enhances LLM capabilities?',\n",
       " '01eec326-c547-4600-830e-5511cf885be4': 'How was the release of Claude Artifacts initially received or noticed?',\n",
       " '4e4ebd05-3901-4b58-8b57-e1db4ce7128a': 'What version of Claude was announced alongside the introduction of Claude Artifacts?',\n",
       " 'b6106540-a318-4bec-a258-8735d3d374fd': 'What unique ability does Claude Artifacts provide to users within the Claude interface?',\n",
       " '3c4db51d-64c8-4327-8e60-9e6a43aef8dd': 'What example application was mentioned as being generated entirely by Claude?',\n",
       " '0ce653d1-9ae9-4268-bfb5-49bef97fa80f': 'How does prompting affect the ability of LLMs to write code?',\n",
       " 'f1f8a30e-e020-447f-a87c-633708f58fec': 'In what way does Claude Artifacts differ from previous LLM coding capabilities?',\n",
       " '34c90922-9599-4edc-b755-3ad94a6f3763': 'Why might Claude Artifacts have been overlooked initially during its announcement?',\n",
       " '451525b6-7a97-4d65-ac1f-1a92ac34f8b3': 'What other companies besides OpenAI have multi-modal audio models?',\n",
       " '4c79fd8b-e815-4460-b316-abaa4cc8fcbd': 'What type of input does Google’s Gemini accept?',\n",
       " 'b3d2c5ec-e25b-40cf-b42a-73622dbbf6fb': 'How do Google Gemini apps compare to ChatGPT in terms of audio capabilities?',\n",
       " '18f09225-5eae-4b8d-ba37-5e9ef555cd47': 'When is Amazon planning to roll out voice mode for Amazon Nova?',\n",
       " '8bd567d8-1f62-49a6-bea4-5ca2953d2436': 'What unique feature did Google’s NotebookLM introduce in September?',\n",
       " 'dd50637d-a2ad-4f79-8005-cdef385ed351': 'How does Google’s NotebookLM create conversations?',\n",
       " '890687ff-ce95-4fab-a78f-b39e50653ac0': 'What new addition did Google’s NotebookLM make after its initial release?',\n",
       " '0f7871cd-7154-4bce-be7d-a6419330bef3': 'What example was given to demonstrate the custom instructions feature in NotebookLM?',\n",
       " 'bb99c5c0-810b-4298-adb0-525234aac52c': 'Which company pre-announced a voice mode feature for one of its products?',\n",
       " '0fe90b6a-119f-4ce5-99ff-72afd1a99013': 'What is the significance of the audio element mentioned in the context?',\n",
       " '31e1d11c-b144-4ec4-9af5-c1281d54ab79': 'What new feature was demonstrated in the May 13th announcement of GPT-4o?',\n",
       " 'b38d6e52-5a32-4bcc-b605-5f288496355e': 'What does the \"o\" in GPT-4o stand for?',\n",
       " 'cc504a85-1823-4f37-9578-8a5ef1ea753e': 'What type of input and output does the new voice mode of GPT-4o support?',\n",
       " '00d94e8f-7be7-4e15-92a1-3d4caa14d9d5': 'Did the new voice mode require separate TTS or STT models to function?',\n",
       " 'f7db2561-1b41-469c-9f1f-bab494a0b429': 'Which celebrity’s voice did the GPT-4o demo voice resemble?',\n",
       " '31c0fd1d-8b70-41da-9a64-57d6ab5bd855': 'What was the name of the voice used in the GPT-4o demo?',\n",
       " 'fc19a3ff-71e9-4fbd-b10c-a31e52517d3f': 'Why was the voice from the GPT-4o demo never released as a production product?',\n",
       " 'da2d5e88-a49b-4b71-82d3-3ebabe06a20f': 'What caused confusion regarding the release of the new voice mode after the demo?',\n",
       " '7f640b5f-4f3d-4e32-93e8-30f8254a2ace': 'Where was the delay in releasing the new voice mode discussed?',\n",
       " '137b8caf-1c4f-4cc9-8d8f-257dc748a779': 'Is the new voice mode currently running in ChatGPT’s “4o” mode according to the context?',\n",
       " '0cef7efb-28b9-4afe-a245-c4816576958b': 'What new feature did ChatGPT introduce in December related to live video?',\n",
       " '3ba046bb-565a-402a-9e77-295842b2ee76': \"How does ChatGPT's voice mode utilize the camera feed?\",\n",
       " '1e63cccd-f84b-43df-86fa-b0a9cac17414': \"What can users do with ChatGPT's live video feature?\",\n",
       " '3a88c5ae-d8eb-4220-a748-f174208ec68e': 'Which company released a similar live video feature before ChatGPT?',\n",
       " '5683a12e-50af-47ae-bd36-b0ed1012e57d': 'When did Google Gemini preview their live video feature?',\n",
       " '9cc75ac2-aa33-4e24-883f-a9b988a865f4': \"How does the timing of Google Gemini's feature release compare to ChatGPT's?\",\n",
       " '2ca45fd7-3739-4806-84ac-739cf4a4771e': 'What is the significance of December in the context of the recent updates?',\n",
       " '123b4c12-7dc0-42e4-ab61-f055173d4fdd': 'What does the live video feature allow the model to do in real time?',\n",
       " 'dceda587-d63a-442a-9200-02fde5323fe6': \"How do ChatGPT and Google Gemini's live video features relate to each other?\",\n",
       " '24f598b7-7c40-4034-a642-5fba717f9f53': 'What does the phrase \"December was a lot\" imply about the updates released that month?',\n",
       " '59dc1855-4798-4ecb-9737-8293a1cdc335': 'When was Google’s Gemini 15 Pro released?',\n",
       " '5aea152c-c844-4c47-964e-c26d16f57466': 'What level of output quality does Google’s Gemini 15 Pro produce?',\n",
       " '0082694c-3b92-456a-8d06-b8e71e283bd3': 'What is one of the brand new capabilities introduced by Gemini 15 Pro?',\n",
       " '0259c850-eb1f-4507-afbb-8949244b2fe9': 'How many tokens could Gemini 15 Pro initially handle in its input context length?',\n",
       " '283314ba-d89e-4dff-a488-3efe07e8997c': 'To what number was the token input context length of Gemini 15 Pro later increased?',\n",
       " 'b833c251-2cd2-41d3-8cf1-e45bc7d65771': 'What unique type of input can Gemini 15 Pro process?',\n",
       " '6ac6c053-569a-4d69-88b6-898abdcbaaa1': 'Where was the author’s writing about Gemini 15 Pro featured?',\n",
       " '1dd2cf6c-075e-4fae-b682-a72823309a83': 'What was the title of the author’s article about Gemini 15 Pro?',\n",
       " '4c9f07df-4118-4651-858f-1d91d48c8891': 'What event featured the author as a talking head due to their article on Gemini 15 Pro?',\n",
       " 'c620cba1-b719-4a33-8e86-4e7acf386acd': 'In which month did the Google I/O opening keynote, where the author appeared, take place?',\n",
       " '4a30224c-a1fa-48be-a7d2-2e6cd4fb8a7b': 'What is the advantage of a 64GB Mac for running models according to the context?',\n",
       " '8d14bd88-5818-4929-b66b-ba02bdd41bd0': 'Why do many models favor NVIDIA’s CUDA over other platforms in practice?',\n",
       " '121e655a-39c5-4e4e-bf4c-4df8f2516aa6': 'How did the llamacpp ecosystem contribute to running models on Macs?',\n",
       " 'efbfd6b8-89a2-40bb-82ba-ae9cfc62ab48': 'What is Apple’s MLX library described as in the context?',\n",
       " '25041dce-ed3d-4f75-b45a-9df6ac6fe833': 'What role does Apple’s mlx-lm Python library play in running models on a Mac?',\n",
       " '6dfc7163-7bd7-4a42-bcd2-a365aaba656e': 'How many models does mlx-community on Hugging Face offer that are compatible with MLX?',\n",
       " '4dd1658f-e6bc-42d0-9dff-886e1f27d44b': 'What type of models does the mlx-community on Hugging Face provide?',\n",
       " '2f9c457b-37c9-4c87-a3f1-af024af8d035': 'Why is Apple’s MLX library considered a breakthrough for running models on Apple Silicon?',\n",
       " 'f959e044-6da5-4ac5-bdaf-74e8ca814714': 'How does the shared memory between CPU and GPU benefit model running on a 64GB Mac?',\n",
       " '01e9eef9-9c18-45aa-9e1c-d166d450b31a': 'What is the significance of converting models to the necessary format for MLX compatibility?',\n",
       " '2d85addc-0bf7-4b13-a301-8bfa9e5ea4fb': 'What does it mean when the text says language models are gullible?',\n",
       " 'ea5094b9-fca1-45ee-9752-f15fb2b1de86': 'In what order do language models \"believe\" the information they receive?',\n",
       " '18897cfc-ce6c-4511-b086-af400a78b7f1': 'Why is it important for language models to believe what we feed them?',\n",
       " '6a85c8c7-32f8-4173-8914-04eb79339aca': 'What is a limitation of language models being gullible according to the context?',\n",
       " '7a88e669-cae2-4289-ae43-628c3b3162fe': 'How does the gullibility of language models affect their usefulness as personal assistants?',\n",
       " 'e927d312-8f0e-47c3-bc7b-20e957789e00': 'What analogy is used to explain the problem with language models believing everything?',\n",
       " 'ccd65504-8fcf-48b8-beda-68f977b78c02': 'Why would hiring a real-world personal assistant who believed everything limit their positive impact?',\n",
       " '89d87c29-a693-4d1d-9e64-cace4f2edcb5': 'What challenge is highlighted about building AI personal assistants?',\n",
       " 'adc63251-d76a-44bb-a41a-f0d3a8e356ff': 'How does the context describe the relationship between training data, fine-tuning data, and prompts?',\n",
       " 'a9c75aec-1b3d-414e-bf82-81efff7f726e': 'What is the main issue with language models that the context is addressing?',\n",
       " '69e0889c-ce72-4dfd-a760-b176eb454db3': 'When did Google release their gemini-20-flash-thinking-exp model?',\n",
       " '34f9ea67-e0ad-4b80-8904-5b4f0a3307b3': 'What is the name of Alibaba’s model released on November 28th?',\n",
       " 'b1389c08-8d02-48fd-bd74-e5392821e2ac': 'Under which license was Alibaba’s QwQ model released?',\n",
       " 'f6d43516-b8f4-47bd-8b9b-9c8e207fb9c7': 'Which Alibaba model was released on December 24th?',\n",
       " '6f125559-3ccc-44ed-a140-dcd23b0ebb5b': 'What type of model is QvQ described as?',\n",
       " '7df4f0dc-21e2-46fb-adc9-37088b944ac1': 'Which model did DeepSeek make available on November 20th?',\n",
       " 'ac2482e6-3d19-4008-bd56-96175636318a': 'How was DeepSeek’s DeepSeek-R1-Lite-Preview model made available to users?',\n",
       " '33ed3b4b-c076-4761-9750-5e3f16930b0c': 'Who are the authors of the recommended reading on inference scaling?',\n",
       " '25b0d5bf-0ba7-4748-80dc-a8b62594c185': 'What is the title of the recommended article by Arvind Narayanan and Sayash Kapoor?',\n",
       " 'c07b0eb0-e1af-4032-8dac-b012158a43bc': 'Which models mentioned in the context were run locally by the author?',\n",
       " 'ede4d1a4-fa0d-40a7-8e7a-c8310b4ddc94': 'What does the phrase \"The year of slop\" refer to in the context of LLMs?',\n",
       " '4ec14a3c-13fd-4ec5-aebe-5e9df8b02b74': 'How effective is synthetic training data according to the provided context?',\n",
       " '81e5f057-9b73-4fe8-87d0-d8b0aecf7f02': 'In what way have LLMs become harder to use?',\n",
       " '76388440-d119-448e-ae39-75e3229ba42e': 'Why is knowledge described as \"incredibly unevenly distributed\"?',\n",
       " '8b9689a0-eb82-4d62-8136-00b49a2baa7c': 'What improvements are suggested for LLMs in terms of criticism?',\n",
       " 'c86f5260-9e3c-4278-b9e7-00f6ed8cc7c4': 'What significance does the tag “llms” have on the blog mentioned?',\n",
       " 'cf2ba7fe-112d-4e87-b187-9ea84aff9322': 'How does synthetic training data impact the performance of LLMs?',\n",
       " 'a9ed10a5-e932-4aa6-aecf-0d5a1321eba2': 'What challenges might arise from the uneven distribution of knowledge?',\n",
       " '8c852612-d692-43a8-866f-d9ff5d4ae0ce': 'What might be the reasons behind LLMs becoming harder to use despite advancements?',\n",
       " '8da88b44-966b-40c6-81e9-126fc3064dde': 'How could better criticism benefit the development and use of LLMs?',\n",
       " '79b6366c-e4e3-4c01-b1ee-7e126779ba73': 'How does the ability to run generated code help in verifying its correctness?',\n",
       " '9728908f-128c-42c0-a23d-d31f3c189592': 'What role does the ChatGPT Code Interpreter play in improving code generation?',\n",
       " '51acdb7c-6916-44b6-afde-79bb0bca181a': 'Why is hallucination less of a problem in code generation compared to other tasks?',\n",
       " '6541157a-5009-4388-b88b-e5e797eebf6e': 'What would be the benefit of having a Code Interpreter equivalent for fact-checking natural language?',\n",
       " 'd481320c-9593-4aa2-9a7b-1eedfc16c5af': 'How might software engineers perceive the capability of ChatGPT to write code?',\n",
       " 'c128fe7c-d4a3-49d4-8ca0-7e947964b01a': 'In what ways could ChatGPT’s code generation be seen as a threat to programmers?',\n",
       " 'a292e292-de83-4c65-94a5-31a43814021b': 'How does the iterative process of executing, processing errors, and rewriting code improve code accuracy?',\n",
       " 'bc8a8d03-042f-4c81-a05d-e3d6169b1c69': 'What challenges remain in fact-checking natural language compared to code generation?',\n",
       " 'cccd1b37-95b2-45da-93d9-4c781efe6580': 'How might the development of tools like Code Interpreter change the role of software engineers?',\n",
       " '96b01de1-4c6b-4b7c-95fc-8e29a8562dfc': 'What are the implications of reduced hallucination in code generation for software development?',\n",
       " '797d65c8-47e5-4a48-a5a0-ee194b26e128': 'What are embeddings and why do they matter?',\n",
       " '6107b12d-fae6-43ce-afbd-b96e6a3a621b': 'What recent developments have been discussed in the world of large language models (LLMs)?',\n",
       " 'f0366c29-883b-4b07-b7f2-56c156029fdb': 'How does llamafile enable running an LLM on a personal computer?',\n",
       " '2eaaae18-fee4-4e44-99e8-8783a21d6a7e': 'What is prompt injection and how is it explained through video, slides, and transcripts?',\n",
       " '999701f1-2b38-44f3-9609-0c421935bb0e': 'In what ways has AI-enhanced development influenced project ambition?',\n",
       " '3b991e94-cd80-426c-931f-b9f6da1990ab': 'What are GPT tokenizers and how do they function?',\n",
       " 'dd9aa56b-6200-4cac-95cb-d4068a654824': 'What does the phrase \"ChatGPT in a trench coat\" imply about exploring GPTs?',\n",
       " '75d02194-a34a-4844-8a4a-165765acd778': 'Is it feasible to train a model that outperforms ChatGPT for $85,000 and run it in a browser?',\n",
       " '2f7f9d8a-ba9a-4228-afc6-1a52c32735b9': 'How can Q&A be implemented against documentation using GPT3, embeddings, and Datasette?',\n",
       " '928c0ef0-0434-4fcd-bc00-de2c3eaa5aff': 'What happened when a lawyer cited fake cases invented by ChatGPT in court?',\n",
       " '84b3fc1e-3265-4456-9eb4-6b905c157684': 'What do some people miss when they complain that LLM improvement has slowed?',\n",
       " 'edaa3731-21b9-4334-874e-3212887db7b5': 'What new capabilities do multi-modal models offer?',\n",
       " 'ffd780be-ef05-4bbd-84c8-e25b12024bc8': 'How can prompts be applied in multi-modal models beyond text?',\n",
       " 'b476436d-df3b-49fc-80f0-5801de22fd3c': 'Why are voice and live camera modes described as \"science fiction come to life\"?',\n",
       " 'dc1846e6-649d-4381-a1f7-bf5136a261f5': 'Which emerging modes deserve special mention according to the context?',\n",
       " 'c63c8a89-20a7-413b-9893-749d57f6beaf': 'When did the ability to talk to ChatGPT first arrive?',\n",
       " '93df9a69-6067-4b9e-9446-3c83a82332f6': 'What technology did OpenAI use to enable conversations with ChatGPT mobile apps?',\n",
       " '67cc52d1-103c-4a13-acff-1e089a7eb12c': 'What is the name of the text-to-speech model mentioned in the context?',\n",
       " '2dcb6dfb-6238-4645-9ea3-998f2cdb9f3e': 'How did the initial voice interaction with ChatGPT function behind the scenes?',\n",
       " '2ec215d2-a277-4af7-b0c1-5226dd1dc5bb': 'Did the actual ChatGPT model process audio input during the early voice interaction feature?',\n",
       " '0f810fbc-0910-4365-b176-1b4cea9763a8': 'What recent legal action did the New York Times take against OpenAI and Microsoft?',\n",
       " '866e1553-50ec-4b7e-a07e-99d43e828b11': 'How long is the PDF document related to the lawsuit mentioned in the context?',\n",
       " '71b338d5-c617-46e6-91a5-3cc4af41ba1a': 'What makes the first few pages of the PDF document particularly noteworthy?',\n",
       " '91500e20-c39c-43b1-abed-51bf76266dc8': 'What topics are clearly explained in the rest of the lawsuit document?',\n",
       " '97f8d5b9-a35d-4f1a-94e8-23317c855fd1': 'Why does the author recommend reading the PDF document?',\n",
       " '79a6f520-d59b-4345-9159-d9589bbb4fcd': 'How does the author describe the complexity of the legal arguments in the lawsuit?',\n",
       " '1e35c810-4070-47a6-9f3e-f75eba2d281c': 'What is the author’s perspective on how easily the lawsuit will be decided?',\n",
       " 'ade9b4c5-817e-4e00-a1bb-fea2abed06a5': 'What potential impact does the author expect the lawsuit to have on technology?',\n",
       " '44fdf075-80d3-45b7-85a4-df72dc08fab3': 'Which companies are involved in the lawsuit mentioned in the context?',\n",
       " '73f1f3c1-f5d4-43c7-8adc-fda2e6d22a4d': 'What is the main subject or issue at the center of the lawsuit?',\n",
       " '597b003c-9b97-4a7b-b7c8-8f03be306a3e': 'What was the initial approach to protecting the prompt when @v0 first came out?',\n",
       " 'fe5ca015-56e3-4181-8862-6ac97a328a70': 'How did the team’s strategy change after the initial approach to prompt protection?',\n",
       " '3002fd24-4a64-477f-b180-24d5563ce7eb': 'What elements are mentioned as being removed from the prompt in the new approach?',\n",
       " '79d30ca3-cc1f-4dfc-a39b-c16ba2a31099': 'Why is a prompt without evals, models, and UX compared to a broken ASML machine without a manual?',\n",
       " '48bb091b-f9f0-41d6-b52b-2b242ac74d95': 'What does the phrase \"let it rip\" imply about the new approach to handling prompts?',\n",
       " 'f40599d7-1226-46af-8a4f-e4761893b438': 'What kinds of complexity were originally used to protect the prompt?',\n",
       " '964e222e-733b-4c57-93b0-9a211ee49603': 'How important is UX in the context of the prompt, according to the passage?',\n",
       " '511cc187-8e4b-4bdd-b44c-c90632d70143': 'What might be the consequences of having a prompt without evals and models?',\n",
       " 'd185e4a8-fe31-4d39-a5fa-cc5c47a30d28': 'What does the comparison to an ASML machine suggest about the role of documentation or guidance?',\n",
       " '7601ed6f-4afa-4a6a-8260-653985d79736': 'How does the passage reflect the evolution of thinking about prompt design and usage?',\n",
       " '7b55f066-b35e-45da-952a-d0874b0274fc': 'What are the top five keywords mentioned in the context along with their counts?',\n",
       " 'd6045a60-c810-443b-8e4e-5f65d1f9927a': 'How did the author obtain the data used for analysis?',\n",
       " '3fae9f4a-feb2-4412-83ec-b0645e972f10': 'What tool was used to extract data into a table from the screenshot?',\n",
       " '57fd7842-a47e-483b-ba37-54b1c92ab0b2': 'How were the entry titles incorporated into the data table?',\n",
       " '3dc2edbe-9be0-482f-8418-dd4c31391e0b': 'Which article received the highest number of visitors according to the data?',\n",
       " 'c51fc7a7-a990-4f70-8c36-f0ad48f88d54': 'How many visitors did the article titled \"Leaked Google document: \\'We Have No Moat, And Neither Does OpenAI\\'\" receive?',\n",
       " '8651ba6d-d1f4-4fce-9cc7-dac60c850c34': 'What is the pageview count for the article \"Large language models are having their Stable Diffusion moment\"?',\n",
       " '87144037-dbd7-4b69-9eca-0b404e3e90e3': 'Which article had the lowest number of visitors among the top entries listed?',\n",
       " '8d470e0f-6d60-406b-96ec-ae0b2fe0ae3b': 'What is the relationship between visitors and pageviews in the provided data?',\n",
       " '99f5d34c-b301-48e4-a17a-fb8b7dd954f5': 'How does the author describe their experience with the topics mentioned in the context?',\n",
       " '3fb72816-ca6e-4e4b-ad02-e9f9bd47dea1': 'What is the suggested reason why individual users should not feel guilty about the energy consumed by their prompts?',\n",
       " '8f3cc68c-6e99-4e73-b78c-89f7ec6b4292': 'How does the energy consumption of user prompts compare to driving a car or watching a YouTube video?',\n",
       " '346174b9-d270-40db-b21c-df18ee3afc97': 'What does the training cost of DeepSeek v3 indicate about future training expenses?',\n",
       " '76458e2f-c546-49e7-92d1-0803ab73f0c6': 'How much did the training of DeepSeek v3 cost?',\n",
       " '2c903173-d69d-4cfd-9df0-5cb221f87741': 'Why is it useful to compare the energy usage of less efficient models to commercial flights?',\n",
       " 'a6031403-e9bf-40d9-9ce1-89d7bc150053': 'How does the training cost of the largest Llama 3 model compare to commercial flights?',\n",
       " '407fe28a-48e7-4d98-b30e-40b080da999c': 'What route is used as a comparison for the energy cost of training the largest Llama 3 model?',\n",
       " '9a53104e-53df-4e4a-b7f1-7aab225012ce': 'What is the significance of the Llama 3 model being used by millions of people after training?',\n",
       " '42ceb7ad-4c88-425f-8334-51dbc941f6be': 'According to the context, what happens to the training cost once a model like Llama 3 is trained?',\n",
       " '2c64d769-9497-4ac9-8717-e7763852bf81': 'What overall message does the context convey about the environmental impact of AI model training and usage?',\n",
       " 'a098f741-a94a-4f23-91f1-713285d7ba22': 'What is the overall opinion expressed about the use of LLMs on a personal level?',\n",
       " '00a48335-c0e7-438e-b13b-a5f8ff9ba542': \"How have LLMs impacted the author's productivity and entertainment?\",\n",
       " '13bb8206-0fe2-4d83-adbf-89e8a5dd052f': 'What potential benefits do people gain by learning to use LLMs effectively?',\n",
       " '81138b3c-c045-440f-961f-75f4d8d6c965': 'What are some of the differing opinions people have about the value of LLMs?',\n",
       " 'fa346fff-7da3-4947-be62-7eed4aa2d5c5': 'What are some negative perceptions people have about LLMs?',\n",
       " '1d678505-d38f-44a6-a275-32b034715aab': 'How do some people view the threat level posed by LLMs to humanity?',\n",
       " '4fb61c76-7a35-448e-bafc-c0b66728c0a6': 'What surprising fact about LLMs has been learned this year?',\n",
       " '4530491a-f9ed-46c5-859e-1b0845daf342': 'How difficult are LLMs to build according to the context?',\n",
       " 'a6d11176-a12c-4391-ada0-9638ae6c9eda': 'Why might some people still be unconvinced about the value of LLMs?',\n",
       " '5ff16148-22c7-46dd-bc75-8c31e98be2eb': 'In what ways can LLMs contribute to improving quality of life?',\n",
       " '56a90e57-3efd-4083-bcf7-5fdbbfbf1fc9': 'What is the biggest innovation mentioned in the context regarding model scaling?',\n",
       " 'b2a6db99-9996-4ce3-a46f-a772589ed6e5': 'How does the new approach to scaling models differ from traditional methods?',\n",
       " '2196b62d-acd8-4fb7-b019-776ffca07851': 'What is the name of the model sequel mentioned in the context?',\n",
       " 'aab79b78-af6b-4fa3-921c-d7d65b7c1f8e': 'Why was the model named o3 instead of o2?',\n",
       " '6013a71d-e9a5-414d-812c-960c897ad78c': 'When was the o3 model announced?',\n",
       " '4afe5388-ce05-4b02-a7b9-f08604347a03': 'What benchmark did o3 achieve an impressive result against?',\n",
       " '7334b3bd-e98b-443a-aff0-7f4fdaedd7fb': 'Approximately how much compute time expense was involved in achieving the o3 result?',\n",
       " '1c049a17-a871-417f-be6a-1b5b394364a3': 'When is o3 expected to be shipped?',\n",
       " 'aac707a6-14c7-43cd-b9ec-09cba63aa98b': 'According to the context, do many people have real-world problems that would benefit from the compute expenditure required by o3?',\n",
       " 'f74c8be0-6a7b-4215-98c7-863b32371e96': 'What does the context suggest about the significance of o3 in LLM architecture?',\n",
       " 'ecb6e750-8f24-4c24-9924-b9131eeed702': 'When was GPT-4 officially released by OpenAI?',\n",
       " '9d6b54b6-0ee6-4153-9953-efec27b92a39': 'Which company provided a sneak peek of GPT-4 before its official release?',\n",
       " '65e90719-6b93-4855-b74d-be0166a503f7': 'What product did Microsoft integrate GPT-4 into during the sneak peek?',\n",
       " '828a27a9-e7b9-40ea-8481-aec6b63401bd': 'What is the name of the upcoming model from Google mentioned in the context?',\n",
       " 'e0e22c6c-aab0-4273-85b8-c7deff7aafa5': 'Has Google’s Gemini Ultra been made available for public use yet?',\n",
       " 'fb406198-0a36-454b-8a0b-ef62b6dd123b': 'Which team is working to create a model that can outperform GPT-4?',\n",
       " '7df4ee69-37c7-4e14-8e9f-ec4172cbd3fe': 'When did the team behind Mistral release their first public model?',\n",
       " 'ab0396dc-aa36-4092-9bba-cd4a9c1a8ffb': 'How many significant improvements has the Mistral team released since their first model?',\n",
       " '45095a48-d589-4275-ac55-1ae5fef81531': 'According to the context, have any alternative models surpassed GPT-4 so far?',\n",
       " 'a074287c-051e-492e-99c6-2372d2897f34': 'What is the general sentiment about building a model better than GPT-4 based on the context?',\n",
       " 'd11fbf78-2cab-4449-96bb-1b8418e83845': 'What are some of the annotated presentations given in 2023 mentioned in the context?',\n",
       " '7b55f9d8-feb5-40c1-901c-b050f5e077d5': 'Which presentation explains prompt injection and includes video, slides, and a transcript?',\n",
       " '6c36a645-b1d1-4816-8552-2e3ca4a31f55': 'What topics are covered in the talks related to Large Language Models (LLMs)?',\n",
       " '50e64164-f3cc-4bbb-bedb-f9f668d87c85': 'What is the focus of the presentation titled \"Embeddings: What they are and why they matter\"?',\n",
       " '7825c8de-0c78-402d-98a3-d34a83a5bb18': 'Which talk discusses financial sustainability for open source projects?',\n",
       " '9e2c3ca2-fc4d-4cb2-9782-a999c0569073': 'On which podcast was the topic \"What AI can do for you\" discussed?',\n",
       " '7d99886f-9d67-4cdb-ac8d-18e78672492f': 'What is the subject of the podcast appearance on \"Path to Citus Con\"?',\n",
       " 'ca1a79dc-9b77-491d-9bc0-e98a27714dde': 'Which podcast episode is titled \"LLMs break the internet\"?',\n",
       " '7ce878c2-1337-4368-8455-40a9cf7cd63b': 'What was discussed on the \"Rooftop Ruby\" podcast?',\n",
       " 'f846f2b2-881d-4c1a-b2d5-d3a044db7d03': 'Which podcast covered thoughts on the OpenAI board situation?',\n",
       " '6dbfca97-a714-473c-9bb9-a5710f379277': 'What ethical concerns arise from training AI models on people’s content without their permission?',\n",
       " '6c851a9f-ab8b-4ecd-aef9-b43645993a8f': 'How does the quality improvement of AI models over time affect the ethical considerations of their use?',\n",
       " '4b2103b7-1e31-4d85-a01e-6a3f3423094e': 'In what ways have AI models impacted human society according to the context?',\n",
       " '5396f836-3257-4e33-9bb7-c3bbfcdc8c1c': 'Which professions are mentioned as having lost work to AI models anecdotally?',\n",
       " '140cc03a-9cb1-4b84-b0e4-c06010576645': 'Why is it difficult to objectively measure the impact of AI models on society?',\n",
       " '8f356557-1051-4d1a-a53e-2626ab68c83c': 'What kind of stories does the author hope to see more of in 2024 regarding AI and its effects?',\n",
       " 'a7f27ac1-645b-4b4e-a3bc-d39d36986822': 'How does the author differentiate between law and ethics in the context of AI training?',\n",
       " '0e201d1c-90a0-4ed4-952a-70fd6a3ddb54': 'What tool was used to generate the tag cloud for the author’s blog content in 2023?',\n",
       " '6cfaf412-f358-46d3-9077-23157ebbeb25': 'Why might the question of training AI models on people’s content without permission be considered pressing?',\n",
       " 'b7def787-b502-4c89-a268-7e8017a70685': 'What role does dedicated journalism play in addressing the issues raised about AI models?',\n",
       " '97103374-d726-4590-8a01-764873adf1ac': 'What new feature did the Chatbot Arena team introduce in December?',\n",
       " 'e863b6d2-f2c2-4115-b3df-e169620a747b': 'How is the new leaderboard in the Chatbot Arena feature driven?',\n",
       " '2992c0b7-acdb-48ed-8125-d26d6b0ee842': 'What does the voting mechanism in the Chatbot Arena leaderboard involve?',\n",
       " 'ef98bf77-cc25-4788-8b9b-29302322ec92': 'Why is the new feature considered a commodity according to the context?',\n",
       " 'bfd84a6d-c2c5-4fd7-ac37-f8c3390d8112': 'What project is the author working on that involves a similar feature?',\n",
       " 'a710cb1b-31de-495c-992a-dd8ff8a25830': 'What is the goal of the author’s version of the feature in the Datasette project?',\n",
       " '51a0d52a-79c2-41ba-b517-5dc27abd9565': 'How does the author plan to let users interact with their own data in the Datasette project?',\n",
       " 'a298ce08-529e-481a-a1d4-cf41af9f3941': 'What programming language is mentioned in relation to writing one-shot programs?',\n",
       " 'ee14ef2d-2723-4706-9282-a488b41085ac': 'What tool or library is enabling the author’s pattern for writing one-shot Python programs?',\n",
       " 'd99bc5ed-15b6-4dde-8fe8-452addb6c679': 'How does the author’s approach relate to the feature introduced by the Chatbot Arena team?',\n",
       " 'b7224f31-aa21-4396-8ca5-108c22af8958': 'What is described as the crucial secret behind good system prompts?',\n",
       " '2e496e05-d9b1-4de4-8960-1c39f264c9ae': 'According to the context, what is the recommended approach to writing system prompts?',\n",
       " 'cb82285a-e571-447d-99ea-17191ab906d3': 'Why is test-driven development important for creating system prompts?',\n",
       " 'd6152438-8f43-4cdf-b37d-33cfcffa1c7a': 'What has become clear in 2024 about building useful applications on top of LLM-powered systems?',\n",
       " '8199f41b-2c6f-4632-99e9-41677a53c066': 'What skill is identified as most needed for building applications with LLMs?',\n",
       " 'b24e30e4-152a-425a-83b2-4b80572194fa': 'How does having a strong evaluation suite benefit developers working with new models?',\n",
       " '921ec47c-99b9-4578-9a0b-0a2ce3b306d1': 'What advantages does a strong eval suite provide in terms of product development?',\n",
       " 'a77831d6-5b3d-4725-8255-1548a136c61f': 'Who is mentioned in the context as a source or authority on this topic?',\n",
       " '811ca61c-0e64-48f6-aa64-fd56c6cf97cd': 'How does test-driven development differ from traditional methods when applied to system prompts?',\n",
       " 'dcc6c9a4-f137-4273-b77b-74f5f52de1ef': 'What impact does writing good automated evals have on competition in the field?',\n",
       " '18e0438d-bddb-4cb5-a8c7-79e2221f6e90': 'What were some of the major challenges involved in constructing railways in the 1800s?',\n",
       " '5eb30987-bbf5-4833-9c23-1683c07bda7f': 'How did the construction of railways in the 1800s impact the environment?',\n",
       " '76ba2f6c-5bf2-4db9-8c99-2c79530b3f5a': 'Why were many of the railway lines built in the 1800s considered unnecessary?',\n",
       " '7396b566-1796-464c-9aa0-5f2e70ca6f14': 'What was a common issue with multiple railway companies during the 1800s?',\n",
       " '2434cbe1-e73a-49c2-828f-2444012add3f': 'How did the railway construction bubbles contribute to financial crashes?',\n",
       " '6eba5781-ee3b-4ec3-ae72-9e1696eaa4e6': 'Which financial crashes are associated with the railway construction bubbles?',\n",
       " '2a547ebd-a229-416e-95f7-775293521e57': 'What is the significance of the UK’s Railway Mania in the context of railway construction?',\n",
       " '750a5211-be28-4f9e-9aa6-aeeabe2af16e': 'What were some of the positive outcomes of the railway construction despite the financial crashes?',\n",
       " '48414617-8442-416e-a7fa-9eaed67f12f7': 'How did the railway construction bubbles affect the economy and businesses at the time?',\n",
       " '37c0cea3-6d14-4002-a7ea-18b7a8346cae': 'Why might the phrase \"The year of slop\" be relevant in the context of railway construction and its consequences?',\n",
       " 'b6ae758e-0ad1-4f82-99b4-b12651ee0229': 'What kind of ecosystem is described in the context?',\n",
       " 'dc66556e-7b5f-4a82-acc0-6c35f977df94': 'What activities are people engaging in within this ecosystem?',\n",
       " 'd2226757-c126-458c-81dc-fae7ba5a5c88': 'What is the Hugging Face Open LLM Leaderboard used for?',\n",
       " '678cd1f3-cfe2-4e71-ba57-c07d3cc2ed88': 'Why is it difficult to count the number of models being trained and published?',\n",
       " '252e387b-4be7-4c8f-9aa5-8610918f9382': 'According to the context, what type of model is usually the best overall openly licensed LLM?',\n",
       " 'c2a8915c-329b-4a75-b28e-774e4673dca5': 'What factor determines the best fine-tuned community model at any given time?',\n",
       " 'b512e4db-733b-454a-b0cb-4973b1035a3c': 'How does the advantage of open models compare to closed models?',\n",
       " 'fb371d04-8ad3-4f8b-a0de-1885603741ff': 'Why do closed, hosted models have a disadvantage compared to open models?',\n",
       " '70216909-2b76-497e-b306-0412032beb65': 'Who are the contributors improving open models according to the context?',\n",
       " '420ffdcc-3928-4a2c-9bac-7fa9bcd5381b': 'How frequently does the status of the best openly licensed LLM change?',\n",
       " '56bd2ee8-11bd-4042-adee-d472814bc9e2': 'What has happened to the prices of running prompts through top tier hosted LLMs in the past twelve months?',\n",
       " 'f6938e0d-3c7d-4a96-8558-ea2b7adcb808': 'How much was OpenAI charging for GPT-4 input tokens in December 2023?',\n",
       " 'c5a5a744-186d-4924-9306-aeaee8fab2af': 'What was the cost per million input tokens for GPT-4 Turbo in December 2023?',\n",
       " '18c85534-9811-4bd8-9a9e-0f31e2f075bc': 'How much did OpenAI charge for GPT-35 Turbo input tokens in December 2023?',\n",
       " '7e17e1b2-1a68-4cc0-929b-8bac7660a49a': 'What factors contributed to the crash in LLM prices according to the context?',\n",
       " 'ee79290f-3a32-4ffd-ad32-406debb28df3': 'What example is given to illustrate the use of bland and generic content generated by LLMs?',\n",
       " '6d4676ea-21f0-4f98-9e1c-5b7ac74d1f29': 'Which month and year is referenced for the OpenAI pricing page snapshot?',\n",
       " '59a7ad9d-37a8-4bf8-91df-4185a4b0a193': 'How does the cost of GPT-4 Turbo compare to GPT-4 according to the December 2023 pricing?',\n",
       " '23ac92d9-2411-4976-a088-8d6edbb1b5fa': 'What is the significance of the Internet Archive in the context provided?',\n",
       " '612b1b2d-53fe-441d-83df-40dd6d917395': 'What does the context imply about the efficiency of LLMs over the past year?',\n",
       " '9afda41d-4da1-4021-81d2-399f551604d8': \"Why does the author find the model's capabilities astonishing?\",\n",
       " 'be3983c1-bf98-43f6-ab8d-8f97b520636e': 'What did the author originally think was necessary to run a model like GPT-4?',\n",
       " 'd755ad59-9136-408a-87f4-ede9d0e45e9f': 'How much RAM does the author have, and how does running these models affect it?',\n",
       " '8ce2b7c5-38f1-453d-a458-a3701e1fd9d8': 'Why does the author not run these models often?',\n",
       " '5fbe9e50-8efd-4815-807a-d8954f1398b7': 'What does the author credit for the ability to run these models despite hardware limitations?',\n",
       " 'ace11d8b-6422-420a-8230-89d8aeebdd07': 'What does the author mean by \"low-hanging fruit\" in terms of model efficiency?',\n",
       " '17c24734-d369-44e9-a5d6-5ff25ab9f923': 'How has model efficiency changed over the past year according to the author?',\n",
       " '89cc4a4e-1911-42ba-a82c-84957eef77e6': 'What are the implications of the training and inference performance gains mentioned?',\n",
       " '4ec1714c-99c9-4d04-90d0-21a58044f61f': 'Does the author believe that model efficiency improvements have reached their limit?',\n",
       " '1f6b1d98-9d14-411f-aa90-7faa6cfdf0bc': 'What expectations does the author have for future developments in model efficiency?',\n",
       " '20971e08-fbb7-47bb-abd4-1ffe68940e10': 'What advantage do software engineers have when working with coding interns according to the context?',\n",
       " '6b744258-e24c-4e6b-9df0-a0db7d06b775': 'How can software engineers use their deep knowledge in relation to coding interns?',\n",
       " '09f77520-e6c1-424f-9a1f-853ba935a1ed': 'What is described as \"diabolically complex\" in the context?',\n",
       " '3c2cb352-72e5-4056-85e6-4da94a114a99': 'Who produced the first major story on the unlicensed training data behind Stable Diffusion?',\n",
       " 'c5e49eeb-3000-479f-a7f2-e7eb21719577': 'When was the first major story on the unlicensed training data behind Stable Diffusion produced?',\n",
       " 'acc99de7-b2fc-400a-b0c1-6dba34821020': 'What type of data was used to train Stable Diffusion according to the context?',\n",
       " '5831839e-0724-440d-a6b9-e3c27966c959': 'Besides Stable Diffusion, what other models have been trained on unlicensed data?',\n",
       " 'dfc0fc08-123d-4cea-9cc5-ccafd6aba4c0': 'What does the context imply about the prevalence of unlicensed data in training major LLMs?',\n",
       " 'cfe5cd2d-a1f8-4f97-9912-1a72cc36880e': 'How does the context describe the relationship between software engineers and coding interns?',\n",
       " 'df41ca54-6824-48a4-9718-0934634a347b': 'What ethical concerns are hinted at in the context regarding training data?',\n",
       " '7b0dd037-5dd4-4075-842b-5965b6dabccf': 'When did the ChatGPT Advanced Voice mode roll out?',\n",
       " '1315d6c7-adf8-4503-a1e6-ad5f39756586': 'How did the rollout of ChatGPT Advanced Voice mode occur?',\n",
       " 'f06c5384-c3cb-4e07-8a24-ee841fc4b23e': 'What activity did the user mention doing while using the Advanced Voice mode?',\n",
       " 'c5a6635c-76af-437a-88f4-c18ff90c9c6a': 'What aspect of the Advanced Voice mode significantly improved the user experience?',\n",
       " 'fbfb3238-77ea-43ba-b5ac-8728b0246e46': 'What other OpenAI feature did the user experiment with besides Advanced Voice mode?',\n",
       " 'f8f6f90b-ed6a-471d-a43a-99a2a80b4d0b': 'What unique capability does the Advanced Voice mode have related to accents?',\n",
       " '62a315a9-d026-4e14-9564-7ff887d11df8': 'What specific accent did the user ask the Advanced Voice mode to imitate?',\n",
       " '38238b42-6cda-4e73-a333-7b4ce23ce5f4': 'Which animal did the user ask the Advanced Voice mode to pretend to be?',\n",
       " '2e9ef985-ff2f-4878-97d0-ab406233b05c': 'In what language did the user request the Advanced Voice mode to speak while using the Russian accent?',\n",
       " '110690a3-bebe-4f81-b492-54b48a5b0b63': 'How did the user describe their experience with the Advanced Voice mode overall?',\n",
       " 'a276ffdc-5b61-47a4-b802-00d5d8413c13': 'What is the cost per million tokens for OpenAI’s most expensive model, o1?',\n",
       " '91418da4-3239-42af-adda-a5957f4c5eab': 'How much does GPT-4o cost per million tokens, and how does this compare to GPT-4?',\n",
       " 'f6a17bc7-5558-48b9-880b-7f053e375bb2': 'What is the price of GPT-4o mini per million tokens, and how does it compare to GPT-4 and GPT-35?',\n",
       " 'bd30cc14-a49b-49aa-b8f0-0597079433ea': 'Which model provider offers Claude 3 Haiku, and what is its cost per million tokens?',\n",
       " '74fb4d30-6637-4750-92a3-e66a6e11fb34': 'How much does Google’s Gemini 15 Flash cost per million tokens?',\n",
       " '13debd42-355d-4c10-a3a8-2b3f2a16da0f': 'What is the price difference between Google’s Gemini 15 Flash 8B and GPT-35 Turbo from last year?',\n",
       " '2d5e8782-7cfe-47e7-8caa-32f63ed9a260': 'How much cheaper is GPT-4o compared to GPT-4?',\n",
       " '297c5c8c-64b2-4230-b8dc-bd27b9ee738e': 'By what factor is GPT-4o mini cheaper than GPT-35?',\n",
       " '30270483-f302-4069-81d8-dda2cac67aad': 'What pricing trend is being tracked under the llm-pricing tag?',\n",
       " 'b0e5d3ea-fda6-40d5-9c8c-3ff0e1d6404c': 'Which model mentioned is the cheapest per million tokens, and what is its cost?',\n",
       " '98348c36-d7a4-4f39-89ac-0c3fc94d537b': 'What sizes are the Meta Llama 32 models mentioned in the context?',\n",
       " '7b763853-4dc9-49e4-b376-320cff0abd20': 'How does the performance of Llama 32 models compare to GPT-4 according to the context?',\n",
       " 'f3837331-0ac3-46e2-87a5-d9fa5b8e78fc': 'On which device is the Llama 32 3B model run in the example?',\n",
       " '55ff4b55-2d1e-4263-944f-faab2ce0a649': 'What app is used to run the Llama 32 3B model on the iPhone?',\n",
       " 'dc3919ef-f02e-42a4-801c-d45b23f78aa2': 'How large is the Llama 32 3B model in terms of storage size?',\n",
       " '8fc7f851-9230-45c2-870d-a912eb607bfa': 'What kind of plot outline was requested from the Llama 32 3B model in the example?',\n",
       " '8237be18-a96e-4707-ac5d-fbdeed800438': 'How fast did the Llama 32 3B model generate tokens in the example?',\n",
       " '86414673-10a1-4100-b834-4bda28d320a6': 'What genre does the requested plot outline belong to?',\n",
       " '5569d332-7386-4c81-bf90-8884e858f4f3': 'What professions are involved in the plot outline given to the Llama 32 3B model?',\n",
       " '988de177-39b3-4047-99be-2f9810f0aa8f': 'What is the general impression of the Llama 32 3B model’s capabilities based on the context?',\n",
       " '8db820a1-0969-45e2-a460-ef86c0714238': 'What is the approximate cost of processing 260 input tokens and 92 output tokens according to the context?',\n",
       " '78d13aff-b8a0-4866-bb71-2c4d204d632d': 'Why is the increase in efficiency and reduction in price considered a favorite trend from 2024?',\n",
       " 'e27cdf0b-cf45-427c-a47b-90e82f83d0f0': 'What is the significance of achieving LLM utility at a fraction of the energy cost?',\n",
       " '023a3b21-fa08-46dc-9cba-3d96fdced5c4': 'Which modalities are becoming common and which are starting to emerge in 2024?',\n",
       " 'ddee56f5-498e-4662-ae53-db48f2cf135c': 'What example is used to illustrate the rise of multi-modal LLMs in 2024?',\n",
       " '3e76a23d-6eb1-4edd-aa85-297159eb1966': 'When was GPT-4 Vision released and at what event?',\n",
       " '8485d100-e800-4c7c-8f69-ddb1a321027b': 'What is the name of Google’s multi-modal model announced in December 2023?',\n",
       " '91421308-2920-46ad-9c49-72f0cd7aff16': 'How does Google’s Gemini 10 relate to the 2023 and 2024 timeline?',\n",
       " 'e4a874a6-893e-40f5-bd98-e2cf0a1a1153': 'What key trend from 2024 is highlighted by the butterfly example?',\n",
       " '5a174479-161c-4c9c-b7d1-a24643a712c1': 'How does the context describe the evolution of multi-modal LLMs over the past year?',\n",
       " 'c11b55ef-d9ce-4100-8e7e-1879aae068b3': 'How do longer inputs affect the scope of problems that can be solved with an LLM?',\n",
       " '66c5356a-2967-45f5-86ee-9360d3770130': 'What is an example of a long input that can be fed into an LLM according to the context?',\n",
       " '386a7fc1-a2cd-4cc1-bf10-19386912be6e': 'Why are long inputs considered more interesting than short prompts for LLM use-cases?',\n",
       " '43335763-fb38-41ea-a51c-3049d81800c0': 'How can feeding example code into an LLM help in solving coding problems?',\n",
       " 'c301d319-b9b0-4b18-a70c-0c65d9717eb4': 'What advantage does using longer inputs provide when asking questions about a book’s contents?',\n",
       " '03bb9131-46ba-4c6e-a1b4-87b5e26c073a': 'According to the context, what pattern was used to build many of the author’s tools?',\n",
       " 'ec478530-1b2a-40c2-a249-342d7cfdb6ad': 'What is the relationship between the length of input and the reliance on information baked into model weights?',\n",
       " '15773246-552e-41c4-9680-71fa81c73292': 'Why might short prompts be less effective compared to longer inputs in LLM applications?',\n",
       " 'c482f015-b025-4d5f-a77b-5eadf467e3fb': 'What types of problems become more solvable with the ability to input longer text into an LLM?',\n",
       " 'fd6f05de-c33a-4e95-92fa-75cc674f2863': 'How does the context describe the impact of longer inputs on the capabilities of LLMs?',\n",
       " '886d9f52-cbbe-4aed-a37a-143b83672cb0': 'What are the two main factors driving the recent price drops in running prompts?',\n",
       " '24cc21df-b1a7-466e-bc5e-f075819549b2': 'Why is increased efficiency important in the context of LLMs and the environment?',\n",
       " '3f014e2f-7b44-4fdd-954e-03d3a88d009e': 'How do the recent price drops relate to the energy consumption of running prompts?',\n",
       " '70565770-3f35-426b-94d7-8e4174495675': 'What environmental concerns remain despite improvements in prompt energy efficiency?',\n",
       " '8d36dd8a-f29f-4609-8b6c-c033edcf836a': 'Why are concerns about the energy cost of individual prompts becoming less credible?',\n",
       " 'd30f7334-a5dd-47ce-b396-37e576a141e0': 'What example is given to illustrate the cost of generating descriptions using an AI model?',\n",
       " '710a2564-aed7-4131-8e47-7e76e6d3d2ab': 'How many photos are mentioned in the personal photo library used for the napkin calculation?',\n",
       " '43589aac-821f-4f6d-8c4c-605b619ef17f': 'Which AI model is referenced for generating short descriptions of photos?',\n",
       " 'b729db78-4777-46a7-afcb-7afce07263e5': 'When was Google’s Gemini 15 Flash 8B model released?',\n",
       " 'cbbd82c0-0481-47a6-aa2d-a3e6541b867e': 'What is notable about Google’s Gemini 15 Flash 8B in terms of pricing?',\n",
       " 'ac556b5c-c987-4588-b99a-2150eae337fe': 'When did Meta release the original Llama model?',\n",
       " 'bb749430-6b3a-41eb-9e45-cc3eecea9dbf': 'Who released code to get Llama working on a MacBook, and when?',\n",
       " '832bb45f-eafd-42d3-9ba2-45a574928acc': 'What comparison was made about large language models in the context provided?',\n",
       " '42e7f2cd-dd68-42f6-9e3f-c49b64bdf0cc': 'What event in July accelerated innovation in large language models?',\n",
       " '040df46e-492a-4f9f-8ec7-b0707a239473': 'What was a crucial new feature of Llama 2 released by Meta?',\n",
       " '8ac59645-9968-4fd2-9f9d-11d81986bd90': 'How did the release of Llama 2 impact the use of large language models?',\n",
       " '6c5ea740-7a08-4b39-9521-75e37f10bc9d': 'What does the context suggest about the availability of large language models today?',\n",
       " '938e44e4-9d4d-4754-856b-bcf15738cf98': 'How did the release of Llama influence the development of local LLMs?',\n",
       " '4840be41-fc7a-4845-b114-1e2b18d79c9d': 'What role did Georgi Gerganov play in the development of Llama?',\n",
       " '6552037e-25ad-4af1-90d5-a66ee7ea8b2f': 'What does the phrase \"Stable Diffusion moment\" imply about large language models?',\n",
       " '6c19c4fd-6889-42fb-903e-07b426957567': 'What was the typical context length accepted by most models last year?',\n",
       " '58b7aef7-5bf1-452c-8ed4-866408c64871': 'Which model was the notable exception in terms of context length last year?',\n",
       " 'f562fbf2-2135-4cea-9db3-7f7ca546d200': 'How many tokens did Claude 21 accept?',\n",
       " '5a686f19-b3ee-48c4-9a8c-da4097d1a9a5': 'What is one of the key themes of 2024 according to the context?',\n",
       " '623d5062-2bd4-4780-91bf-70f58bf336ca': 'What is the minimum context length accepted by serious providers today?',\n",
       " 'ca820a7b-b597-430f-9240-eda548d996e1': 'How many tokens does Google’s Gemini series accept?',\n",
       " '39db01a6-6017-48aa-ad0a-8b7724389f08': 'What version of Gemini is mentioned in the context?',\n",
       " '9c372b24-b4d9-484e-b481-f3ac98466aad': 'How does the context length of Google’s Gemini series compare to Claude 21?',\n",
       " '663e5221-0ec7-46d2-aa17-db9e51bd3e8d': 'Why is Gemini 15 Pro significant in the context of 2024?',\n",
       " '64b6124f-dfc0-45cd-a3ae-2b4a0de675e2': 'What trend in model capabilities is highlighted by the increase in context lengths?',\n",
       " 'db0577d6-4167-4b86-9562-b7bdce74a83f': 'What is the current accessibility of training a large language model (LLM) compared to before?',\n",
       " '8b50aad5-1642-402f-b1e3-fa1c6a608ccc': 'How does the author compare the difficulty of training an LLM?',\n",
       " '2c7aad67-be45-4853-a1a8-c5596aa16bdc': 'According to the context, how many countries have figured out how to build suspension bridges?',\n",
       " 'bef91955-048c-48ad-a16b-32c5969ba2d1': 'What correction does the author make regarding the number of countries with suspension bridges?',\n",
       " 'fb8bc4e9-2601-4aaf-b39a-4e5794dc10f3': 'Is it possible to run LLMs on personal devices according to the context?',\n",
       " '3d8d0a56-ad34-43cd-8fd4-aefb00181ab0': \"What was the author's expectation in January about running a useful LLM on a personal computer?\",\n",
       " '3bcc8459-a6aa-4f25-804a-39e2c89ed725': 'Which versions of GPT were mentioned as the main options earlier in the year?',\n",
       " 'bbdb6333-1d32-4210-8960-9eeae1991365': 'How much did the author think it would cost to run model weights on a server?',\n",
       " '001e0a77-757d-4898-a0eb-de008fa68a11': 'What analogy is used to describe the complexity of training an LLM?',\n",
       " '869c2844-5f9f-4f7b-985d-467e4a4f827a': 'Does the context suggest that training LLMs is now limited only to the super-rich?',\n",
       " 'bacc6e42-cf73-49c3-8edf-7ac67d82af19': 'What is the general definition of AI agents as described in the context?',\n",
       " 'd98ceef9-6209-4411-bf5c-884213a5338d': 'Why does the author believe there are few AI agents running in production despite many prototypes?',\n",
       " '98f3f2b5-53ad-488d-bfe7-66fb9735df96': 'What issue does the author identify as a barrier to the success of AI agents?',\n",
       " 'de1f48f8-2e5a-4b53-b4ac-954c887b780d': 'According to the context, what might be necessary to fully solve the problem of gullibility in AI agents?',\n",
       " '36f81542-5a2d-4e1a-825e-20f9c0eef604': 'How does the author feel about the timeline for AI agents to become truly effective?',\n",
       " '25d6911d-c112-42cc-a5e4-bbe031690137': 'What application of AI does the author suggest might be the best so far?',\n",
       " '178638bf-6059-4e1c-9a65-000c502292d3': 'What capability of large language models (LLMs) has become increasingly clear over the year?',\n",
       " 'cdb98097-3811-4413-82ef-8c4a07341c64': 'Why might achieving AGI be important for the future of AI agents?',\n",
       " 'f866fa80-f129-44ee-ba4b-b8829bb9b3a2': 'What does the author imply about the current state of AI agent prototypes versus production use?',\n",
       " '91b1f890-bd9e-46f8-a367-23799a522d87': 'How does the author describe the term “AI agents” in terms of clarity?',\n",
       " '2d47c3df-8685-4292-8904-a11f800f5fd4': 'What are the two main categories of AI agents mentioned in the context?',\n",
       " '4e84b6af-4e3a-49a0-bfe2-b96a1a6bf9ea': 'How is the \"travel agent model\" of AI agents described?',\n",
       " 'd365a71d-471a-49cf-9e89-6162a7a037d2': 'What role do LLMs play in the second category of AI agents?',\n",
       " 'e0d8302c-5523-4c2e-8db2-aad0ee1cb7ef': 'How is the concept of \"autonomy\" treated in discussions about AI agents according to the context?',\n",
       " '09bd221c-64cb-468f-88bc-33330cd57e71': 'What resource was used to collect definitions of AI agents, and how many definitions were collected?',\n",
       " '820fed89-b645-4dec-adc9-d17880726437': 'What tool or model was used to summarize the collected definitions of AI agents?',\n",
       " '08a9e371-7beb-4c28-8b32-b9d5b3521bea': 'What does the context suggest about the current state or availability of AI agents?',\n",
       " 'b9214b1b-9f40-4862-b0e0-fa9226e69b76': 'Why might the term \"autonomy\" be problematic when discussing AI agents?',\n",
       " '503875d3-41a3-4f1f-86a1-d451db972fbc': 'What does the phrase \"coming soon\" imply about AI agents in the context?',\n",
       " 'f59f9def-a037-4a7c-b362-dcd7b1394324': 'How does the context differentiate between acting on behalf of a user and running tools in a loop?',\n",
       " '220b702a-16db-4bdb-ac0b-ff621484ff44': 'Why is it not surprising that programming languages have simpler grammar rules compared to natural languages like Chinese, Spanish, or English?',\n",
       " '11bcfb2d-e448-41da-ac8a-fd757d264805': 'How do the grammar rules of Python and JavaScript compare to those of natural languages?',\n",
       " 'c5f2fd75-cd1d-4832-b9e3-adf9b36f7653': 'What aspect of LLMs is described as one of their great weaknesses?',\n",
       " '25c0b998-1aef-4ee6-98e7-2624f9bf0582': 'What does it mean when an LLM hallucinates?',\n",
       " 'bbbcee8c-8d48-448f-aa8b-da3c48ac2f34': 'Why would hallucination be a particularly bad problem for code generated by LLMs?',\n",
       " 'ac0d532e-65f0-4f09-9fe2-89c30799864f': 'How might hallucinating a non-existent method affect the usefulness of generated code?',\n",
       " '31aeabe3-e271-4c41-9f3e-00c0c790c82d': 'Despite their weaknesses, what is still astonishing about LLMs?',\n",
       " 'a431e1c1-dceb-4d85-af7c-599244890050': 'What are examples of programming languages mentioned in the context?',\n",
       " '87a2e013-16fa-48fb-966b-6f79801e723a': 'What natural languages are used as a comparison to programming languages in terms of grammar complexity?',\n",
       " 'f9ccb7ba-6266-4a31-a085-007d68d5bcd8': 'What is the relationship between hallucination in LLMs and the reliability of their code output?',\n",
       " '77a73cb0-6fce-4a6e-b3c2-029db68194ca': 'What is the main concern expressed about the utility of LLMs in the context provided?',\n",
       " '03738b93-d6db-4f15-be7d-1f3d1943bb02': 'Why is gullibility considered a challenge for systems that make decisions on behalf of users?',\n",
       " 'a04cf2eb-7545-4d74-b95d-61cc57d9dac1': 'How does the example of Google Search relate to the issue of distinguishing truth from fiction?',\n",
       " '1455d481-9759-4880-a462-4119aa44e0f2': 'What was the specific error made by Google Search mentioned in the context?',\n",
       " '3fa733c5-fc2f-427b-8537-ef3b76079c38': 'Why might a travel agent or digital assistant struggle if it cannot distinguish truth from fiction?',\n",
       " '804e564e-3d67-4228-b0ff-ee0b92f9e5e4': 'What does the term \"LLMs believe anything you tell them\" imply about their reliability?',\n",
       " '22cc4877-5f95-4619-8daa-bfee45734928': 'How does the context describe the impact of LLMs’ gullibility on research tools?',\n",
       " '8e62aedf-17a7-467f-bb73-e2734cd7ebdb': 'What role did a fan fiction wiki play in the Google Search error described?',\n",
       " '6b054d9a-3b95-41ff-9842-a929682afe3c': 'What does the example of the fake movie “Encanto 2” illustrate about information accuracy?',\n",
       " '8b811d2b-0649-4d4e-b3ed-f5a70bc9c2cd': 'Based on the context, what is a potential limitation of AI systems in making meaningful decisions?',\n",
       " '72585aa8-a3f0-47f8-b12d-79b49d9f56f2': 'What did the author write about in October regarding Claude Artifacts?',\n",
       " 'f6d1edac-4598-4b22-a3d4-07221eb2d103': 'How many tools did the author describe building in a seven-day period?',\n",
       " '77340aff-6e61-419a-ac9f-819389dc19c7': 'Which company announced a similar system called GitHub Spark?',\n",
       " 'd4880e6f-b588-4666-af04-355d0b23a09a': 'When did GitHub announce their version of the system?',\n",
       " '9824ff3e-44ec-4c59-b934-28e4afec13fe': 'What feature did Mistral Chat add in November?',\n",
       " 'a521f780-5e54-4c20-9f94-7a170ab9b2d9': 'Who is Steve Krouse and what did he build?',\n",
       " '8baa1fb5-15c3-449f-8e9f-1c4867275f9e': 'What technology did Steve Krouse use to build his version of the system?',\n",
       " '0a1cd133-c35b-4bc1-859c-5daf777fe0c3': 'How fast can the LLM used by Steve Krouse iterate on an application?',\n",
       " '3e893238-4bf0-4c39-bcdf-86ab6cd9eb93': 'What is notable about the changes made by Steve Krouse’s system?',\n",
       " '0594909e-5fdb-4d4a-afb7-aed455ce043c': 'Which teams or companies have built systems similar to Claude Artifacts?',\n",
       " '10d6cc9b-6bd4-4b9a-9093-1b262ed27d25': 'What are some key insights about AI discovered in 2023?',\n",
       " 'e4b34d03-2a37-4a94-b0bb-35a83173c8e8': 'What new information was learned about large language models (LLMs) in 2024?',\n",
       " '5182a29a-28be-44dc-bffb-b4fe6988da1f': 'How many blog posts are tagged with \"ai\" according to the context?',\n",
       " 'b8abf9b9-3483-4f88-b0d3-681827ae3dfc': 'What is the significance of the date December 31, 2023, 11:59 pm in the context?',\n",
       " '73684569-2fa8-4c46-a501-be4817245fb2': 'How many posts are associated with the \"generative-ai\" tag?',\n",
       " '90f27fe6-a466-4f42-964e-7f1d1c5605f3': 'What does the tag \"llms\" represent in the context, and how many posts does it have?',\n",
       " '3376a6be-7964-4807-a93a-060398140715': 'Who is mentioned as the next topic after the current notes?',\n",
       " '74b88d0b-7762-4fef-aa84-0a02c1103d83': 'What is the title of the previous notes before the current context?',\n",
       " 'deb22e99-52ea-4f7f-989b-55969deb9c77': 'What years are listed in the colophon section of the context?',\n",
       " '172e7aed-2b0b-4c75-8837-cb2b74ec229a': 'How might the \"formidable power of escalating streaks\" relate to Tom Scott in the context?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FSTG0bb7w73"
   },
   "source": [
    "We'll use the function to generate training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4547,
     "status": "ok",
     "timestamp": 1746470147425,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "eIZm4CqGVzBx",
    "outputId": "c88e330f-492a-4c0b-f1c9-b36ef88e3a6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 12/12 [00:04<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6187,
     "status": "ok",
     "timestamp": 1746470166752,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "o6qUHg9sV2_y",
    "outputId": "4f918810-7656-457a-a3b7-9afb5dda0591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 12/12 [00:06<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_jYOnAI43zK"
   },
   "source": [
    "### Reformating and Saving Datasets\n",
    "\n",
    "Now, we can save our datasets for later use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746470190098,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "iF6IFFq9VsNu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746470322492,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "PqF9WaueV-V8"
   },
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746470332162,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "0DSQ7WMnWAu6"
   },
   "outputs": [],
   "source": [
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAwklqQCgVi-"
   },
   "source": [
    "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
    "\n",
    "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
    "\n",
    "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
    "\n",
    "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
    "\n",
    ">> NOTE: Skip installing dependencies if you are running this notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6523,
     "status": "ok",
     "timestamp": 1746470347300,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "AXzVHP3v1Cno",
    "outputId": "2694ea48-da79-450e-d516-b2a83f41c8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
      "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU sentence_transformers pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1746470615672,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "G-PGsQB7Xo6V"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ztG07iB8CFO"
   },
   "source": [
    "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
    "\n",
    "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746470622034,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "B-WbpuUWYFJr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJtPPlck8HBE"
   },
   "source": [
    "We're using a toy batch size here to reflect the limited number of examples we have.\n",
    "\n",
    "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746470629274,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "8Lokhy6KYHAv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-6DT8hc8PmT"
   },
   "source": [
    "Let's move our dataset into the expected format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746470632815,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "JJk37zQsYJ4P"
   },
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjFx7KHI8TL0"
   },
   "source": [
    "Now we can create a `torch` `DataLoader`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746470635696,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "tiizmeIqZ_-w"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vA8rzlX8XbT"
   },
   "source": [
    "Next up, we'll prepare our loss function!\n",
    "\n",
    "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
    "\n",
    "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
    "\n",
    "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1746471502150,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "Uga4nnBqlVeh"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJG4fOm66PHI"
   },
   "source": [
    "##### 🏗️ Activity #2:\n",
    "\n",
    "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
    "\n",
    "Why are these losses specifically doing? Please write a short summary of each loss.\n",
    "\n",
    "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKxRuXfH844c"
   },
   "source": [
    "Now we can set-up our evaluator.\n",
    "\n",
    "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746471505992,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "f0hAFwUyaHQG"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfap_ct8-bU"
   },
   "source": [
    "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746471509914,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "svZG0pBHiQr6"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxitWoNX9DwW"
   },
   "source": [
    "It's training time!\n",
    "\n",
    "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1746471513804,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "CyPd18oujq1t",
    "outputId": "054f78aa-9997-46bd-b769-462d67c977c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/2oxjlqzl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d50b6801f10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ129sKWjq1u"
   },
   "source": [
    "> NOTE: You may not see direct improvement during the training cycles - this is absolutely expected. We will verify performance later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904,
     "referenced_widgets": [
      "5d389807e43d4c5c8d7d06e994f48001",
      "2ba00b9fb60349a0b23147132b7bd441",
      "7e9c3524c47348909e9b3b83a4ad59f0",
      "2125c4e42d1d487eb012943308be5b7e",
      "ceb5625f0b4d4d1389600b7ae92d2b7c",
      "f974cf8016bf4b788e2ff1b9cb8c181d",
      "cb7123cc47554a7fb68b6179866c3520",
      "2baa8c9c9261412896571fa5c6bb86df",
      "f00f745738f741f386b8aaf59bbc49f0",
      "8521f7f3c81d45fbaeadb4411deb0cf0",
      "b666683101fd48a3a00beb73fcd518f1"
     ]
    },
    "executionInfo": {
     "elapsed": 513855,
     "status": "ok",
     "timestamp": 1746472050381,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "aDhUHZY-iR09",
    "outputId": "ffc3d90a-4565-44ac-cb75-0a71e89138d4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d389807e43d4c5c8d7d06e994f48001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 08:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy@1</th>\n",
       "      <th>Cosine Accuracy@3</th>\n",
       "      <th>Cosine Accuracy@5</th>\n",
       "      <th>Cosine Accuracy@10</th>\n",
       "      <th>Cosine Precision@1</th>\n",
       "      <th>Cosine Precision@3</th>\n",
       "      <th>Cosine Precision@5</th>\n",
       "      <th>Cosine Precision@10</th>\n",
       "      <th>Cosine Recall@1</th>\n",
       "      <th>Cosine Recall@3</th>\n",
       "      <th>Cosine Recall@5</th>\n",
       "      <th>Cosine Recall@10</th>\n",
       "      <th>Cosine Ndcg@10</th>\n",
       "      <th>Cosine Mrr@10</th>\n",
       "      <th>Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919906</td>\n",
       "      <td>0.892639</td>\n",
       "      <td>0.892639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932193</td>\n",
       "      <td>0.909028</td>\n",
       "      <td>0.909028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935052</td>\n",
       "      <td>0.913056</td>\n",
       "      <td>0.913056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941465</td>\n",
       "      <td>0.921389</td>\n",
       "      <td>0.921389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943449</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>0.924167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943815</td>\n",
       "      <td>0.924583</td>\n",
       "      <td>0.924583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940374</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943237</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>0.923889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934521</td>\n",
       "      <td>0.912431</td>\n",
       "      <td>0.912431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935612</td>\n",
       "      <td>0.913819</td>\n",
       "      <td>0.913819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939251</td>\n",
       "      <td>0.918403</td>\n",
       "      <td>0.918403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935810</td>\n",
       "      <td>0.913819</td>\n",
       "      <td>0.913819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931643</td>\n",
       "      <td>0.908264</td>\n",
       "      <td>0.908264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933507</td>\n",
       "      <td>0.910926</td>\n",
       "      <td>0.910926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934964</td>\n",
       "      <td>0.912731</td>\n",
       "      <td>0.912731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941235</td>\n",
       "      <td>0.921181</td>\n",
       "      <td>0.921181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.099167</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.937515</td>\n",
       "      <td>0.918750</td>\n",
       "      <td>0.919508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.099167</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.940591</td>\n",
       "      <td>0.922917</td>\n",
       "      <td>0.923674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941015</td>\n",
       "      <td>0.920972</td>\n",
       "      <td>0.920972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937462</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.916204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933295</td>\n",
       "      <td>0.910648</td>\n",
       "      <td>0.910648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937462</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.916204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937462</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.916204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933195</td>\n",
       "      <td>0.910556</td>\n",
       "      <td>0.910556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.346500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933195</td>\n",
       "      <td>0.910556</td>\n",
       "      <td>0.910556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic_ft',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "0dd96532d8b742c2a305885d2198e0ac",
      "bdac169781294cd2b92fa78f9e6873da",
      "2699b1047f924dc08b186f25a5882632",
      "5b662dfad4d74fb7836c4eb46c6f56ff",
      "96975c72aefb4cd5bd74f7b919678dbd",
      "dd6571aca75a474ebc776214ff641d27",
      "9f6adadc7c384bbab5a5bc84bbf331f3",
      "d68ce41e3c5d4b3b9ca6cbaf97df201f",
      "1ed0a0cbd79a4530858a548adcb2e021",
      "4896e10bfebb47c5a417df397fe68101",
      "e398aebe12fe419aaea5062d20f87de0",
      "9a4919d5f5d241bba7e803797d4088dd",
      "f33c324a78bd44ab9fde0c4592508d6b",
      "00ec564dcb1c43a593d5f9741dc192ab",
      "0a7c4b72307247eab9911dcdb3cea9ce",
      "4ca3cc5957a74471b4a65953de8d8307",
      "fbd0549062c44fc080c911b34632315c",
      "cba40d5976704346a6c3af9d85bfe9bd",
      "751603d61886401eb5162db1792209cc",
      "ec57cc84ea224e869e800cdd2f1f86d6"
     ]
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1746472087697,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "b3iwclvyRD8L",
    "outputId": "2380e87f-de9e-48f3-fea7-7b1ae2d90836"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd96532d8b742c2a305885d2198e0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746472101589,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "_pn-Y6yjRoHk"
   },
   "outputs": [],
   "source": [
    "hf_username = \"vivnatan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "715dc2cb8fa4417ca8067d50dafd8e73",
      "7f82bda2421a468aa4504e71aed04be7",
      "5c142a72ff164e3084523c025c2f0e3b",
      "9ce8675b1ac14c3e8874f56f95540ca9",
      "4bd51055dd054cccaa9cc347105beb63",
      "102ebd93ae9240608603ed1b7b90df52",
      "27e8214d12d546729dd0ccc4bf39ef92",
      "713e2794bc1b49f08d6661adfd7c71be",
      "f5261202cfa2407bb1af42aa3a530ed9",
      "44040803cf25423d849772b9975de852",
      "ee66adea3c4f48eca23773b5b9200be9"
     ]
    },
    "executionInfo": {
     "elapsed": 75789,
     "status": "ok",
     "timestamp": 1746472184485,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "Nqhf3zWa9AiJ",
    "outputId": "9be63255-08bc-4604-ebc4-41357ad285b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715dc2cb8fa4417ca8067d50dafd8e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://huggingface.co/vivnatan/legal-ft-20c85cc6-30d1-49ca-97e6-cce1045a4b4a/commit/4f0f08e1bcdde7d300971b6019968fdd206c2a59'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "model.push_to_hub(f\"{hf_username}/legal-ft-{uuid.uuid4()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bo0zW5k9Poq"
   },
   "source": [
    "## Task 5: Evaluating our Retriever\n",
    "\n",
    "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
    "\n",
    "We'll start with some basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1746472605838,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "Vq-2oqU0wHFr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jD0qrIh9X8f"
   },
   "source": [
    "Now we'll define a function that will help us evaluate our retrieval process.\n",
    "\n",
    "> NOTE: We're assuming 1 correct document in a \"hit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746472715350,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "0713_3cowX4q"
   },
   "outputs": [],
   "source": [
    "def evaluate_openai(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "  corpus = dataset['corpus']\n",
    "  questions = dataset['questions']\n",
    "  relevant_docs = dataset['relevant_contexts']\n",
    "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
    "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "  eval_results = []\n",
    "  for id, question in tqdm.tqdm(questions.items()):\n",
    "    retrieved_nodes = retriever.invoke(question)\n",
    "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
    "    expected_id = relevant_docs[id][0]\n",
    "    is_hit = expected_id in retrieved_ids\n",
    "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
    "\n",
    "  return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOr49m4O9lxY"
   },
   "source": [
    "All that's left to do is evaluate, we'll evaluate our model against:\n",
    "\n",
    "1. OpenAI's closed source `text-embedding-3-small`\n",
    "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
    "\n",
    "Let's see how it stacks up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijaeYpf593IW"
   },
   "source": [
    "### `text-embedding-3-small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42345,
     "status": "ok",
     "timestamp": 1746472765712,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "kyY3PztaxnU3",
    "outputId": "dc500144-10c7-4298-b044-b7b548d74f79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:40<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "te3_results = evaluate_openai(test_dataset, te3_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746472765716,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "kkyW90TCxx_i"
   },
   "outputs": [],
   "source": [
    "te3_results_df = pd.DataFrame(te3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746472765721,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "MscVRdNCylJ-",
    "outputId": "ea38e8f4-0530-4925-9332-e6c78faeff83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9916666666666667)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
    "te3_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ra-mh0L96dQ"
   },
   "source": [
    "### `Snowflake/snowflake-arctic-embed-l` (base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4983,
     "status": "ok",
     "timestamp": 1746472770705,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "OEskxwvFypHe",
    "outputId": "8f3f1213-01b6-4674-a3e4-282c3d86fde8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:02<00:00, 46.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
    "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1746472770747,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "KlKgiXTWzMTg"
   },
   "outputs": [],
   "source": [
    "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746472770750,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "zV5vJWrJzOhc",
    "outputId": "374e5dbd-3b50-4e95-fc2c-676dcca86b00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8333333333333334)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
    "arctic_embed_m_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcR3-0s19_lu"
   },
   "source": [
    "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3544,
     "status": "ok",
     "timestamp": 1746472815194,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "Ilse1LduzP1i",
    "outputId": "633a67c3-3a33-4919-fcb5-25e8c9fa4731"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 120/120 [00:02<00:00, 46.22it/s]\n"
     ]
    }
   ],
   "source": [
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
    "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1746472815235,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "xxhZPqkNzZlh"
   },
   "outputs": [],
   "source": [
    "finetune_results_df = pd.DataFrame(finetune_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746472815238,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "4thAK2BXzaj6",
    "outputId": "2cecdc3f-13cd-4a64-84bc-8d9940692191"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9916666666666667)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
    "finetune_hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iegFM209mBk3"
   },
   "source": [
    "## Task 1: Vibe Checking the RAG Pipeline\n",
    "\n",
    "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzg0AA5krgR4"
   },
   "source": [
    "### Creating New Chunks\n",
    "\n",
    "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1746472882600,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "KwQ2_LqNr0Tw"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "training_documents = text_splitter.split_documents(text_loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIdxahHXpP-c"
   },
   "source": [
    "### Base Chain\n",
    "\n",
    "We'll start by constructing our base chain, which will use the untrained retrieval model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOsxIXpNpWC2"
   },
   "source": [
    "#### R - Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1746472888003,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "azIGIKYfmNCT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
    "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-1nVZ0KpX5N"
   },
   "source": [
    "#### A - Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746472891156,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "G10Fr-aKojeA"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euq6RQEopZvD"
   },
   "source": [
    "#### G - Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1746472896328,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "5-mfbbrypMHG"
   },
   "outputs": [],
   "source": [
    "rag_llm =  ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ2p4mnUpbYY"
   },
   "source": [
    "#### RAG - LCEL RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746472899421,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "ssuR-LaboyGq"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "base_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1746472902222,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "emm6WbB9pfKt",
    "outputId": "e7203940-5307-46dc-e8cf-9725770ce385"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Based on the provided context, an \"agent\" in the context of AI refers to systems that can act on your behalf, such as travel agents or digital assistants. However, the term is highly vague and lacks a clear, universally accepted definition. Different people interpret \"agents\" differently—some see them as systems that autonomously perform tasks, while others think of them as tools that access and utilize various resources or tools to solve problems. Despite ongoing discussions and prototypes, true AI agents that reliably operate in production are still elusive, partly due to issues like gullibility and the difficulty of distinguishing truth from fiction.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1746472907991,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "mUOrd0OBprAq",
    "outputId": "27c898aa-3d38-48d0-ce4f-43aacbdecdc5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Several organizations have produced models that are better than GPT-3, including Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, and Baidu.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1746472914201,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "OnfuFl59py7I",
    "outputId": "5b8043e1-eba0-408c-dbd1-ac62377e2911"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The provided context does not specify a particular time of year that is considered the \"laziest\" for AI.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is the laziest time of the year for AI?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1746472932833,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "-NmqwHBDqTZ8",
    "outputId": "97e72663-c0d5-49b3-f28e-6748f1854e10"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The provided context does not specify the name \"Simon\" or details about the largest model he has run on his phone. Therefore, I do not know the answer.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqNS0UJAp3lC"
   },
   "source": [
    "### Fine-tuned Embedding Model\n",
    "\n",
    "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 1508,
     "status": "ok",
     "timestamp": 1746472938882,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "ihO7tP6mqATy"
   },
   "outputs": [],
   "source": [
    "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
    "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1746472942394,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "1_cIFvWzqKGY"
   },
   "outputs": [],
   "source": [
    "finetune_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "executionInfo": {
     "elapsed": 1694,
     "status": "ok",
     "timestamp": 1746472945739,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "OJmRHJF2qNgj",
    "outputId": "afd850eb-fadc-474a-a24b-9ef288ca4596"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'An agent, in the context of AI and large language models, is a term that lacks a single, clear definition and is used in various ways. Some people consider AI agents to be systems that act on your behalf, similar to a travel agent, while others think of them as LLMs given access to tools that they can use iteratively to solve problems. Overall, the term remains vague and is often associated with systems that are expected to perform autonomous actions or decision-making, but its precise meaning varies among different users and contexts.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1746472956338,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "EnK-c2ugqPPh",
    "outputId": "9749f1ea-df31-4dd4-a9fd-bbc899d8e920"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'According to the provided context, organizations such as Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII (Falcon), Microsoft Research, xAI, Replit, Baidu, and others have produced models that are better than GPT-3.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1746472971070,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "83hssg1AWozc",
    "outputId": "87cc9766-6232-4ac6-98d8-515110fb4554"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The provided context suggests that ChatGPT may become less useful or \"lazy\" in December, possibly because its system prompt includes the current date and the model\\'s training data indicates that people tend to provide less useful answers around the holidays. Therefore, the laziest time of the year for AI, according to the context, is December.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is the laziest time of the year for AI?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1746472999429,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "rsHmGeFbqRET",
    "outputId": "a10bba50-1e50-4a40-c5b4-2e88136dbb1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The provided context mentions that Simon runs Mistral 7B on his iPhone. There is no information about him running any larger models on his phone. Therefore, the largest model Simon has run on his phone is Mistral 7B.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDgD8seY_I3W"
   },
   "source": [
    "#### ❓Question #2:\n",
    "\n",
    "Which LCEL RAG Chain do you think answered the questions better, and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCbq1sZArIx4"
   },
   "source": [
    "## Task 2: RAGAS Evaluation\n",
    "\n",
    "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
    "\n",
    "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq880DtHk9pX"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3414,
     "status": "ok",
     "timestamp": 1746473293711,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "79OBFQFm619c",
    "outputId": "890450fe-e7d3-437e-be03-b6a169f88acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.7/175.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU ragas==0.2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17857,
     "status": "ok",
     "timestamp": 1746473313372,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "s6Hk3RGG630M",
    "outputId": "6e10e8da-8309-4ea2-964d-ac67eaf4ee37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.9/326.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
      "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1746474018652,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "9d3feLqs68oA"
   },
   "outputs": [],
   "source": [
    "# 1) import RAGAS evaluate API & metrics\n",
    "from ragas.metrics import ContextPrecision, ContextRecall, ResponseRelevancy, Faithfulness\n",
    "from ragas import evaluate, RunConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "executionInfo": {
     "elapsed": 4337,
     "status": "ok",
     "timestamp": 1746476315836,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "4KxTmO4FGVts"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.html\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1746476331351,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "5L0u9D5FGeNK"
   },
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "9525569d20bb46cb8b6961855da82c84",
      "e22eaaa41327415eacca172d69387461",
      "1c6e8671009440279edf5bd31629cd8f",
      "a22bf68e37f344d188e8995b549a7e09",
      "29b0937b71d04e618de3b3dbda85d481",
      "88832729e9ff49d9a117ea3dd881a980",
      "a99d854c0d764be6a96b14bf29486487",
      "02181e8a21474ca596d849b54986a7ef",
      "da61170c2b6f44aeafd46102f5b8ec0b",
      "a46831a634f74b3b93f302f26863968f",
      "25794408aceb48a59bc098b4ac315d8d",
      "2d951491ce1a4abf9f1869e25ed0d674",
      "b2daa4c01c544b9aa5f19cea631bfe96",
      "7ef9d0013dec4a999340437e4444a897",
      "e2a715d48a504961bbc23e499cc3db3e",
      "40753e2e6488468da2739fc9a6ba33fb",
      "e30dd42bcbc8466cb51538a2a7a116f1",
      "fbf7291aa3dd44c183efe27fcef3cb8e",
      "25f759f7bd9346d9b401d2cae332ff9f",
      "9678141cfcfd46fe9a5fb7248514a31f",
      "12934683c7784ef38f727d4f5a609eaf",
      "3eaf71c79d1e446181f2ac847da63f03",
      "0bf2476b449244d097a4127508f3189a",
      "ac3613f393a04752871bd659ef00f6a1",
      "5c053a2fee294738a23b58d1137a0018",
      "9d846c1932b843f08d0898b4cd834913",
      "8f9e1d6114474c80b59f02295e78c3ed",
      "0be68545bb594f5bb58ebf2bffcb6d98",
      "abe92a85c01c4a71951777998c7c9048",
      "403a843e33854016a1cb6694d953f621",
      "b136301f39bd4a9795e83d5a91153a5c",
      "a67dddb1a34f4608866a7a19e57348ef",
      "ac97843887e1457d933a032d3e39d1e2",
      "f9e1354ee63f4217ad048c3cea56c4f8",
      "b9ee3d7e912d43f0b65b20a28bcf58e9",
      "2f4a9a9415584f138520dac005adca0d",
      "fa288e3b53ed42128d7780fd79e98e33",
      "e97fdbda245b4ad5ae6ef333b2c36c4a",
      "a0d4fb18c04d4dfcbc581bdd67862910",
      "3237af9d961f43b2a5f2fcba7ad0ae81",
      "5c16d797c27041d097445930dd0b451d",
      "966176d6f33b4a3e9338839c7a1a9d16",
      "1b0fda2e06224fb7bc554165d80fc824",
      "4cfaf6da2ee74932867926a063fa4c7e",
      "05032e792ed74dd5b702c63c2fe8b49c",
      "c84e0530bf41426eb02eb5d541d4273c",
      "139f012430be4970bfa31db27463f021",
      "d28e8b2e405e42a4ba469a593d3f8721",
      "0fa5daccdae149ffb395365641d4e323",
      "d1b9428dd4a2415192dcde126603fcfb",
      "190d32b2b205445cb5aac5183efa6273",
      "5c4ee659753b4068b1219a133817d65a",
      "b5277ec53f81438bab95bd3a3333fe97",
      "2e504702b52f4c12a433c7ec493f32ec",
      "a27440e2f1bf413e900e8aebbd48ca06",
      "3721b8de0ef54c07872ffc0ee080796e",
      "946a5bfe29644c1b84af3911eaabe8b0",
      "7876e4d3d6334600b6f6624067a8a14c",
      "3c0ada0cae784b9fb8b635ed107c50b2",
      "ff616f98f13f4484bdf726eb4ad30047",
      "dcf44192f3ba4ffab95901fa28d72623",
      "ec745a0b3e144c43808855e76a0ab6c0",
      "c8ea3ebe15274427b3f5315c475a74dd",
      "85b51f2453014edea901f3180edcf15c",
      "6253dac241da4e81b00bcbf669189c91",
      "0fe6841a644a4055abf7c38e38f6ccec",
      "77d7ca48c738438e850d6cd1c5ed9fa5",
      "60a3387e2f7e48b08d359ad27f9b3697",
      "d9ff8ef8512f448d8e98ee3d7d26856d",
      "823f22fdc1d2457a8cb7fe38871b5533",
      "b59de75c15814f2db4e0aa32041f47dc",
      "f223590aeee34a44a10d495a08c7d4af",
      "71dd6e28a80f437aa3a6f0a5a018a74e",
      "7734f6c14408433ab6feda59e9002507",
      "0aa77f84068446cba7649b43ca86155f",
      "cab5303c0abc48d498b85d667d41dc50",
      "7b8d37ba0cfe46e8945c62ba6156c278",
      "6629a1e9dcf04107b6be419d6fbe79c7",
      "28e912eeb7b94f5ca4b419c119d896fc",
      "890255e0857b4cddbd6449c6eeab1473",
      "ace32d66ea6e4c799b9416a36daa0bee",
      "559593b700864da9b35569b20eb66ca3",
      "9898450628354757ba244a2c51d6fb00",
      "1b30158a93be48109d40e5d1f2d0976c",
      "48113a09667146f095560c45025ddc46",
      "8166caf2897d48caacbc12d66f235ca5",
      "580c405ec96047c8acbee5f8676b9af4",
      "43e8c5c0dd7f412c82a0e0442d565d53",
      "a94e4fe679be471b843046fea1a40ad7",
      "c982ea751ec14f28a909ec418e011d5c",
      "a0964ee9826a4796986f0a0b722be13b",
      "529e5f8942324f8aa0173c3b27c64675",
      "531715b1b0514cd3837801ff13e6276e",
      "9927e9df270e43b6a8e47b18d929da54",
      "a74bfec978d14a39b02255441afe8593",
      "ed87d56303234f4b8f1412e7cd34af57",
      "ecb82c23461a4344a8a7a091c73a3999",
      "adc062fba3d64e36b922dbae4dac82df",
      "7eda79260bc8476595462bb1620442e8"
     ]
    },
    "executionInfo": {
     "elapsed": 207135,
     "status": "ok",
     "timestamp": 1746476551835,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "pEpOkSiUGf3Z",
    "outputId": "9befaa4b-dc41-47ed-ad55-98500a01e8d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9525569d20bb46cb8b6961855da82c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d951491ce1a4abf9f1869e25ed0d674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf2476b449244d097a4127508f3189a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e1354ee63f4217ad048c3cea56c4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05032e792ed74dd5b702c63c2fe8b49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3721b8de0ef54c07872ffc0ee080796e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d7ca48c738438e850d6cd1c5ed9fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6629a1e9dcf04107b6be419d6fbe79c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94e4fe679be471b843046fea1a40ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1746476554360,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "jNpcVHOYGrgN",
    "outputId": "49661fad-5913-4745-db47-3bb616eadf68"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Why people think ChatGPT is easy but it not, and what problems come from people using ChatGPT wrong, like with screenshots, and what did the blogger say about this in his 2024 posts?\",\n          \"What is Claude 3.5 Sonnet, and how did its introduction of Claude Artifacts impact the development and accessibility of prompt-driven app generation compared to other leading models?\",\n          \"Wut did Meta do with Llama?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"People think ChatGPT is easy because it look like just typing to a chatbot, but really you need lots of understanding and experience to use it right and not fall in traps. The blogger say in 2024 that most users get thrown in deep end, and many have wrong ideas about what ChatGPT can do. Some people try to win arguments with screenshots from ChatGPT, but that is silly because the models are unreliable and can say anything if you prompt them right. The blogger wrote in his 2024 posts about these problems, saying there is a big knowledge gap and people need better education to use ChatGPT and other LLMs responsibly.\",\n          \"Claude 3.5 Sonnet is a leading large language model that was ranked 7th in the Vibe benchmarks (Chatbot Arena), just behind Gemini 2.0 and OpenAI 4o/o1 models, making it the highest ranking openly licensed model. Its introduction of Claude Artifacts was a groundbreaking feature that allowed users to generate and use interactive applications directly within the Claude interface. This innovation significantly advanced prompt-driven app generation, making it easy for users to build full interactive applications with a single prompt. Following Claude Artifacts, other teams and companies, such as GitHub (with Spark) and Mistral Chat (with Canvas), introduced similar features, demonstrating that prompt-driven custom interfaces had become a commodity among leading models. The accessibility and ease of use provided by Claude 3.5 Sonnet and its Artifacts feature contributed to a broader adoption and development of prompt-driven app generation across the AI landscape.\",\n          \"In February, Meta released Llama, and later in July, Meta released Llama 2\\u2014an improved version which included permission for commercial use.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"synthesizer_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"single_hop_specifc_query_synthesizer\",\n          \"multi_hop_abstract_query_synthesizer\",\n          \"multi_hop_specific_query_synthesizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ec2a0099-58c4-4d92-bdeb-21063683d94a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut did Meta do with Llama?</td>\n",
       "      <td>[We don’t yet know how to build GPT-4 Vibes Ba...</td>\n",
       "      <td>In February, Meta released Llama, and later in...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Given the increasing capabilities of large lan...</td>\n",
       "      <td>[I’m surprised that no-one has beaten the now ...</td>\n",
       "      <td>The grammar rules of programming languages lik...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whatt role did the 1950s play in the devellopm...</td>\n",
       "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
       "      <td>The academic field of Artificial Intelligence ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As an AI enthusiast and technology blogger int...</td>\n",
       "      <td>[Microsoft over this issue. The 69 page PDF is...</td>\n",
       "      <td>According to the provided context, Stanford Al...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How did the emergence of prompt-driven app gen...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\ndid. These abilities are just a fe...</td>\n",
       "      <td>The emergence of prompt-driven app generation ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Based on the blog posts and analytics data, ho...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nof things, here’s every long-form ...</td>\n",
       "      <td>ChatGPT has been a recurring topic in blog pos...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is Claude 3.5 Sonnet, and how did its int...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nup there with Claude 3.5 Sonnet. V...</td>\n",
       "      <td>Claude 3.5 Sonnet is a leading large language ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Why people think ChatGPT is easy but it not, a...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nby DeepSeek-R1. Meta’s Llama 3.3 7...</td>\n",
       "      <td>People think ChatGPT is easy because it look l...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How has the rapid evolution of LLMs (Large Lan...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>Over the past year, the landscape of LLMs (Lar...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec2a0099-58c4-4d92-bdeb-21063683d94a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ec2a0099-58c4-4d92-bdeb-21063683d94a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ec2a0099-58c4-4d92-bdeb-21063683d94a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-14fecf4d-1e72-44f8-a67d-209bd6e288ca\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14fecf4d-1e72-44f8-a67d-209bd6e288ca')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-14fecf4d-1e72-44f8-a67d-209bd6e288ca button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0                         Wut did Meta do with Llama?   \n",
       "1   Given the increasing capabilities of large lan...   \n",
       "2   Whatt role did the 1950s play in the devellopm...   \n",
       "3   As an AI enthusiast and technology blogger int...   \n",
       "4   How has the rise of fine-tuning and customizat...   \n",
       "5   How has the rise of fine-tuning and customizat...   \n",
       "6   How has the rise of fine-tuning and customizat...   \n",
       "7   How did the emergence of prompt-driven app gen...   \n",
       "8   Based on the blog posts and analytics data, ho...   \n",
       "9   What is Claude 3.5 Sonnet, and how did its int...   \n",
       "10  Why people think ChatGPT is easy but it not, a...   \n",
       "11  How has the rapid evolution of LLMs (Large Lan...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [We don’t yet know how to build GPT-4 Vibes Ba...   \n",
       "1   [I’m surprised that no-one has beaten the now ...   \n",
       "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
       "3   [Microsoft over this issue. The 69 page PDF is...   \n",
       "4   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "5   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "6   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "7   [<1-hop>\\n\\ndid. These abilities are just a fe...   \n",
       "8   [<1-hop>\\n\\nof things, here’s every long-form ...   \n",
       "9   [<1-hop>\\n\\nup there with Claude 3.5 Sonnet. V...   \n",
       "10  [<1-hop>\\n\\nby DeepSeek-R1. Meta’s Llama 3.3 7...   \n",
       "11  [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   In February, Meta released Llama, and later in...   \n",
       "1   The grammar rules of programming languages lik...   \n",
       "2   The academic field of Artificial Intelligence ...   \n",
       "3   According to the provided context, Stanford Al...   \n",
       "4   The rise of fine-tuning and customization by h...   \n",
       "5   The rise of fine-tuning and customization by h...   \n",
       "6   The rise of fine-tuning and customization by h...   \n",
       "7   The emergence of prompt-driven app generation ...   \n",
       "8   ChatGPT has been a recurring topic in blog pos...   \n",
       "9   Claude 3.5 Sonnet is a leading large language ...   \n",
       "10  People think ChatGPT is easy because it look l...   \n",
       "11  Over the past year, the landscape of LLMs (Lar...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1746477229754,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "Zz_KTW6mHZft"
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.html\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746477247442,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "vuVZeofmJ9rs",
    "outputId": "cc64d7c5-b836-48c7-d128-ad9957114f2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3187,
     "status": "ok",
     "timestamp": 1746480267431,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "oPdFkyMAJ_Lg",
    "outputId": "c9d635f5-a765-4938-ef17-153dd5f9dc20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "baseline_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
    "finetuned_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
    "# arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1746480318381,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "phUJnHhrKOLI"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"ai_across_years\",\n",
    "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_across_years\",\n",
    "    embedding=baseline_embeddings\n",
    ")\n",
    "\n",
    "vector_store_ft = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_across_years\",\n",
    "    embedding=finetuned_embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "executionInfo": {
     "elapsed": 3208,
     "status": "ok",
     "timestamp": 1746480354347,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "N5gxwV9PKmNt"
   },
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=split_documents)\n",
    "_ = vector_store_ft.add_documents(documents=split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746480413508,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "tvJHI0G3Kr8o"
   },
   "outputs": [],
   "source": [
    "baseline_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "finetune_retriever = vector_store_ft.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746480431342,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "-2BoXEbgKyHZ"
   },
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "  retrieved_docs = baseline_retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}\n",
    "\n",
    "def retrieve_ft(state):\n",
    "  retrieved_docs = finetune_retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746480479662,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "HmGRBTXMQSqL"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1746480484643,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "-MT43UN-9rQK"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746480635824,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "KhgQw41GQbNv"
   },
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "  messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "  response = llm.invoke(messages)\n",
    "  return {\"response\" : response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746480637344,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "HDh2PY4rQe1r"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746480655408,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "3QxgByqQQiI2"
   },
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "graph_builder_ft = StateGraph(State).add_sequence([retrieve_ft, generate])\n",
    "graph_builder_ft.add_edge(START, \"retrieve_ft\")\n",
    "graph_ft = graph_builder_ft.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "executionInfo": {
     "elapsed": 2564,
     "status": "ok",
     "timestamp": 1746480666190,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "wcPyyjgjQkb9"
   },
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\" : \"How are LLM agents useful?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746480667184,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "znSdc0x9Q5_3",
    "outputId": "6e653fd4-0c39-4660-d6eb-eeade1a48e51"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'LLM agents are useful because they represent AI systems that can potentially act on your behalf, automating tasks or making decisions. However, their usefulness is currently limited by their inherent gullibility—they believe anything you tell them and cannot reliably distinguish truth from fiction. Despite this challenge, there are genuine valuable applications for LLMs, particularly in areas like writing code, where their capabilities have proven astonishing and effective.\\n\\nThe key to benefiting from LLM agents lies in developing the skill to work with technology that is both powerful and inherently unreliable. This requires careful design, guidance, and education to avoid intuitive traps and to apply these tools effectively. While fully reliable LLM agents may require breakthroughs like AGI, they already hold promise as powerful, if complex, assistive tools for expert or power users.'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "executionInfo": {
     "elapsed": 2798,
     "status": "ok",
     "timestamp": 1746480711553,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "WXRma7ZqXCQx"
   },
   "outputs": [],
   "source": [
    "response_ft = graph_ft.invoke({\"question\" : \"How are LLM agents useful?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746480725773,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "JQj37GJUXMzN",
    "outputId": "90b589c6-d72c-40f2-d13e-fde6513a45f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'LLM agents are useful in that they represent AI systems that can potentially act on your behalf by using tools and running processes in loops to solve problems. They hold promise as entities that could perform tasks autonomously, such as acting like a travel agent or a digital assistant. Additionally, LLMs are notably effective in generating and writing code, which is considered one of their best applications so far. However, despite the excitement around AI agents, practical and reliable implementations remain limited, partly due to challenges like gullibility—LLMs tend to believe any input they receive, which complicates trustworthiness and decision-making in autonomous systems. Thus, while LLM agents have potential usefulness, especially in coding and task automation, their full capabilities and reliability in acting independently are still largely unrealized.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_ft[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "executionInfo": {
     "elapsed": 85236,
     "status": "ok",
     "timestamp": 1746479291544,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "ilXKwGrSRYAa"
   },
   "outputs": [],
   "source": [
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1746479308093,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "ijHgvnOgRzuZ",
    "outputId": "8df68141-e738-4cfa-abcf-2e08d76eef33"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Why people think ChatGPT is easy but it not, and what problems come from people using ChatGPT wrong, like with screenshots, and what did the blogger say about this in his 2024 posts?\",\n          \"What is Claude 3.5 Sonnet, and how did its introduction of Claude Artifacts impact the development and accessibility of prompt-driven app generation compared to other leading models?\",\n          \"Wut did Meta do with Llama?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"People often think ChatGPT is easy to use because it provides human-like conversational responses and seems straightforward on the surface. However, according to the blogger's 2024 posts, this perception is misleading. The tool is inherently unreliable, and mastering it requires a non-obvious skill: learning to work effectively with technology that is both powerful and flawed. Many users have inaccurate mental models of how ChatGPT works and its capabilities, which leads to misuse.\\n\\nOne major problem arising from people using ChatGPT incorrectly\\u2014such as sharing screenshots to \\\"win arguments\\\"\\u2014is that these screenshots are inherently unreliable evidence. ChatGPT can be prompted to say virtually anything, so using its outputs as definitive proof is ludicrous and misleading. This misuse contributes to misunderstandings and misinformation.\\n\\nAdditionally, the blogger highlights that there is a significant knowledge gap: while many know of ChatGPT, far fewer understand the nuances or the existence of alternatives like Claude. The industry hasn't done enough to educate users properly; instead, educational efforts are often outsourced to \\\"AI grifters with bombastic Twitter threads,\\\" which is insufficient.\\n\\nThe blogger also notes that the user experience with LLMs is like throwing new computer users into a Linux terminal without guidance\\u2014most users are left to figure things out on their own, which only worsened in 2024 despite advancements.\\n\\nIn summary, people think ChatGPT is easy because of its conversational interface, but the real skill lies in understanding its limitations and appropriate use. Misuse, like relying on screenshots as evidence, leads to problems. The blogger calls for much better education and criticism of LLMs to bridge the large knowledge gap and help users use these tools effectively and responsibly.\",\n          \"Claude 3.5 Sonnet is a version of Anthropic's Claude large language model that introduced a significant new feature called **Claude Artifacts**. This feature enables Claude to generate on-demand interactive applications directly within its interface. Users can prompt the model to create small functional tools or apps that can be used immediately without leaving the Claude environment.\\n\\nThe introduction of Claude Artifacts significantly impacted the development and accessibility of prompt-driven app generation by making it much easier and more immediate to create and use such tools. Prior to this, building prompt-driven apps often required additional steps outside the LLM interface, such as coding or integrating external systems. With Claude Artifacts, the process was streamlined into a single, interactive workflow.\\n\\nThis innovation accelerated the interest and development of similar systems by other companies. For example, GitHub announced **GitHub Spark** in October, and Mistral Chat introduced a comparable feature called **Canvas** in November. The Claude Artifacts feature catalyzed a broader movement toward embedding interactive app generation capabilities directly into LLM interfaces, helping democratize access to customizable AI-powered tools for end users and developers alike.\\n\\nIn summary, Claude 3.5 Sonnet and its Claude Artifacts feature advanced prompt-driven app generation by embedding interactive app creation natively inside the model's interface, thereby improving accessibility, convenience, and fostering rapid innovation compared to earlier and competing models.\",\n          \"Meta released Llama 2, an improved version of their large language model, which importantly included permission for commercial use. This release helped accelerate innovation in large language models by making a powerful model available for local and commercial applications.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"People think ChatGPT is easy because it look like just typing to a chatbot, but really you need lots of understanding and experience to use it right and not fall in traps. The blogger say in 2024 that most users get thrown in deep end, and many have wrong ideas about what ChatGPT can do. Some people try to win arguments with screenshots from ChatGPT, but that is silly because the models are unreliable and can say anything if you prompt them right. The blogger wrote in his 2024 posts about these problems, saying there is a big knowledge gap and people need better education to use ChatGPT and other LLMs responsibly.\",\n          \"Claude 3.5 Sonnet is a leading large language model that was ranked 7th in the Vibe benchmarks (Chatbot Arena), just behind Gemini 2.0 and OpenAI 4o/o1 models, making it the highest ranking openly licensed model. Its introduction of Claude Artifacts was a groundbreaking feature that allowed users to generate and use interactive applications directly within the Claude interface. This innovation significantly advanced prompt-driven app generation, making it easy for users to build full interactive applications with a single prompt. Following Claude Artifacts, other teams and companies, such as GitHub (with Spark) and Mistral Chat (with Canvas), introduced similar features, demonstrating that prompt-driven custom interfaces had become a commodity among leading models. The accessibility and ease of use provided by Claude 3.5 Sonnet and its Artifacts feature contributed to a broader adoption and development of prompt-driven app generation across the AI landscape.\",\n          \"In February, Meta released Llama, and later in July, Meta released Llama 2\\u2014an improved version which included permission for commercial use.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"synthesizer_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"single_hop_specifc_query_synthesizer\",\n          \"multi_hop_abstract_query_synthesizer\",\n          \"multi_hop_specific_query_synthesizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ed37d6a1-45d0-40b1-aa65-72f980dc96b4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut did Meta do with Llama?</td>\n",
       "      <td>[Article Visitors Pageviews Bing: “I will not ...</td>\n",
       "      <td>[We don’t yet know how to build GPT-4 Vibes Ba...</td>\n",
       "      <td>Meta released Llama 2, an improved version of ...</td>\n",
       "      <td>In February, Meta released Llama, and later in...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Given the increasing capabilities of large lan...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[I’m surprised that no-one has beaten the now ...</td>\n",
       "      <td>The context does not explicitly compare the co...</td>\n",
       "      <td>The grammar rules of programming languages lik...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whatt role did the 1950s play in the devellopm...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
       "      <td>The provided context does not specifically dis...</td>\n",
       "      <td>The academic field of Artificial Intelligence ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As an AI enthusiast and technology blogger int...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[Microsoft over this issue. The 69 page PDF is...</td>\n",
       "      <td>According to the provided context, Stanford Al...</td>\n",
       "      <td>According to the provided context, Stanford Al...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[Article Visitors Pageviews Bing: “I will not ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How did the emergence of prompt-driven app gen...</td>\n",
       "      <td>[These abilities are just a few weeks old at t...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\ndid. These abilities are just a fe...</td>\n",
       "      <td>The emergence of prompt-driven app generation ...</td>\n",
       "      <td>The emergence of prompt-driven app generation ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Based on the blog posts and analytics data, ho...</td>\n",
       "      <td>[Law is not ethics. Is it OK to train models o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nof things, here’s every long-form ...</td>\n",
       "      <td>Based on the provided blog posts and analytics...</td>\n",
       "      <td>ChatGPT has been a recurring topic in blog pos...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is Claude 3.5 Sonnet, and how did its int...</td>\n",
       "      <td>[Article Visitors Pageviews Bing: “I will not ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nup there with Claude 3.5 Sonnet. V...</td>\n",
       "      <td>Claude 3.5 Sonnet is a version of Anthropic's ...</td>\n",
       "      <td>Claude 3.5 Sonnet is a leading large language ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Why people think ChatGPT is easy but it not, a...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nby DeepSeek-R1. Meta’s Llama 3.3 7...</td>\n",
       "      <td>People often think ChatGPT is easy to use beca...</td>\n",
       "      <td>People think ChatGPT is easy because it look l...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How has the rapid evolution of LLMs (Large Lan...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>The rapid evolution of LLMs over the past year...</td>\n",
       "      <td>Over the past year, the landscape of LLMs (Lar...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed37d6a1-45d0-40b1-aa65-72f980dc96b4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ed37d6a1-45d0-40b1-aa65-72f980dc96b4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ed37d6a1-45d0-40b1-aa65-72f980dc96b4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-101bcf2a-b132-4565-85d4-33bbc67971d9\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-101bcf2a-b132-4565-85d4-33bbc67971d9')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-101bcf2a-b132-4565-85d4-33bbc67971d9 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0                         Wut did Meta do with Llama?   \n",
       "1   Given the increasing capabilities of large lan...   \n",
       "2   Whatt role did the 1950s play in the devellopm...   \n",
       "3   As an AI enthusiast and technology blogger int...   \n",
       "4   How has the rise of fine-tuning and customizat...   \n",
       "5   How has the rise of fine-tuning and customizat...   \n",
       "6   How has the rise of fine-tuning and customizat...   \n",
       "7   How did the emergence of prompt-driven app gen...   \n",
       "8   Based on the blog posts and analytics data, ho...   \n",
       "9   What is Claude 3.5 Sonnet, and how did its int...   \n",
       "10  Why people think ChatGPT is easy but it not, a...   \n",
       "11  How has the rapid evolution of LLMs (Large Lan...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [Article Visitors Pageviews Bing: “I will not ...   \n",
       "1   [Meanwhile, it’s increasingly common for end u...   \n",
       "2   [Meanwhile, it’s increasingly common for end u...   \n",
       "3   [Meanwhile, it’s increasingly common for end u...   \n",
       "4   [Meanwhile, it’s increasingly common for end u...   \n",
       "5   [Meanwhile, it’s increasingly common for end u...   \n",
       "6   [Article Visitors Pageviews Bing: “I will not ...   \n",
       "7   [These abilities are just a few weeks old at t...   \n",
       "8   [Law is not ethics. Is it OK to train models o...   \n",
       "9   [Article Visitors Pageviews Bing: “I will not ...   \n",
       "10  [Meanwhile, it’s increasingly common for end u...   \n",
       "11  [Meanwhile, it’s increasingly common for end u...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [We don’t yet know how to build GPT-4 Vibes Ba...   \n",
       "1   [I’m surprised that no-one has beaten the now ...   \n",
       "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
       "3   [Microsoft over this issue. The 69 page PDF is...   \n",
       "4   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "5   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "6   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "7   [<1-hop>\\n\\ndid. These abilities are just a fe...   \n",
       "8   [<1-hop>\\n\\nof things, here’s every long-form ...   \n",
       "9   [<1-hop>\\n\\nup there with Claude 3.5 Sonnet. V...   \n",
       "10  [<1-hop>\\n\\nby DeepSeek-R1. Meta’s Llama 3.3 7...   \n",
       "11  [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "\n",
       "                                             response  \\\n",
       "0   Meta released Llama 2, an improved version of ...   \n",
       "1   The context does not explicitly compare the co...   \n",
       "2   The provided context does not specifically dis...   \n",
       "3   According to the provided context, Stanford Al...   \n",
       "4   The rise of fine-tuning and customization by h...   \n",
       "5   The rise of fine-tuning and customization by h...   \n",
       "6   The rise of fine-tuning and customization by h...   \n",
       "7   The emergence of prompt-driven app generation ...   \n",
       "8   Based on the provided blog posts and analytics...   \n",
       "9   Claude 3.5 Sonnet is a version of Anthropic's ...   \n",
       "10  People often think ChatGPT is easy to use beca...   \n",
       "11  The rapid evolution of LLMs over the past year...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   In February, Meta released Llama, and later in...   \n",
       "1   The grammar rules of programming languages lik...   \n",
       "2   The academic field of Artificial Intelligence ...   \n",
       "3   According to the provided context, Stanford Al...   \n",
       "4   The rise of fine-tuning and customization by h...   \n",
       "5   The rise of fine-tuning and customization by h...   \n",
       "6   The rise of fine-tuning and customization by h...   \n",
       "7   The emergence of prompt-driven app generation ...   \n",
       "8   ChatGPT has been a recurring topic in blog pos...   \n",
       "9   Claude 3.5 Sonnet is a leading large language ...   \n",
       "10  People think ChatGPT is easy because it look l...   \n",
       "11  Over the past year, the landscape of LLMs (Lar...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "executionInfo": {
     "elapsed": 85302,
     "status": "ok",
     "timestamp": 1746481513332,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "6ReXjI8wYxof"
   },
   "outputs": [],
   "source": [
    "for test_row in dataset:\n",
    "  response_ft = graph_ft.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response_ft[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1746481520185,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "aYT2kq22Y8Ws",
    "outputId": "6b0a6301-f9d7-4255-f2ff-72038b271cad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Why people think ChatGPT is easy but it not, and what problems come from people using ChatGPT wrong, like with screenshots, and what did the blogger say about this in his 2024 posts?\",\n          \"What is Claude 3.5 Sonnet, and how did its introduction of Claude Artifacts impact the development and accessibility of prompt-driven app generation compared to other leading models?\",\n          \"Wut did Meta do with Llama?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LLM agents are useful because they represent AI systems that can potentially act on your behalf, automating tasks or making decisions. However, their usefulness is currently limited by their inherent gullibility\\u2014they believe anything you tell them and cannot reliably distinguish truth from fiction. Despite this challenge, there are genuine valuable applications for LLMs, particularly in areas like writing code, where their capabilities have proven astonishing and effective.\\n\\nThe key to benefiting from LLM agents lies in developing the skill to work with technology that is both powerful and inherently unreliable. This requires careful design, guidance, and education to avoid intuitive traps and to apply these tools effectively. While fully reliable LLM agents may require breakthroughs like AGI, they already hold promise as powerful, if complex, assistive tools for expert or power users.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"People think ChatGPT is easy because it look like just typing to a chatbot, but really you need lots of understanding and experience to use it right and not fall in traps. The blogger say in 2024 that most users get thrown in deep end, and many have wrong ideas about what ChatGPT can do. Some people try to win arguments with screenshots from ChatGPT, but that is silly because the models are unreliable and can say anything if you prompt them right. The blogger wrote in his 2024 posts about these problems, saying there is a big knowledge gap and people need better education to use ChatGPT and other LLMs responsibly.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"synthesizer_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"single_hop_specifc_query_synthesizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-4addac2b-0905-4239-8d44-acd681f4b7c5\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wut did Meta do with Llama?</td>\n",
       "      <td>[I wrote about how Large language models are h...</td>\n",
       "      <td>[We don’t yet know how to build GPT-4 Vibes Ba...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>In February, Meta released Llama, and later in...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Given the increasing capabilities of large lan...</td>\n",
       "      <td>[Code may be the best application\\n\\nThe ethic...</td>\n",
       "      <td>[I’m surprised that no-one has beaten the now ...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>The grammar rules of programming languages lik...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whatt role did the 1950s play in the devellopm...</td>\n",
       "      <td>[Simon Willison’s Weblog\\n\\nSubscribe\\n\\nStuff...</td>\n",
       "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>The academic field of Artificial Intelligence ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As an AI enthusiast and technology blogger int...</td>\n",
       "      <td>[a browser? 40.5k 49.2k How to implement Q&amp;A a...</td>\n",
       "      <td>[Microsoft over this issue. The 69 page PDF is...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>According to the provided context, Stanford Al...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[Gullibility is the biggest unsolved problem\\n...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[LLMs need better criticism\\n\\nA lot of people...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has the rise of fine-tuning and customizat...</td>\n",
       "      <td>[You can even run them entirely in your browse...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>The rise of fine-tuning and customization by h...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How did the emergence of prompt-driven app gen...</td>\n",
       "      <td>[These abilities are just a few weeks old at t...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\ndid. These abilities are just a fe...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>The emergence of prompt-driven app generation ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Based on the blog posts and analytics data, ho...</td>\n",
       "      <td>[Law is not ethics. Is it OK to train models o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nof things, here’s every long-form ...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>ChatGPT has been a recurring topic in blog pos...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is Claude 3.5 Sonnet, and how did its int...</td>\n",
       "      <td>[Anthropic kicked this idea into high gear whe...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nup there with Claude 3.5 Sonnet. V...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>Claude 3.5 Sonnet is a leading large language ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Why people think ChatGPT is easy but it not, a...</td>\n",
       "      <td>[Meanwhile, it’s increasingly common for end u...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nby DeepSeek-R1. Meta’s Llama 3.3 7...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>People think ChatGPT is easy because it look l...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How has the rapid evolution of LLMs (Large Lan...</td>\n",
       "      <td>[I wrote about how Large language models are h...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWe don’t yet know how to build GPT...</td>\n",
       "      <td>LLM agents are useful because they represent A...</td>\n",
       "      <td>Over the past year, the landscape of LLMs (Lar...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4addac2b-0905-4239-8d44-acd681f4b7c5')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-4addac2b-0905-4239-8d44-acd681f4b7c5 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-4addac2b-0905-4239-8d44-acd681f4b7c5');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-5bee7e99-7863-4840-959d-74c900f8db91\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bee7e99-7863-4840-959d-74c900f8db91')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-5bee7e99-7863-4840-959d-74c900f8db91 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0                         Wut did Meta do with Llama?   \n",
       "1   Given the increasing capabilities of large lan...   \n",
       "2   Whatt role did the 1950s play in the devellopm...   \n",
       "3   As an AI enthusiast and technology blogger int...   \n",
       "4   How has the rise of fine-tuning and customizat...   \n",
       "5   How has the rise of fine-tuning and customizat...   \n",
       "6   How has the rise of fine-tuning and customizat...   \n",
       "7   How did the emergence of prompt-driven app gen...   \n",
       "8   Based on the blog posts and analytics data, ho...   \n",
       "9   What is Claude 3.5 Sonnet, and how did its int...   \n",
       "10  Why people think ChatGPT is easy but it not, a...   \n",
       "11  How has the rapid evolution of LLMs (Large Lan...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [I wrote about how Large language models are h...   \n",
       "1   [Code may be the best application\\n\\nThe ethic...   \n",
       "2   [Simon Willison’s Weblog\\n\\nSubscribe\\n\\nStuff...   \n",
       "3   [a browser? 40.5k 49.2k How to implement Q&A a...   \n",
       "4   [Gullibility is the biggest unsolved problem\\n...   \n",
       "5   [LLMs need better criticism\\n\\nA lot of people...   \n",
       "6   [You can even run them entirely in your browse...   \n",
       "7   [These abilities are just a few weeks old at t...   \n",
       "8   [Law is not ethics. Is it OK to train models o...   \n",
       "9   [Anthropic kicked this idea into high gear whe...   \n",
       "10  [Meanwhile, it’s increasingly common for end u...   \n",
       "11  [I wrote about how Large language models are h...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [We don’t yet know how to build GPT-4 Vibes Ba...   \n",
       "1   [I’m surprised that no-one has beaten the now ...   \n",
       "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
       "3   [Microsoft over this issue. The 69 page PDF is...   \n",
       "4   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "5   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "6   [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "7   [<1-hop>\\n\\ndid. These abilities are just a fe...   \n",
       "8   [<1-hop>\\n\\nof things, here’s every long-form ...   \n",
       "9   [<1-hop>\\n\\nup there with Claude 3.5 Sonnet. V...   \n",
       "10  [<1-hop>\\n\\nby DeepSeek-R1. Meta’s Llama 3.3 7...   \n",
       "11  [<1-hop>\\n\\nWe don’t yet know how to build GPT...   \n",
       "\n",
       "                                             response  \\\n",
       "0   LLM agents are useful because they represent A...   \n",
       "1   LLM agents are useful because they represent A...   \n",
       "2   LLM agents are useful because they represent A...   \n",
       "3   LLM agents are useful because they represent A...   \n",
       "4   LLM agents are useful because they represent A...   \n",
       "5   LLM agents are useful because they represent A...   \n",
       "6   LLM agents are useful because they represent A...   \n",
       "7   LLM agents are useful because they represent A...   \n",
       "8   LLM agents are useful because they represent A...   \n",
       "9   LLM agents are useful because they represent A...   \n",
       "10  LLM agents are useful because they represent A...   \n",
       "11  LLM agents are useful because they represent A...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   In February, Meta released Llama, and later in...   \n",
       "1   The grammar rules of programming languages lik...   \n",
       "2   The academic field of Artificial Intelligence ...   \n",
       "3   According to the provided context, Stanford Al...   \n",
       "4   The rise of fine-tuning and customization by h...   \n",
       "5   The rise of fine-tuning and customization by h...   \n",
       "6   The rise of fine-tuning and customization by h...   \n",
       "7   The emergence of prompt-driven app generation ...   \n",
       "8   ChatGPT has been a recurring topic in blog pos...   \n",
       "9   Claude 3.5 Sonnet is a leading large language ...   \n",
       "10  People think ChatGPT is easy because it look l...   \n",
       "11  Over the past year, the landscape of LLMs (Lar...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746481910922,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "tUlWgqOKbj2_"
   },
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset_ft = EvaluationDataset.from_pandas(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1746479363733,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "m9-Rvo-KR-95"
   },
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1746482020057,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "rQOM-JrsSFQA"
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "bc2347a880da479f98f6c389e1724d37",
      "5194cd5d66a14b1f8d726a8aafc90104",
      "eee425060a15459083bec80477174a92",
      "17a2d8c0c9f44d84820261200cac47ec",
      "b6452bbe3d4d40d1ad0dbfc55022fbf5",
      "44dc1a63ee39441f9db77857a20d1aa1",
      "7f2426168614458abacc580059edced7",
      "81bc6bc10171420ca9b5c7ee901ebefe",
      "ac121c364e554b1e8c50ea445ea5b97b",
      "2e174f748d9e4c2183e94e3929cdf209",
      "13f015e77c5e4b4fb11e8234111ebf9a"
     ]
    },
    "executionInfo": {
     "elapsed": 833059,
     "status": "ok",
     "timestamp": 1746480217056,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "xLSoA2L8SGco",
    "outputId": "13b468eb-bdc0-4969-a27e-e3539228e73c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2347a880da479f98f6c389e1724d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[11]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29732, Requested 1835. Please try again in 3.134s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29721, Requested 2168. Please try again in 3.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[26]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29063, Requested 1787. Please try again in 1.7s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[22]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29892, Requested 1834. Please try again in 3.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29575, Requested 2273. Please try again in 3.696s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29054, Requested 2347. Please try again in 2.802s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[7]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29042, Requested 2453. Please try again in 2.99s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[25]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29425, Requested 2099. Please try again in 3.048s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[28]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29744, Requested 1824. Please try again in 3.136s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[24]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29035, Requested 2381. Please try again in 2.832s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[29]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29783, Requested 2099. Please try again in 3.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[36]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29531, Requested 2338. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[35]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29873, Requested 2043. Please try again in 3.832s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29655, Requested 2420. Please try again in 4.15s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[42]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 30000, Requested 2423. Please try again in 4.846s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[49]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 30000, Requested 2295. Please try again in 4.59s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[43]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 30000, Requested 2574. Please try again in 5.148s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[41]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29698, Requested 1928. Please try again in 3.252s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[54]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29650, Requested 2380. Please try again in 4.06s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[37]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28890, Requested 2837. Please try again in 3.454s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[48]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28820, Requested 2225. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[55]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28624, Requested 2535. Please try again in 2.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[31]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[53]: TimeoutError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.4672, 'faithfulness': 0.8835, 'factual_correctness': 0.5664, 'answer_relevancy': 0.8508, 'context_entity_recall': 0.3520, 'noise_sensitivity_relevant': 0.1166}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715,
     "referenced_widgets": [
      "ee0be34c950d423697c26d1450068bc6",
      "43487a3c966742a882c21f9ba7343d46",
      "a0504197855242b79f72c4cdbe467468",
      "3516260f42ba49448f6d0dd0c5e23a97",
      "08655436974b46b8a2607513654ef857",
      "bdff5ef28c394c0dbf5a5651b3039280",
      "cfab68da3d134da88b08e8c7555f620d",
      "a1b82138a39d4ebbaadd76c4f38df6ba",
      "8dd9ce2885ed433d879b339a1647727a",
      "7ca28608daa540d48adf36d3db3a6c47",
      "6b29241cd98644e4bde41c079831e8c4"
     ]
    },
    "executionInfo": {
     "elapsed": 873656,
     "status": "ok",
     "timestamp": 1746482914220,
     "user": {
      "displayName": "Vivek Natan",
      "userId": "05039321159873810551"
     },
     "user_tz": 240
    },
    "id": "DvSE4y0tb_az",
    "outputId": "d4499c45-6483-40de-c282-776f31687bf4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0be34c950d423697c26d1450068bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28840, Requested 2285. Please try again in 2.25s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[22]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29578, Requested 1775. Please try again in 2.706s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[16]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29898, Requested 1841. Please try again in 3.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[7]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29277, Requested 2463. Please try again in 3.48s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[28]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29706, Requested 1812. Please try again in 3.036s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28967, Requested 2339. Please try again in 2.612s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[24]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28946, Requested 2369. Please try again in 2.63s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[25]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29944, Requested 2443. Please try again in 4.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29853, Requested 2472. Please try again in 4.65s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28743, Requested 2460. Please try again in 2.406s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[31]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28989, Requested 2422. Please try again in 2.822s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[5]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[11]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[35]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 30000, Requested 1520. Please try again in 3.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[23]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[34]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29581, Requested 1838. Please try again in 2.838s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[40]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28746, Requested 1786. Please try again in 1.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[36]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29118, Requested 2319. Please try again in 2.874s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[29]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[43]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28793, Requested 2334. Please try again in 2.254s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[37]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28919, Requested 2417. Please try again in 2.672s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[49]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29211, Requested 2195. Please try again in 2.812s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[48]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29200, Requested 2209. Please try again in 2.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[42]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29063, Requested 2317. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[52]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29808, Requested 1619. Please try again in 2.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[46]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28556, Requested 1711. Please try again in 534ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[54]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29584, Requested 2279. Please try again in 3.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[41]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29918, Requested 1521. Please try again in 2.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[55]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28566, Requested 2336. Please try again in 1.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[61]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 28965, Requested 2471. Please try again in 2.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[58]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 30000, Requested 1717. Please try again in 3.434s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[66]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29906, Requested 2642. Please try again in 5.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[60]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-oZLvnlDjSb13xHAN8u5M1czV on tokens per min (TPM): Limit 30000, Used 29677, Requested 2315. Please try again in 3.984s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[53]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[71]: TimeoutError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.6667, 'faithfulness': 0.0500, 'factual_correctness': 0.1025, 'answer_relevancy': 0.7688, 'context_entity_recall': 0.4794, 'noise_sensitivity_relevant': 0.0435}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "result_ft = evaluate(\n",
    "    dataset=evaluation_dataset_ft,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "result_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJojegm8VXSH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ItmCnxsgVXN"
   },
   "source": [
    "<b>Baseline :: </b>{'context_recall': 0.4672, 'faithfulness': 0.8835, 'factual_correctness': 0.5664, 'answer_relevancy': 0.8508, 'context_entity_recall': 0.3520, 'noise_sensitivity_relevant': 0.1166}\n",
    "<br><b>Fine Tuned :: </b>{'context_recall': 0.6667, 'faithfulness': 0.0500, 'factual_correctness': 0.1025, 'answer_relevancy': 0.7688, 'context_entity_recall': 0.4794, 'noise_sensitivity_relevant': 0.0435}\n",
    "\n",
    "Notes: Improved context recall, context_entity_recall, noise_sensitivity_relevant.\n",
    "Reduced faithfulness, factual correctness and answer relevancy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lcR3-0s19_lu"
   ],
   "gpuType": "L4",
   "provenance": [
    {
     "file_id": "1oISd4Tt3CKcF8SZ9T90PKX_PDhN5JKuM",
     "timestamp": 1746456194803
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00ec564dcb1c43a593d5f9741dc192ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02181e8a21474ca596d849b54986a7ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05032e792ed74dd5b702c63c2fe8b49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c84e0530bf41426eb02eb5d541d4273c",
       "IPY_MODEL_139f012430be4970bfa31db27463f021",
       "IPY_MODEL_d28e8b2e405e42a4ba469a593d3f8721"
      ],
      "layout": "IPY_MODEL_0fa5daccdae149ffb395365641d4e323"
     }
    },
    "08655436974b46b8a2607513654ef857": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a7c4b72307247eab9911dcdb3cea9ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "0aa77f84068446cba7649b43ca86155f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0be68545bb594f5bb58ebf2bffcb6d98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bf2476b449244d097a4127508f3189a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac3613f393a04752871bd659ef00f6a1",
       "IPY_MODEL_5c053a2fee294738a23b58d1137a0018",
       "IPY_MODEL_9d846c1932b843f08d0898b4cd834913"
      ],
      "layout": "IPY_MODEL_8f9e1d6114474c80b59f02295e78c3ed"
     }
    },
    "0dd96532d8b742c2a305885d2198e0ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_9f6adadc7c384bbab5a5bc84bbf331f3"
     }
    },
    "0fa5daccdae149ffb395365641d4e323": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "0fe6841a644a4055abf7c38e38f6ccec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "102ebd93ae9240608603ed1b7b90df52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12934683c7784ef38f727d4f5a609eaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "139f012430be4970bfa31db27463f021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c4ee659753b4068b1219a133817d65a",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b5277ec53f81438bab95bd3a3333fe97",
      "value": 26
     }
    },
    "13f015e77c5e4b4fb11e8234111ebf9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17a2d8c0c9f44d84820261200cac47ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e174f748d9e4c2183e94e3929cdf209",
      "placeholder": "​",
      "style": "IPY_MODEL_13f015e77c5e4b4fb11e8234111ebf9a",
      "value": " 72/72 [13:52&lt;00:00, 57.69s/it]"
     }
    },
    "190d32b2b205445cb5aac5183efa6273": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b0fda2e06224fb7bc554165d80fc824": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b30158a93be48109d40e5d1f2d0976c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c6e8671009440279edf5bd31629cd8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02181e8a21474ca596d849b54986a7ef",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da61170c2b6f44aeafd46102f5b8ec0b",
      "value": 2
     }
    },
    "1ed0a0cbd79a4530858a548adcb2e021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2125c4e42d1d487eb012943308be5b7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8521f7f3c81d45fbaeadb4411deb0cf0",
      "placeholder": "​",
      "style": "IPY_MODEL_b666683101fd48a3a00beb73fcd518f1",
      "value": " 0/1 [00:00&lt;?, ?example/s]"
     }
    },
    "25794408aceb48a59bc098b4ac315d8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25f759f7bd9346d9b401d2cae332ff9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2699b1047f924dc08b186f25a5882632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_4896e10bfebb47c5a417df397fe68101",
      "placeholder": "​",
      "style": "IPY_MODEL_e398aebe12fe419aaea5062d20f87de0",
      "value": ""
     }
    },
    "27e8214d12d546729dd0ccc4bf39ef92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28e912eeb7b94f5ca4b419c119d896fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9898450628354757ba244a2c51d6fb00",
      "placeholder": "​",
      "style": "IPY_MODEL_1b30158a93be48109d40e5d1f2d0976c",
      "value": "Generating Scenarios: 100%"
     }
    },
    "29b0937b71d04e618de3b3dbda85d481": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "2ba00b9fb60349a0b23147132b7bd441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f974cf8016bf4b788e2ff1b9cb8c181d",
      "placeholder": "​",
      "style": "IPY_MODEL_cb7123cc47554a7fb68b6179866c3520",
      "value": "Computing widget examples:   0%"
     }
    },
    "2baa8c9c9261412896571fa5c6bb86df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d951491ce1a4abf9f1869e25ed0d674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2daa4c01c544b9aa5f19cea631bfe96",
       "IPY_MODEL_7ef9d0013dec4a999340437e4444a897",
       "IPY_MODEL_e2a715d48a504961bbc23e499cc3db3e"
      ],
      "layout": "IPY_MODEL_40753e2e6488468da2739fc9a6ba33fb"
     }
    },
    "2e174f748d9e4c2183e94e3929cdf209": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e504702b52f4c12a433c7ec493f32ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f4a9a9415584f138520dac005adca0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c16d797c27041d097445930dd0b451d",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966176d6f33b4a3e9338839c7a1a9d16",
      "value": 12
     }
    },
    "3237af9d961f43b2a5f2fcba7ad0ae81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3516260f42ba49448f6d0dd0c5e23a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ca28608daa540d48adf36d3db3a6c47",
      "placeholder": "​",
      "style": "IPY_MODEL_6b29241cd98644e4bde41c079831e8c4",
      "value": " 72/72 [14:33&lt;00:00, 59.40s/it]"
     }
    },
    "3721b8de0ef54c07872ffc0ee080796e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_946a5bfe29644c1b84af3911eaabe8b0",
       "IPY_MODEL_7876e4d3d6334600b6f6624067a8a14c",
       "IPY_MODEL_3c0ada0cae784b9fb8b635ed107c50b2"
      ],
      "layout": "IPY_MODEL_ff616f98f13f4484bdf726eb4ad30047"
     }
    },
    "3c0ada0cae784b9fb8b635ed107c50b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6253dac241da4e81b00bcbf669189c91",
      "placeholder": "​",
      "style": "IPY_MODEL_0fe6841a644a4055abf7c38e38f6ccec",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "3eaf71c79d1e446181f2ac847da63f03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "403a843e33854016a1cb6694d953f621": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40753e2e6488468da2739fc9a6ba33fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "43487a3c966742a882c21f9ba7343d46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdff5ef28c394c0dbf5a5651b3039280",
      "placeholder": "​",
      "style": "IPY_MODEL_cfab68da3d134da88b08e8c7555f620d",
      "value": "Evaluating: 100%"
     }
    },
    "43e8c5c0dd7f412c82a0e0442d565d53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44040803cf25423d849772b9975de852": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44dc1a63ee39441f9db77857a20d1aa1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48113a09667146f095560c45025ddc46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4896e10bfebb47c5a417df397fe68101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bd51055dd054cccaa9cc347105beb63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ca3cc5957a74471b4a65953de8d8307": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cfaf6da2ee74932867926a063fa4c7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5194cd5d66a14b1f8d726a8aafc90104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44dc1a63ee39441f9db77857a20d1aa1",
      "placeholder": "​",
      "style": "IPY_MODEL_7f2426168614458abacc580059edced7",
      "value": "Evaluating: 100%"
     }
    },
    "529e5f8942324f8aa0173c3b27c64675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc062fba3d64e36b922dbae4dac82df",
      "placeholder": "​",
      "style": "IPY_MODEL_7eda79260bc8476595462bb1620442e8",
      "value": " 12/12 [00:45&lt;00:00,  8.19s/it]"
     }
    },
    "531715b1b0514cd3837801ff13e6276e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "559593b700864da9b35569b20eb66ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "580c405ec96047c8acbee5f8676b9af4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b662dfad4d74fb7836c4eb46c6f56ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_9a4919d5f5d241bba7e803797d4088dd",
      "style": "IPY_MODEL_f33c324a78bd44ab9fde0c4592508d6b",
      "value": true
     }
    },
    "5c053a2fee294738a23b58d1137a0018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403a843e33854016a1cb6694d953f621",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b136301f39bd4a9795e83d5a91153a5c",
      "value": 2
     }
    },
    "5c142a72ff164e3084523c025c2f0e3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_713e2794bc1b49f08d6661adfd7c71be",
      "max": 1336413848,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5261202cfa2407bb1af42aa3a530ed9",
      "value": 1336413848
     }
    },
    "5c16d797c27041d097445930dd0b451d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c4ee659753b4068b1219a133817d65a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d389807e43d4c5c8d7d06e994f48001": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ba00b9fb60349a0b23147132b7bd441",
       "IPY_MODEL_7e9c3524c47348909e9b3b83a4ad59f0",
       "IPY_MODEL_2125c4e42d1d487eb012943308be5b7e"
      ],
      "layout": "IPY_MODEL_ceb5625f0b4d4d1389600b7ae92d2b7c"
     }
    },
    "60a3387e2f7e48b08d359ad27f9b3697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f223590aeee34a44a10d495a08c7d4af",
      "placeholder": "​",
      "style": "IPY_MODEL_71dd6e28a80f437aa3a6f0a5a018a74e",
      "value": "Generating personas: 100%"
     }
    },
    "6253dac241da4e81b00bcbf669189c91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6629a1e9dcf04107b6be419d6fbe79c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28e912eeb7b94f5ca4b419c119d896fc",
       "IPY_MODEL_890255e0857b4cddbd6449c6eeab1473",
       "IPY_MODEL_ace32d66ea6e4c799b9416a36daa0bee"
      ],
      "layout": "IPY_MODEL_559593b700864da9b35569b20eb66ca3"
     }
    },
    "6b29241cd98644e4bde41c079831e8c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "713e2794bc1b49f08d6661adfd7c71be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "715dc2cb8fa4417ca8067d50dafd8e73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f82bda2421a468aa4504e71aed04be7",
       "IPY_MODEL_5c142a72ff164e3084523c025c2f0e3b",
       "IPY_MODEL_9ce8675b1ac14c3e8874f56f95540ca9"
      ],
      "layout": "IPY_MODEL_4bd51055dd054cccaa9cc347105beb63"
     }
    },
    "71dd6e28a80f437aa3a6f0a5a018a74e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "751603d61886401eb5162db1792209cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7734f6c14408433ab6feda59e9002507": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d7ca48c738438e850d6cd1c5ed9fa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60a3387e2f7e48b08d359ad27f9b3697",
       "IPY_MODEL_d9ff8ef8512f448d8e98ee3d7d26856d",
       "IPY_MODEL_823f22fdc1d2457a8cb7fe38871b5533"
      ],
      "layout": "IPY_MODEL_b59de75c15814f2db4e0aa32041f47dc"
     }
    },
    "7876e4d3d6334600b6f6624067a8a14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8ea3ebe15274427b3f5315c475a74dd",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85b51f2453014edea901f3180edcf15c",
      "value": 2
     }
    },
    "7b8d37ba0cfe46e8945c62ba6156c278": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ca28608daa540d48adf36d3db3a6c47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e9c3524c47348909e9b3b83a4ad59f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2baa8c9c9261412896571fa5c6bb86df",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f00f745738f741f386b8aaf59bbc49f0",
      "value": 1
     }
    },
    "7eda79260bc8476595462bb1620442e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ef9d0013dec4a999340437e4444a897": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25f759f7bd9346d9b401d2cae332ff9f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9678141cfcfd46fe9a5fb7248514a31f",
      "value": 2
     }
    },
    "7f2426168614458abacc580059edced7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f82bda2421a468aa4504e71aed04be7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_102ebd93ae9240608603ed1b7b90df52",
      "placeholder": "​",
      "style": "IPY_MODEL_27e8214d12d546729dd0ccc4bf39ef92",
      "value": "model.safetensors: 100%"
     }
    },
    "8166caf2897d48caacbc12d66f235ca5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81bc6bc10171420ca9b5c7ee901ebefe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "823f22fdc1d2457a8cb7fe38871b5533": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cab5303c0abc48d498b85d667d41dc50",
      "placeholder": "​",
      "style": "IPY_MODEL_7b8d37ba0cfe46e8945c62ba6156c278",
      "value": " 2/2 [00:01&lt;00:00,  1.17s/it]"
     }
    },
    "8521f7f3c81d45fbaeadb4411deb0cf0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85b51f2453014edea901f3180edcf15c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88832729e9ff49d9a117ea3dd881a980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "890255e0857b4cddbd6449c6eeab1473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48113a09667146f095560c45025ddc46",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8166caf2897d48caacbc12d66f235ca5",
      "value": 3
     }
    },
    "8dd9ce2885ed433d879b339a1647727a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f9e1d6114474c80b59f02295e78c3ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "946a5bfe29644c1b84af3911eaabe8b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcf44192f3ba4ffab95901fa28d72623",
      "placeholder": "​",
      "style": "IPY_MODEL_ec745a0b3e144c43808855e76a0ab6c0",
      "value": "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%"
     }
    },
    "9525569d20bb46cb8b6961855da82c84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e22eaaa41327415eacca172d69387461",
       "IPY_MODEL_1c6e8671009440279edf5bd31629cd8f",
       "IPY_MODEL_a22bf68e37f344d188e8995b549a7e09"
      ],
      "layout": "IPY_MODEL_29b0937b71d04e618de3b3dbda85d481"
     }
    },
    "966176d6f33b4a3e9338839c7a1a9d16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9678141cfcfd46fe9a5fb7248514a31f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96975c72aefb4cd5bd74f7b919678dbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_00ec564dcb1c43a593d5f9741dc192ab",
      "style": "IPY_MODEL_0a7c4b72307247eab9911dcdb3cea9ce",
      "tooltip": ""
     }
    },
    "9898450628354757ba244a2c51d6fb00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9927e9df270e43b6a8e47b18d929da54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a4919d5f5d241bba7e803797d4088dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ce8675b1ac14c3e8874f56f95540ca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44040803cf25423d849772b9975de852",
      "placeholder": "​",
      "style": "IPY_MODEL_ee66adea3c4f48eca23773b5b9200be9",
      "value": " 1.34G/1.34G [00:50&lt;00:00, 15.5MB/s]"
     }
    },
    "9d846c1932b843f08d0898b4cd834913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a67dddb1a34f4608866a7a19e57348ef",
      "placeholder": "​",
      "style": "IPY_MODEL_ac97843887e1457d933a032d3e39d1e2",
      "value": " 1/2 [00:03&lt;00:03,  3.52s/it]"
     }
    },
    "9f6adadc7c384bbab5a5bc84bbf331f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "a0504197855242b79f72c4cdbe467468": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1b82138a39d4ebbaadd76c4f38df6ba",
      "max": 72,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8dd9ce2885ed433d879b339a1647727a",
      "value": 72
     }
    },
    "a0964ee9826a4796986f0a0b722be13b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed87d56303234f4b8f1412e7cd34af57",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ecb82c23461a4344a8a7a091c73a3999",
      "value": 12
     }
    },
    "a0d4fb18c04d4dfcbc581bdd67862910": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1b82138a39d4ebbaadd76c4f38df6ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a22bf68e37f344d188e8995b549a7e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a46831a634f74b3b93f302f26863968f",
      "placeholder": "​",
      "style": "IPY_MODEL_25794408aceb48a59bc098b4ac315d8d",
      "value": " 2/2 [00:01&lt;00:00,  1.26it/s]"
     }
    },
    "a27440e2f1bf413e900e8aebbd48ca06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a46831a634f74b3b93f302f26863968f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a67dddb1a34f4608866a7a19e57348ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a74bfec978d14a39b02255441afe8593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a94e4fe679be471b843046fea1a40ad7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c982ea751ec14f28a909ec418e011d5c",
       "IPY_MODEL_a0964ee9826a4796986f0a0b722be13b",
       "IPY_MODEL_529e5f8942324f8aa0173c3b27c64675"
      ],
      "layout": "IPY_MODEL_531715b1b0514cd3837801ff13e6276e"
     }
    },
    "a99d854c0d764be6a96b14bf29486487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abe92a85c01c4a71951777998c7c9048": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac121c364e554b1e8c50ea445ea5b97b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ac3613f393a04752871bd659ef00f6a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0be68545bb594f5bb58ebf2bffcb6d98",
      "placeholder": "​",
      "style": "IPY_MODEL_abe92a85c01c4a71951777998c7c9048",
      "value": "Applying SummaryExtractor:  50%"
     }
    },
    "ac97843887e1457d933a032d3e39d1e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ace32d66ea6e4c799b9416a36daa0bee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_580c405ec96047c8acbee5f8676b9af4",
      "placeholder": "​",
      "style": "IPY_MODEL_43e8c5c0dd7f412c82a0e0442d565d53",
      "value": " 3/3 [00:04&lt;00:00,  1.41s/it]"
     }
    },
    "adc062fba3d64e36b922dbae4dac82df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b136301f39bd4a9795e83d5a91153a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2daa4c01c544b9aa5f19cea631bfe96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e30dd42bcbc8466cb51538a2a7a116f1",
      "placeholder": "​",
      "style": "IPY_MODEL_fbf7291aa3dd44c183efe27fcef3cb8e",
      "value": "Applying HeadlineSplitter:   0%"
     }
    },
    "b5277ec53f81438bab95bd3a3333fe97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b59de75c15814f2db4e0aa32041f47dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6452bbe3d4d40d1ad0dbfc55022fbf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b666683101fd48a3a00beb73fcd518f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ee3d7e912d43f0b65b20a28bcf58e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0d4fb18c04d4dfcbc581bdd67862910",
      "placeholder": "​",
      "style": "IPY_MODEL_3237af9d961f43b2a5f2fcba7ad0ae81",
      "value": "Applying CustomNodeFilter: 100%"
     }
    },
    "bc2347a880da479f98f6c389e1724d37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5194cd5d66a14b1f8d726a8aafc90104",
       "IPY_MODEL_eee425060a15459083bec80477174a92",
       "IPY_MODEL_17a2d8c0c9f44d84820261200cac47ec"
      ],
      "layout": "IPY_MODEL_b6452bbe3d4d40d1ad0dbfc55022fbf5"
     }
    },
    "bdac169781294cd2b92fa78f9e6873da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d68ce41e3c5d4b3b9ca6cbaf97df201f",
      "placeholder": "​",
      "style": "IPY_MODEL_1ed0a0cbd79a4530858a548adcb2e021",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "bdff5ef28c394c0dbf5a5651b3039280": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c84e0530bf41426eb02eb5d541d4273c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1b9428dd4a2415192dcde126603fcfb",
      "placeholder": "​",
      "style": "IPY_MODEL_190d32b2b205445cb5aac5183efa6273",
      "value": "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]: 100%"
     }
    },
    "c8ea3ebe15274427b3f5315c475a74dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c982ea751ec14f28a909ec418e011d5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9927e9df270e43b6a8e47b18d929da54",
      "placeholder": "​",
      "style": "IPY_MODEL_a74bfec978d14a39b02255441afe8593",
      "value": "Generating Samples: 100%"
     }
    },
    "cab5303c0abc48d498b85d667d41dc50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb7123cc47554a7fb68b6179866c3520": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cba40d5976704346a6c3af9d85bfe9bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_751603d61886401eb5162db1792209cc",
      "placeholder": "​",
      "style": "IPY_MODEL_ec57cc84ea224e869e800cdd2f1f86d6",
      "value": "Connecting..."
     }
    },
    "ceb5625f0b4d4d1389600b7ae92d2b7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "cfab68da3d134da88b08e8c7555f620d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1b9428dd4a2415192dcde126603fcfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d28e8b2e405e42a4ba469a593d3f8721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e504702b52f4c12a433c7ec493f32ec",
      "placeholder": "​",
      "style": "IPY_MODEL_a27440e2f1bf413e900e8aebbd48ca06",
      "value": " 26/26 [01:52&lt;00:00, 14.83s/it]"
     }
    },
    "d68ce41e3c5d4b3b9ca6cbaf97df201f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9ff8ef8512f448d8e98ee3d7d26856d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7734f6c14408433ab6feda59e9002507",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0aa77f84068446cba7649b43ca86155f",
      "value": 2
     }
    },
    "da61170c2b6f44aeafd46102f5b8ec0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dcf44192f3ba4ffab95901fa28d72623": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd6571aca75a474ebc776214ff641d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ca3cc5957a74471b4a65953de8d8307",
      "placeholder": "​",
      "style": "IPY_MODEL_fbd0549062c44fc080c911b34632315c",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "e22eaaa41327415eacca172d69387461": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88832729e9ff49d9a117ea3dd881a980",
      "placeholder": "​",
      "style": "IPY_MODEL_a99d854c0d764be6a96b14bf29486487",
      "value": "Applying HeadlinesExtractor: 100%"
     }
    },
    "e2a715d48a504961bbc23e499cc3db3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12934683c7784ef38f727d4f5a609eaf",
      "placeholder": "​",
      "style": "IPY_MODEL_3eaf71c79d1e446181f2ac847da63f03",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "e30dd42bcbc8466cb51538a2a7a116f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e398aebe12fe419aaea5062d20f87de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e97fdbda245b4ad5ae6ef333b2c36c4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "ec57cc84ea224e869e800cdd2f1f86d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec745a0b3e144c43808855e76a0ab6c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecb82c23461a4344a8a7a091c73a3999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed87d56303234f4b8f1412e7cd34af57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee0be34c950d423697c26d1450068bc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43487a3c966742a882c21f9ba7343d46",
       "IPY_MODEL_a0504197855242b79f72c4cdbe467468",
       "IPY_MODEL_3516260f42ba49448f6d0dd0c5e23a97"
      ],
      "layout": "IPY_MODEL_08655436974b46b8a2607513654ef857"
     }
    },
    "ee66adea3c4f48eca23773b5b9200be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eee425060a15459083bec80477174a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81bc6bc10171420ca9b5c7ee901ebefe",
      "max": 72,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac121c364e554b1e8c50ea445ea5b97b",
      "value": 72
     }
    },
    "f00f745738f741f386b8aaf59bbc49f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f223590aeee34a44a10d495a08c7d4af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f33c324a78bd44ab9fde0c4592508d6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5261202cfa2407bb1af42aa3a530ed9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f974cf8016bf4b788e2ff1b9cb8c181d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9e1354ee63f4217ad048c3cea56c4f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9ee3d7e912d43f0b65b20a28bcf58e9",
       "IPY_MODEL_2f4a9a9415584f138520dac005adca0d",
       "IPY_MODEL_fa288e3b53ed42128d7780fd79e98e33"
      ],
      "layout": "IPY_MODEL_e97fdbda245b4ad5ae6ef333b2c36c4a"
     }
    },
    "fa288e3b53ed42128d7780fd79e98e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b0fda2e06224fb7bc554165d80fc824",
      "placeholder": "​",
      "style": "IPY_MODEL_4cfaf6da2ee74932867926a063fa4c7e",
      "value": " 12/12 [00:36&lt;00:00,  3.58s/it]"
     }
    },
    "fbd0549062c44fc080c911b34632315c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbf7291aa3dd44c183efe27fcef3cb8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff616f98f13f4484bdf726eb4ad30047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
